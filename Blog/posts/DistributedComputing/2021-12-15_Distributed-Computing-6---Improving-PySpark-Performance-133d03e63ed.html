<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Distributed Computing 6 | Improving PySpark Performance</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Distributed Computing 6 | Improving PySpark Performance</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Distributed Computing
</section>
<section data-field="body" class="e-content">
<section name="e3c8" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="7ef0" id="7ef0" class="graf graf--h3 graf--leading graf--title">Distributed Computing 6 | Improving PySpark Performance</h3><figure name="db95" id="db95" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*4WeKQZVVttf0_vvY.png" data-width="1144" data-height="634" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*4WeKQZVVttf0_vvY.png"></figure><ol class="postList"><li name="15d8" id="15d8" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Load and Save Data</strong></li></ol><ul class="postList"><li name="210c" id="210c" class="graf graf--li graf-after--li">Load all the files located in a folder as a single RDD. Each line should be one element in the RDD.</li></ul><pre name="5c71" id="5c71" class="graf graf--pre graf-after--li">sc.textFile(folder_name)</pre><ul class="postList"><li name="802e" id="802e" class="graf graf--li graf-after--pre">Load all the files located in a folder as a paired RDD. Each file should be one pair in the RDD.</li></ul><pre name="717a" id="717a" class="graf graf--pre graf-after--li">sc.wholeTextFiles(dir_name)</pre><ul class="postList"><li name="3fad" id="3fad" class="graf graf--li graf-after--pre">Return multiple outputs</li></ul><pre name="4002" id="4002" class="graf graf--pre graf-after--li">rdd.saveAsTextFile(dir_name)</pre><p name="52fb" id="52fb" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">2. Memory-Disk Persistence</strong></p><ul class="postList"><li name="c606" id="c606" class="graf graf--li graf-after--p">Select a persistency level for an RDD</li></ul><pre name="3490" id="3490" class="graf graf--pre graf-after--li">from pyspark.storagelevel import StorageLevel rdd.persist(StorageLevel.&lt;persistency_level&gt;)</pre><ul class="postList"><li name="5c62" id="5c62" class="graf graf--li graf-after--pre">Store RDDs in memory. If an RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they’re needed. This is the default level.</li></ul><pre name="f252" id="f252" class="graf graf--pre graf-after--li">rdd.persist(StorageLevel.MEMORY_ONLY)</pre><p name="550c" id="550c" class="graf graf--p graf-after--pre">or</p><pre name="3f4d" id="3f4d" class="graf graf--pre graf-after--p">rdd.cache()</pre><ul class="postList"><li name="466d" id="466d" class="graf graf--li graf-after--pre">Store RDDs in disk</li></ul><pre name="e011" id="e011" class="graf graf--pre graf-after--li">rdd.persist(StorageLevel.DISK_ONLY)</pre><ul class="postList"><li name="9804" id="9804" class="graf graf--li graf-after--pre">If an RDD does not fit in memory, store the partitions that don’t fit on disk, and read them from there when they’re needed.</li></ul><pre name="612f" id="612f" class="graf graf--pre graf-after--li">rdd.persist(StorageLevel.MEMORY_AND_DISK)</pre><ul class="postList"><li name="4cf0" id="4cf0" class="graf graf--li graf-after--pre">Same as the levels above, but replicate each partition on two cluster nodes.</li></ul><pre name="80d6" id="80d6" class="graf graf--pre graf-after--li">StorageLevel.MEMORY_ONLY_2<br>StorageLevel.MEMORY_AND_DISK_2<br>...</pre><ul class="postList"><li name="2c08" id="2c08" class="graf graf--li graf-after--pre">Manually remove the persistency level</li></ul><pre name="97ef" id="97ef" class="graf graf--pre graf-after--li">rdd.unpersist()</pre><p name="bc7a" id="bc7a" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">3. Parallelism Level</strong></p><ul class="postList"><li name="a1ff" id="a1ff" class="graf graf--li graf-after--p">Returns the number of partitions in RDD.</li></ul><pre name="b2c1" id="b2c1" class="graf graf--pre graf-after--li">rdd.getNumPartitions()</pre><ul class="postList"><li name="4005" id="4005" class="graf graf--li graf-after--pre">Return an RDD created by coalescing all elements within each partition into a list.</li></ul><pre name="6bbb" id="6bbb" class="graf graf--pre graf-after--li">rdd.glom().collect()</pre><ul class="postList"><li name="33f5" id="33f5" class="graf graf--li graf-after--pre">Specify the min number of partitions to cut the RDD into.</li></ul><pre name="1d48" id="1d48" class="graf graf--pre graf-after--li">rdd.&lt;trans&gt;(...,numPartitions)</pre><ul class="postList"><li name="0f8c" id="0f8c" class="graf graf--li graf-after--pre">Defines how the elements in an RDD or paired RDD are partitioned. The default HashPartitioner maps each key to a partition ID, from 0 to <code class="markup--code markup--li-code">numPartitions-1</code> . We can also <code class="markup--code markup--li-code">RangePartitioner</code> or <code class="markup--code markup--li-code">CustomPartitioner</code>.</li></ul><pre name="4b95" id="4b95" class="graf graf--pre graf-after--li">rdd.partitionBy(partition_size, partitionFunc)</pre><ul class="postList"><li name="8b97" id="8b97" class="graf graf--li graf-after--pre">Shuffle data across the network to create a new set of partitions.</li></ul><pre name="97bb" id="97bb" class="graf graf--pre graf-after--li">rdd.repartition(numPartitions: Int)</pre><ul class="postList"><li name="07bb" id="07bb" class="graf graf--li graf-after--pre">An optimized version of <code class="markup--code markup--li-code">repartition()</code> — avoid data movement and reduce the number of RDD partitions.</li></ul><pre name="df5f" id="df5f" class="graf graf--pre graf-after--li">rdd.coalesce(numPartitions: Int, shuffle = false)</pre><p name="bc23" id="bc23" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">4. RDD Dependencies (RDD Lineage)</strong></p><ul class="postList"><li name="481e" id="481e" class="graf graf--li graf-after--p">RDD Dependency: A dependency between an old and a new RDD is created, every time a transformation is performed on an RDD.</li><li name="b85c" id="b85c" class="graf graf--li graf-after--li">RDD Resilience: As Spark records the linage of each RDD, any RDDs can be <strong class="markup--strong markup--li-strong">reconstructed</strong> to the state it was at the time of the failure using RDD lineage.</li><li name="074e" id="074e" class="graf graf--li graf-after--li">Narrow Dependency: When no data shuffle between partitions is required.</li><li name="6ab1" id="6ab1" class="graf graf--li graf-after--li">Wide Dependency: When it requires shuffle when joining RDDs.</li><li name="d076" id="d076" class="graf graf--li graf-after--li">Shows a textual representation of RDD dependencies.</li></ul><pre name="699b" id="699b" class="graf graf--pre graf-after--li">rdd.toDebugString().decode(&#39;utf-8&#39;)</pre><ul class="postList"><li name="eeb0" id="eeb0" class="graf graf--li graf-after--pre">Set a checkpoint to remove the RDD linage including its parents’ information.</li></ul><pre name="2dc3" id="2dc3" class="graf graf--pre graf-after--li">sc.setCheckpointDir(&quot;dir&quot;)<br><strong class="markup--strong markup--pre-strong">rdd.checkpoint()</strong><br>rdd.action()</pre><p name="1e33" id="1e33" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">5. Shared Variables</strong></p><ul class="postList"><li name="58e8" id="58e8" class="graf graf--li graf-after--p">Create an accumulator for communication with Spark executors start by <code class="markup--code markup--li-code">inital_value</code> ,</li></ul><pre name="a1ba" id="a1ba" class="graf graf--pre graf-after--li">accum = sc.accumulator(inital_value)</pre><ul class="postList"><li name="859b" id="859b" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">foreach</code> action used to operate accumulator</li></ul><pre name="492e" id="492e" class="graf graf--pre graf-after--li">rdd.foreach(lambda x: accum.add(1))</pre><ul class="postList"><li name="605d" id="605d" class="graf graf--li graf-after--pre">Get the value of the accumulator</li></ul><pre name="aa08" id="aa08" class="graf graf--pre graf-after--li">accum.value</pre><ul class="postList"><li name="a7fb" id="a7fb" class="graf graf--li graf-after--pre">Create a <strong class="markup--strong markup--li-strong">read-only</strong> broadcast variable set by the driver</li></ul><pre name="745d" id="745d" class="graf graf--pre graf-after--li">broadcast_var = sc.broadcast(value)</pre><ul class="postList"><li name="27e8" id="27e8" class="graf graf--li graf-after--pre">Unpersist method removes from executor nodes. It still stays on the driver node, so it can be re-broadcast.</li></ul><pre name="1eb9" id="1eb9" class="graf graf--pre graf-after--li graf--trailing">broadcast_var.unpersist()</pre></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/133d03e63ed"><time class="dt-published" datetime="2021-12-15T18:12:54.437Z">December 15, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/distributed-computing-6-improving-pyspark-performance-133d03e63ed" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>