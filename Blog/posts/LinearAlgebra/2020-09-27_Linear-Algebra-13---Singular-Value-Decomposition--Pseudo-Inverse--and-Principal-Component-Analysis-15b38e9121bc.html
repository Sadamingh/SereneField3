<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Linear Algebra 13 | Singular Value Decomposition, Pseudo Inverse, and Principal Component Analysis</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Linear Algebra 13 | Singular Value Decomposition, Pseudo Inverse, and Principal Component Analysis</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Linear Algebra
</section>
<section data-field="body" class="e-content">
<section name="58ef" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="8005" id="8005" class="graf graf--h3 graf--leading graf--title">Linear Algebra 13 | Singular Value Decomposition, <strong class="markup--strong markup--h3-strong">Pseudo Inverse, </strong>and Principal Component Analysis</h3><figure name="b5dc" id="b5dc" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*Yj4DEKztkbPyDrnciYX9dQ.jpeg" data-width="1955" data-height="1296" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*Yj4DEKztkbPyDrnciYX9dQ.jpeg"></figure><ol class="postList"><li name="b6a7" id="b6a7" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Singular Value Decomposition</strong></li></ol><p name="61a3" id="61a3" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) The Problems of Eigendecomposition</strong></p><p name="3f72" id="3f72" class="graf graf--p graf-after--p">Although the eigendecomposition can be useful sometimes, the strong condition of this makes it hard to utilize for any given matrix. The problems are as follows,</p><ul class="postList"><li name="b286" id="b286" class="graf graf--li graf-after--p">S is usually not orthonormal (only for symmetric matrix)</li><li name="83d1" id="83d1" class="graf graf--li graf-after--li">There can be not enough eigenvectors (when the algebraic multiplicity &gt; the geometricity of an eigenvalue)</li><li name="9e0c" id="9e0c" class="graf graf--li graf-after--li">The matrix to be decomposed must be a square matrix.</li></ul><p name="26a0" id="26a0" class="graf graf--p graf-after--li">So we develop a new way to decomposition a matrix A, which perfectly solves all of these three problems.</p><p name="cc2d" id="cc2d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Recall: Eigendecomposition of the Symmetric Matrix</strong></p><p name="6cf1" id="6cf1" class="graf graf--p graf-after--p">Suppose A is any m × n transformation matrix and A: ℝⁿ → ℝᵐ. If we then look at AᵀA, this is a symmetric matrix that has a basis of ℝⁿ consisting of eigenvectors and AᵀA is orthonormally diagonalizable.</p><p name="7efa" id="7efa" class="graf graf--p graf-after--p">So if we look at,</p><figure name="3287" id="3287" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yJGHV30ZVrKE7wLgIMfoQA.png" data-width="1340" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*yJGHV30ZVrKE7wLgIMfoQA.png"></figure><p name="1a27" id="1a27" class="graf graf--p graf-after--figure">then the max value occurs in the direction of the eigenvector corresponding to the largest eigenvalue of AᵀA.</p><p name="fa40" id="fa40" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) The Definition of the Singular Value</strong></p><p name="1eaf" id="1eaf" class="graf graf--p graf-after--p">We can take those eigenvectors of AᵀA to be an orthogonal basis. Suppose we assign that <em class="markup--em markup--p-em">r </em>= rank(A). Then we take the basis that {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>r, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>r+1, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>n} corresponding to λ1 ≥ λ2 ≥ …≥ λ<em class="markup--em markup--p-em">r</em> ≥ 0 = … = 0 (based on the fact of the semi-definite quadratic form).</p><figure name="0b35" id="0b35" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*roZ26w-x-SQcuMem_-gy3A.png" data-width="1234" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*roZ26w-x-SQcuMem_-gy3A.png"></figure><p name="7e5b" id="7e5b" class="graf graf--p graf-after--figure">For any λi &gt; 0, we define that,</p><figure name="65aa" id="65aa" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*04_wPo-ioafItyMawR-Kiw.png" data-width="1316" data-height="232" src="https://cdn-images-1.medium.com/max/800/1*04_wPo-ioafItyMawR-Kiw.png"></figure><p name="cdb8" id="cdb8" class="graf graf--p graf-after--figure">so that σ1 ≥ σ2 ≥ …≥ σ<em class="markup--em markup--p-em">r</em> ≥ 0. These values σi are called the singular values.</p><p name="280a" id="280a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) A Fact of the Singular Value</strong></p><p name="3717" id="3717" class="graf graf--p graf-after--p">Because we have that,</p><figure name="ef27" id="ef27" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mYGIp_QnDUWMxk3r51f8Sw.png" data-width="1454" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*mYGIp_QnDUWMxk3r51f8Sw.png"></figure><p name="091e" id="091e" class="graf graf--p graf-after--figure">then,</p><figure name="1f58" id="1f58" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*N7DKKmooeM3eSvijenpahg.png" data-width="1234" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*N7DKKmooeM3eSvijenpahg.png"></figure><p name="9dd9" id="9dd9" class="graf graf--p graf-after--figure">this is to say that,</p><figure name="400d" id="400d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NBOkSYOE5NjhEqfxXYXPmw.png" data-width="1234" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*NBOkSYOE5NjhEqfxXYXPmw.png"></figure><p name="4247" id="4247" class="graf graf--p graf-after--figure">thus,</p><figure name="7d7f" id="7d7f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Cd4Wr7W0_a_S7CnlYQc-gw.png" data-width="1234" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*Cd4Wr7W0_a_S7CnlYQc-gw.png"></figure><p name="ded5" id="ded5" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) The Definition of the Singular Vector</strong></p><p name="ec2f" id="ec2f" class="graf graf--p graf-after--p">Recall what we have learnt for the column space. Suppose we have a vector <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">x </em></strong>and a transformation matrix <strong class="markup--strong markup--p-strong">A</strong>, then the vector <strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">x</em></strong> is a vector in the column space of <strong class="markup--strong markup--p-strong">A</strong>. Therefore, for the eigenvectors {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>r}, we can conduct <strong class="markup--strong markup--p-strong">A</strong> transformation on them and throw them into the column space of <strong class="markup--strong markup--p-strong">A</strong> and finally, we will have a basis that {<strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">v</em></strong>r}. Now we are going to claim that this is actually an orthogonal basis of the column space of <strong class="markup--strong markup--p-strong">A</strong>. How could it be possible? We have to prove our claim.</p><p name="1669" id="1669" class="graf graf--p graf-after--p">Proof:</p><p name="8de0" id="8de0" class="graf graf--p graf-after--p">We have already known that the transformation matrix <strong class="markup--strong markup--p-strong">A </strong>will throw <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong> into the column space of A, then all vectors in the set {<strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">v</em></strong>r} must be in the column space of A.</p><p name="c61f" id="c61f" class="graf graf--p graf-after--p">Also we have,</p><figure name="7574" id="7574" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tmyJnDTW8NJjTVcjXFM0IA.png" data-width="1234" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*tmyJnDTW8NJjTVcjXFM0IA.png"></figure><p name="094a" id="094a" class="graf graf--p graf-after--figure">then for all i ≠ j, we can have,</p><figure name="ee5e" id="ee5e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rB2w8gnj02n0HX26qsr9lw.png" data-width="1234" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*rB2w8gnj02n0HX26qsr9lw.png"></figure><p name="e9b3" id="e9b3" class="graf graf--p graf-after--figure">then {<strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">v</em></strong>r} is an orthogonal basis for the column space of A.</p><p name="ed15" id="ed15" class="graf graf--p graf-after--p">If we then normalize {<strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">v</em></strong>r} as,</p><figure name="a2ad" id="a2ad" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*7Ixeelh2vsSvnEIeLBo71g.png" data-width="1234" data-height="422" src="https://cdn-images-1.medium.com/max/800/1*7Ixeelh2vsSvnEIeLBo71g.png"></figure><p name="faa5" id="faa5" class="graf graf--p graf-after--figure">then the normalized vectors set {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>r} is called the set of singular vectors of <strong class="markup--strong markup--p-strong">A</strong>.</p><p name="2afc" id="2afc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) The Relationship Between the Singular Vector and the Singular Value</strong></p><p name="aae5" id="aae5" class="graf graf--p graf-after--p">Based on the fact that,</p><figure name="27fe" id="27fe" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Cd4Wr7W0_a_S7CnlYQc-gw.png" data-width="1234" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*Cd4Wr7W0_a_S7CnlYQc-gw.png"></figure><p name="1a53" id="1a53" class="graf graf--p graf-after--figure">then we can have,</p><figure name="6abf" id="6abf" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vTlvff7fQYH5fXBAoj-tdA.png" data-width="1234" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*vTlvff7fQYH5fXBAoj-tdA.png"></figure><p name="e9d8" id="e9d8" class="graf graf--p graf-after--figure">This is the basic form for i of the singular value decomposition. Now let’s write the matrix form for this.</p><p name="f32c" id="f32c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Singular Value Decomposition</strong></p><p name="57ad" id="57ad" class="graf graf--p graf-after--p">Suppose we define the <em class="markup--em markup--p-em">n</em> × <em class="markup--em markup--p-em">n</em> orthogonal matrix <strong class="markup--strong markup--p-strong">V</strong> as,</p><figure name="0fc4" id="0fc4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zDr56s0wUFOhZ8iAdlKebQ.png" data-width="1234" data-height="186" src="https://cdn-images-1.medium.com/max/800/1*zDr56s0wUFOhZ8iAdlKebQ.png"></figure><p name="c6db" id="c6db" class="graf graf--p graf-after--figure">with the column vectors <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>r, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>r+1, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>n corresponding to the eigenvalues of AᵀA as λ1 ≥ λ2 ≥ …≥ λ<em class="markup--em markup--p-em">r</em> ≥ 0 = … = 0.</p><p name="bcb6" id="bcb6" class="graf graf--p graf-after--p">Moreover, we define the <em class="markup--em markup--p-em">m</em> × <em class="markup--em markup--p-em">m</em> orthonormal matrix</p><figure name="8ee3" id="8ee3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*eygVVOw8M2-VY3FRdmRJ0A.png" data-width="1234" data-height="168" src="https://cdn-images-1.medium.com/max/800/1*eygVVOw8M2-VY3FRdmRJ0A.png"></figure><p name="4c13" id="4c13" class="graf graf--p graf-after--figure">then,</p><figure name="4655" id="4655" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*F24vCakt1w2Okjsj7jEdZg.png" data-width="1234" data-height="168" src="https://cdn-images-1.medium.com/max/800/1*F24vCakt1w2Okjsj7jEdZg.png"></figure><p name="1fe8" id="1fe8" class="graf graf--p graf-after--figure">By,</p><figure name="77a7" id="77a7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vTlvff7fQYH5fXBAoj-tdA.png" data-width="1234" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*vTlvff7fQYH5fXBAoj-tdA.png"></figure><p name="c4de" id="c4de" class="graf graf--p graf-after--figure">when <em class="markup--em markup--p-em">i </em>= 1, 2 …, <em class="markup--em markup--p-em">r </em>and</p><figure name="9065" id="9065" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YuHO0gBSzLXKW3vrwTziIg.png" data-width="1234" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*YuHO0gBSzLXKW3vrwTziIg.png"></figure><p name="b633" id="b633" class="graf graf--p graf-after--figure">otherwise.</p><p name="5379" id="5379" class="graf graf--p graf-after--p">Therefore, we can have,</p><figure name="1bb0" id="1bb0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Cscm46D7P7eAe2Y9epB2tg.png" data-width="1234" data-height="166" src="https://cdn-images-1.medium.com/max/800/1*Cscm46D7P7eAe2Y9epB2tg.png"></figure><p name="07f0" id="07f0" class="graf graf--p graf-after--figure">This is also,</p><figure name="0b6e" id="0b6e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tYR7HONnojx7eN27I6SNQQ.png" data-width="1234" data-height="166" src="https://cdn-images-1.medium.com/max/800/1*tYR7HONnojx7eN27I6SNQQ.png"></figure><p name="d788" id="d788" class="graf graf--p graf-after--figure">then,</p><figure name="2b77" id="2b77" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Dv_WcTliswPXqxgxLHRx0A.png" data-width="1234" data-height="290" src="https://cdn-images-1.medium.com/max/800/1*Dv_WcTliswPXqxgxLHRx0A.png"></figure><p name="3139" id="3139" class="graf graf--p graf-after--figure">then,</p><figure name="356a" id="356a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DPukKjbdoRYjaMmpCBjEyg.png" data-width="1234" data-height="290" src="https://cdn-images-1.medium.com/max/800/1*DPukKjbdoRYjaMmpCBjEyg.png"></figure><p name="ea73" id="ea73" class="graf graf--p graf-after--figure">if we define that,</p><figure name="3180" id="3180" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PlIZ6Wj0flpKmSBpSjxwQA.png" data-width="1234" data-height="290" src="https://cdn-images-1.medium.com/max/800/1*PlIZ6Wj0flpKmSBpSjxwQA.png"></figure><p name="0d3e" id="0d3e" class="graf graf--p graf-after--figure">then we can write,</p><figure name="8cb9" id="8cb9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FBWt0zZTo5qlC1woDKsvcw.png" data-width="1034" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*FBWt0zZTo5qlC1woDKsvcw.png"></figure><p name="f685" id="f685" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(8) The General Form of the Singular Value Decomposition</strong></p><p name="db84" id="db84" class="graf graf--p graf-after--p">Because all the vectors {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>r} is the eigenvectors of the symmetric matrix AᵀA. Therefore, it is clear that all these eigenvectors are orthogonal (we have used this conclusion before). Suppose these vectors {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>r} are also normalized then we have an orthonormal basis vectors {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>r}.</p><p name="e6dd" id="e6dd" class="graf graf--p graf-after--p">Therefore, the matrix V is an orthonormal matrix, and we can have that,</p><figure name="b6af" id="b6af" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Is63KhkBoGueLtB0jA2Ujw.png" data-width="1214" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*Is63KhkBoGueLtB0jA2Ujw.png"></figure><p name="2a4e" id="2a4e" class="graf graf--p graf-after--figure">then, for</p><figure name="e450" id="e450" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FBWt0zZTo5qlC1woDKsvcw.png" data-width="1034" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*FBWt0zZTo5qlC1woDKsvcw.png"></figure><p name="0458" id="0458" class="graf graf--p graf-after--figure">we can have that,</p><figure name="ff4e" id="ff4e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ekqx7K8lMWn7gdAoi6S2Ig.png" data-width="1214" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*Ekqx7K8lMWn7gdAoi6S2Ig.png"></figure><p name="b231" id="b231" class="graf graf--p graf-after--figure">So in conclusion, the general form of singular value decomposition is,</p><figure name="6c2a" id="6c2a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*izl2duyV5FbAANa2XP6MRg.png" data-width="1214" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*izl2duyV5FbAANa2XP6MRg.png"></figure><p name="e270" id="e270" class="graf graf--p graf-after--figure">where,</p><ul class="postList"><li name="efd0" id="efd0" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">U</strong> an <em class="markup--em markup--li-em">m</em> × <em class="markup--em markup--li-em">m</em> orthonormal matrix whose 1 ~ r columns are the basis of <em class="markup--em markup--li-em">Col</em>(A) and the r ~ n columns are a basis of N(Aᵀ).</li><li name="0d1e" id="0d1e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Σ</strong> is an <em class="markup--em markup--li-em">m</em> × <em class="markup--em markup--li-em">n </em>diagonal matrix with singular values</li><li name="ab0e" id="ab0e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">V</strong> is an <em class="markup--em markup--li-em">n</em> × <em class="markup--em markup--li-em">n</em> orthonormal matrix whose columns are the eigenvectors of the symmetric matrix AᵀA.</li></ul><p name="65f3" id="65f3" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Steps to Find a Singular Value Decomposition</strong></p><p name="0902" id="0902" class="graf graf--p graf-after--p">Suppose we are given a matrix A, how we can find its singular value decomposition?</p><ul class="postList"><li name="53a0" id="53a0" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Step #1</strong></li></ul><p name="0338" id="0338" class="graf graf--p graf-after--li">Compute AᵀA, find all the eigenvalues, and all the eigenvectors of AᵀA. Write down them in the order from the largest eigenvalue to 0, namely <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>r, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>r+1, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>n. Construct <strong class="markup--strong markup--p-strong">V</strong> by the results.</p><ul class="postList"><li name="a0f8" id="a0f8" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Step #2</strong></li></ul><p name="9236" id="9236" class="graf graf--p graf-after--li">Compute <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>r, let them be the 1 ~ r columns of the matrix <strong class="markup--strong markup--p-strong">U</strong> and then find <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>r+1, … , <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>n, to make {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>r, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>r+1, … , <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">u</em></strong>n} be a basis of ℝᵐ space. This step can be time-consuming and sometimes we have to use GS orthogonalization to get the full orthonormal basis of ℝᵐ space.</p><ul class="postList"><li name="d8ed" id="d8ed" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Step #3</strong></li></ul><p name="5f2c" id="5f2c" class="graf graf--p graf-after--li">Compute the singular values and put them in the first 1~r rows of the diagonal matrix, set zeros to the other entries.</p><p name="eb7f" id="eb7f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Pseudo Inverse</strong></p><p name="5bed" id="5bed" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of Pseudo Inverse</strong></p><p name="f426" id="f426" class="graf graf--p graf-after--p">Suppose we define A-plus that satisfies,</p><figure name="90e3" id="90e3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MQRpovJ52UVVRUrDgKudJQ.png" data-width="1214" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*MQRpovJ52UVVRUrDgKudJQ.png"></figure><p name="94ff" id="94ff" class="graf graf--p graf-after--figure">then, A-plus is called the pseudo-inverse of A, where,</p><figure name="49ed" id="49ed" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ul3VhZypXQIDMotA4OZQhw.png" data-width="1214" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*ul3VhZypXQIDMotA4OZQhw.png"></figure><p name="9cbb" id="9cbb" class="graf graf--p graf-after--figure">is a diagonal matrix with non-zero diagonal entries equal to,</p><figure name="138f" id="138f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_iyOR_Cz4_eNDNIcJAqM-w.png" data-width="1214" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*_iyOR_Cz4_eNDNIcJAqM-w.png"></figure><p name="02d3" id="02d3" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Two Facts of Σ-Plus</strong></p><p name="dd6a" id="dd6a" class="graf graf--p graf-after--p">By definition, we can have the facts that,</p><figure name="845e" id="845e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lY8a5S8KwDQ_zNb-yJ-eiA.png" data-width="1214" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*lY8a5S8KwDQ_zNb-yJ-eiA.png"></figure><p name="471c" id="471c" class="graf graf--p graf-after--figure">and,</p><figure name="8d1c" id="8d1c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*58ocwbp3KYq_tZD1paKCKw.png" data-width="1214" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*58ocwbp3KYq_tZD1paKCKw.png"></figure><p name="4ea2" id="4ea2" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) The Results of Pseudo Inverse</strong></p><p name="07d8" id="07d8" class="graf graf--p graf-after--p">We have,</p><figure name="cbe3" id="cbe3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*cXygDnjgf9Vi8cQqNZovCQ.png" data-width="1214" data-height="72" src="https://cdn-images-1.medium.com/max/800/1*cXygDnjgf9Vi8cQqNZovCQ.png"></figure><p name="8f54" id="8f54" class="graf graf--p graf-after--figure">then AA-plus is a projection matrix onto the column space of A.</p><p name="54dc" id="54dc" class="graf graf--p graf-after--p">We have,</p><figure name="4186" id="4186" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8HOCzVkf5OElKg3HTsRqRA.png" data-width="1214" data-height="72" src="https://cdn-images-1.medium.com/max/800/1*8HOCzVkf5OElKg3HTsRqRA.png"></figure><p name="b898" id="b898" class="graf graf--p graf-after--figure">then A-plusA is a projection matrix onto the row space of A.</p><p name="cea5" id="cea5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4. Principal Component Analysis</strong></p><p name="74a5" id="74a5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of Data Matrix</strong></p><p name="86ac" id="86ac" class="graf graf--p graf-after--p">Suppose we have an <em class="markup--em markup--p-em">n</em> × <em class="markup--em markup--p-em">p</em> matrix <strong class="markup--strong markup--p-strong">X</strong> who has the form of,</p><figure name="bf32" id="bf32" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WHHuIBLH6OvuHcE8t081jw.png" data-width="1098" data-height="370" src="https://cdn-images-1.medium.com/max/800/1*WHHuIBLH6OvuHcE8t081jw.png"></figure><p name="5b05" id="5b05" class="graf graf--p graf-after--figure">each column represents a different variable and each row is a single observation. Then we call this matrix X a data matrix.</p><p name="58cf" id="58cf" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Singular Value Decomposition for Data Matrix</strong></p><p name="20c1" id="20c1" class="graf graf--p graf-after--p">For a data matrix X, we can have its SVD as,</p><figure name="768c" id="768c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*GtGpX5ACHCzUhv3oKRrrdQ.png" data-width="1098" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*GtGpX5ACHCzUhv3oKRrrdQ.png"></figure><p name="2a95" id="2a95" class="graf graf--p graf-after--figure">We can also write this as,</p><figure name="30d6" id="30d6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6qQussr18pK72BXJMbO-UA.png" data-width="1098" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*6qQussr18pK72BXJMbO-UA.png"></figure><p name="4d41" id="4d41" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) PCA by a SVD Method</strong></p><p name="e2e3" id="e2e3" class="graf graf--p graf-after--p">The goal of PCA is to find the principal component of a matrix.</p><p name="6ec4" id="6ec4" class="graf graf--p graf-after--p">Because we have ordered the singular value so that,</p><figure name="3e43" id="3e43" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9BvoivjeoFoZwmV7IPUbug.png" data-width="1098" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*9BvoivjeoFoZwmV7IPUbug.png"></figure><p name="6273" id="6273" class="graf graf--p graf-after--figure">then the express in the front is more significant for the matrix X compared with later expressions (this is just a easy way to think about and definitely not a rigorous proof). Then we can reserve the first <em class="markup--em markup--p-em">p</em> dimension to get an approximation of matrix X.</p><figure name="30d5" id="30d5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KTJRd4jrrxjtTfsSL5u1Tw.png" data-width="1098" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*KTJRd4jrrxjtTfsSL5u1Tw.png"></figure><p name="5f57" id="5f57" class="graf graf--p graf-after--figure graf--trailing">By this mean, we can reduce a <em class="markup--em markup--p-em">r-</em>dimension matrix to <em class="markup--em markup--p-em">p</em> dimensions. This algorithm works well when we have small or medium datasets. For a large dataset, this is not the best way to conduct the PCA of a matrix.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/15b38e9121bc"><time class="dt-published" datetime="2020-09-27T17:48:56.404Z">September 27, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/linear-algebra-13-singular-value-decomposition-pseudo-inverse-and-principal-component-analysis-15b38e9121bc" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>