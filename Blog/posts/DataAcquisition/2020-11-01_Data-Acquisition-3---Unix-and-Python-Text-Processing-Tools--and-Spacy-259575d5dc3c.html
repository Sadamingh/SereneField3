<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Data Acquisition 3 | Unix and Python Text Processing Tools, and Spacy</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Data Acquisition 3 | Unix and Python Text Processing Tools, and Spacy</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Data Acquisition
</section>
<section data-field="body" class="e-content">
<section name="3ae2" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="eee0" id="eee0" class="graf graf--h3 graf--leading graf--title">Data Acquisition 3 | Unix and Python Text Processing Tools, and Spacy</h3><figure name="c525" id="c525" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*8RBkxz67rJqBx64ERqZ1Lw.png" data-width="1320" data-height="594" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*8RBkxz67rJqBx64ERqZ1Lw.png"></figure><ol class="postList"><li name="6ae4" id="6ae4" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Unix Text Processing Tools</strong></li></ol><p name="1eb9" id="1eb9" class="graf graf--p graf-after--li">Suppose we are given the following poem. Please store it on your computer.</p><figure name="7b3a" id="7b3a" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/simonsarris/9980a385af4f4c4d3967.js"></script></figure><p name="78b1" id="78b1" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(0) Recall: Commands we are not going to cover</strong></p><p name="44ed" id="44ed" class="graf graf--p graf-after--p">There are also many other useful Unix commands that we are not going to introduce in a way that is too specific because we have already met some of them in the command line part.</p><div name="191d" id="191d" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/adamedelwiess/play-with-the-command-lines-2d1916875184" data-href="https://medium.com/adamedelwiess/play-with-the-command-lines-2d1916875184" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/play-with-the-command-lines-2d1916875184"><strong class="markup--strong markup--mixtapeEmbed-strong">Play With the Command Lines</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Linux Commands</em>medium.com</a><a href="https://medium.com/adamedelwiess/play-with-the-command-lines-2d1916875184" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="46175eb7fe91f9a267dde692022be8a1" data-thumbnail-img-id="1*T3fyEcVhUptM9Bx0U8QguA.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*T3fyEcVhUptM9Bx0U8QguA.png);"></a></div><p name="caa2" id="caa2" class="graf graf--p graf-after--mixtapeEmbed">Here is a list of useful Unix command for text processing that we are not going to cover here, but we will assume that you can fully understand all of them.</p><ul class="postList"><li name="3755" id="3755" class="graf graf--li graf-after--p">echo</li><li name="32a7" id="32a7" class="graf graf--li graf-after--li">cat</li><li name="c2fa" id="c2fa" class="graf graf--li graf-after--li">grep</li><li name="b21f" id="b21f" class="graf graf--li graf-after--li">sort</li><li name="25d5" id="25d5" class="graf graf--li graf-after--li">uniq –c</li><li name="3835" id="3835" class="graf graf--li graf-after--li">tr</li><li name="a39d" id="a39d" class="graf graf--li graf-after--li">sed</li><li name="e793" id="e793" class="graf graf--li graf-after--li">cut</li><li name="6dc7" id="6dc7" class="graf graf--li graf-after--li">paste</li><li name="78c4" id="78c4" class="graf graf--li graf-after--li">rev</li><li name="6323" id="6323" class="graf graf--li graf-after--li">comm</li><li name="2b86" id="2b86" class="graf graf--li graf-after--li">join</li><li name="0946" id="0946" class="graf graf--li graf-after--li">shuf</li></ul><p name="2f4a" id="2f4a" class="graf graf--p graf-after--li">Please google them if you are not sure of their usage. But if there’s any difficulty for most of you to understand, please comment on this article so that we can add more references.</p><p name="2b3a" id="2b3a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) head and tail Command</strong></p><p name="2ba9" id="2ba9" class="graf graf--p graf-after--p">The head command means to show the first n lines of a file,</p><pre name="40af" id="40af" class="graf graf--pre graf-after--p">$ head -3 poem.txt</pre><p name="e64b" id="e64b" class="graf graf--p graf-after--pre">The tail command means to show the last n lines of a file,</p><pre name="31eb" id="31eb" class="graf graf--pre graf-after--p">$ tail -3 poem.txt</pre><p name="300d" id="300d" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(2) wc command</strong></p><p name="85c8" id="85c8" class="graf graf--p graf-after--p">The wc command returns the newline count, word count, and byte count of a file by sequence.</p><pre name="68a6" id="68a6" class="graf graf--pre graf-after--p">$ wc poem.txt</pre><p name="99e7" id="99e7" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(3) od command</strong></p><p name="67de" id="67de" class="graf graf--p graf-after--p">Select octal bytes of the printable characters or backslash escapes,</p><pre name="a360" id="a360" class="graf graf--pre graf-after--p">$ od -cb poem.txt</pre><p name="bc0f" id="bc0f" class="graf graf--p graf-after--pre">Select hexadecimal 2-byte units of the printable characters or backslash escapes,</p><pre name="7ada" id="7ada" class="graf graf--pre graf-after--p">$ od -cx poem.txt</pre><p name="336f" id="336f" class="graf graf--p graf-after--pre">Select hexadecimal 4-byte units of the printable characters or backslash escapes,</p><pre name="d922" id="d922" class="graf graf--pre graf-after--p">$ od -ctx poem.txt</pre><p name="dfea" id="dfea" class="graf graf--p graf-after--pre">Most Useful: Select hexadecimal 1-byte units of the printable characters or backslash escapes,</p><pre name="4cd5" id="4cd5" class="graf graf--pre graf-after--p">$ od -c -t xC poem.txt</pre><p name="bddc" id="bddc" class="graf graf--p graf-after--pre">Note that, for non-ASCII characters, instead of 1-byte per character, it takes 2 bytes for those characters to store (** means this is also for storing the character ahead). For example,</p><pre name="001c" id="001c" class="graf graf--pre graf-after--p"> á  **<br>c3  a1</pre><p name="222e" id="222e" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(4) iconv Command</strong></p><p name="a8ea" id="a8ea" class="graf graf--p graf-after--p">Suppose we only want to have the ASCII characters in a file and we want to get rid of all the non-ASCII characters, what we can do is to use the iconv command. For example,</p><pre name="e6d5" id="e6d5" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">iconv -c -f utf-8 -t ascii poem.txt</code></pre><p name="0fc8" id="0fc8" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(5) curl vs. wget Command</strong></p><p name="3640" id="3640" class="graf graf--p graf-after--p">Both the curl command and the wget command allow us to download information from an Internet server. In most cases, we can use either curl or wget,</p><pre name="611b" id="611b" class="graf graf--pre graf-after--p">$ curl &lt;url&gt; &gt; &lt;path&gt;</pre><p name="c754" id="c754" class="graf graf--p graf-after--pre">or</p><pre name="507b" id="507b" class="graf graf--pre graf-after--p">$ wget &lt;url&gt; &gt; &lt;path&gt;</pre><p name="400d" id="400d" class="graf graf--p graf-after--pre">But there is indeed some difference between them,</p><ul class="postList"><li name="d352" id="d352" class="graf graf--li graf-after--p">wget has a stronger ability to download recursively</li><li name="5854" id="5854" class="graf graf--li graf-after--li">curl supports more internet protocols</li></ul><p name="8af7" id="8af7" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Python Text Processing Tools</strong></p><p name="90ae" id="90ae" class="graf graf--p graf-after--p">Suppose we are given a list of words as variable w,</p><p name="29b3" id="29b3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Case Insensitivity</strong></p><p name="734a" id="734a" class="graf graf--p graf-after--p">This can be done by the list comprehension,</p><pre name="f970" id="f970" class="graf graf--pre graf-after--p">w = [w.lower() for w in words]</pre><p name="da8b" id="da8b" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(2) Porter Stemmer</strong></p><p name="f889" id="f889" class="graf graf--p graf-after--p">Suppose we have duplicated words in the list and they are in different forms. For example, watering and waters, city and citys. Then we have to use the porter stemmer. We have to install this package first,</p><pre name="ea22" id="ea22" class="graf graf--pre graf-after--p">$ pip install -q -U nltk</pre><p name="caf9" id="caf9" class="graf graf--p graf-after--pre">then we import the porter stemmer and create an instance,</p><pre name="087d" id="087d" class="graf graf--pre graf-after--p">from nltk.stem.porter import PorterStemmer<br>stemmer = PorterStemmer()</pre><p name="c58d" id="c58d" class="graf graf--p graf-after--pre">To make it a stemmed list, we have to add,</p><pre name="2e8f" id="2e8f" class="graf graf--pre graf-after--p">w = [stemmer.stem(w) for w in words]</pre><p name="c684" id="c684" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(3) Word Counter</strong></p><p name="d148" id="d148" class="graf graf--p graf-after--p">We can use a default dictionary to create a counter of the word frequency in the list. However, an easier way is to use a Counter. To use a counter, first of all, we have to import this,</p><pre name="9ade" id="9ade" class="graf graf--pre graf-after--p">from collections import Counter</pre><p name="9ef7" id="9ef7" class="graf graf--p graf-after--pre">then we can use Counter on our list,</p><pre name="20fd" id="20fd" class="graf graf--pre graf-after--p">ctr = Counter(w)</pre><p name="0ce1" id="0ce1" class="graf graf--p graf-after--pre">this will give us a dictionary of all the words and its frequency in the list. We can also grab the most common item by the .most_common method,</p><pre name="5ef8" id="5ef8" class="graf graf--pre graf-after--p">ctr.most_common(5)</pre><p name="3f63" id="3f63" class="graf graf--p graf-after--pre">This will give us the five most common items in the list.</p><p name="0985" id="0985" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Filter out the English stop words</strong></p><p name="a149" id="a149" class="graf graf--p graf-after--p">It is likely that our frequency dictionary is dominated by the stop words in English (i.e. the, of, etc.) and they are always useless for our following analysis. The way to filter out all those stop words is based on the library of the scikit-learn. To use this ENGLISH_STOP_WORDS library, first of all, we have to import this in Python,</p><pre name="e0bc" id="e0bc" class="graf graf--pre graf-after--p">from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS</pre><p name="d8b4" id="d8b4" class="graf graf--p graf-after--pre">We can check out this library by,</p><pre name="cd0f" id="cd0f" class="graf graf--pre graf-after--p">print(list(ENGLISH_STOP_WORDS))</pre><p name="80eb" id="80eb" class="graf graf--p graf-after--pre">To make things easier, we use the filter function to exact the words that are not in the ENGLISH_STOP_WORDS library,</p><pre name="a6e7" id="a6e7" class="graf graf--pre graf-after--p">STOP_WORDS = [stemmer.stem(item) for item in ENGLISH_STOP_WORDS]<br>w = list(filter(lambda i: i not in list(STOP_WORDS), w))</pre><p name="91f1" id="91f1" class="graf graf--p graf-after--pre">Then the Counter will not count all these stop words.</p><pre name="8b49" id="8b49" class="graf graf--pre graf-after--p">ctr = Counter(w)</pre><p name="9807" id="9807" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(5) Word Cloud</strong></p><p name="3f37" id="3f37" class="graf graf--p graf-after--p">Basically, this is not always for any actually useful meaning but it’s for fun and visualization in common sense. First of all, we have to install the word cloud package,</p><pre name="0d5e" id="0d5e" class="graf graf--pre graf-after--p">$ <code class="markup--code markup--pre-code">pip install wordcloud</code></pre><p name="af9c" id="af9c" class="graf graf--p graf-after--pre">To create a word cloud of our list, we have to import,</p><pre name="c944" id="c944" class="graf graf--pre graf-after--p">from wordcloud import WordCloud<br>import matplotlib.pyplot as plt<br>from collections import Counter</pre><p name="bd98" id="bd98" class="graf graf--p graf-after--pre">Similarly, we have to make our list a frequency dictionary in order to create the word cloud,</p><pre name="c302" id="c302" class="graf graf--pre graf-after--p">ctr = Counter(w)</pre><p name="765a" id="765a" class="graf graf--p graf-after--pre">then, we have to generate a wordcloud object with the .fit_words method, this will fit our dictionary to an image object,</p><pre name="2fbd" id="2fbd" class="graf graf--pre graf-after--p">wordcloud = WordCloud()<br>wordcloud.fit_words(ctr)</pre><p name="7e2b" id="7e2b" class="graf graf--p graf-after--pre">Finally, we are going to plot this word cloud by matplotlib with the .imshow() method,</p><pre name="4b23" id="4b23" class="graf graf--pre graf-after--p">fig=plt.figure(figsize=(6, 6))<br>plt.imshow(wordcloud)<br>plt.axis(&quot;off&quot;)<br>plt.show()</pre><p name="72ce" id="72ce" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(6) Ignore Non-ASCII Characters</strong></p><p name="d270" id="d270" class="graf graf--p graf-after--p">Suppose we want to ignore all the non-ASCII characters in a text by Python, what we can do is,</p><pre name="88d6" id="88d6" class="graf graf--pre graf-after--p">text = [c for c in text if ord(c)&lt;=127]<br>text = &#39;&#39;.join(text)</pre><p name="9741" id="9741" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">3. Spacy</strong></p><p name="1b83" id="1b83" class="graf graf--p graf-after--p">The full name of Spacy is called the Industrial-Strength Natural Language Processing (NLP), and it is a python package. It is a powerful tool for text processing.</p><p name="3243" id="3243" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Installation</strong></p><p name="bc7b" id="bc7b" class="graf graf--p graf-after--p">To install spacy, we have to use pip,</p><pre name="6d6e" id="6d6e" class="graf graf--pre graf-after--p">$ pip install spacy</pre><p name="39c0" id="39c0" class="graf graf--p graf-after--pre">then also, we have to download the English NLP model information,</p><pre name="00aa" id="00aa" class="graf graf--pre graf-after--p">$ <code class="markup--code markup--pre-code">python -m spacy download en</code></pre><p name="aa2f" id="aa2f" class="graf graf--p graf-after--pre">There are other non-English language models but we are not going to use them now. For the following part, we are going to grab a webpage from Tesla,</p><pre name="49fe" id="49fe" class="graf graf--pre graf-after--p">$ curl https://www.sec.gov/Archives/edgar/data/1318605/000119312510017054/ds1.htm &gt; /tmp/TeslaIPO.html</pre><p name="b894" id="b894" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(2) Extract Text from Html</strong></p><p name="aebf" id="aebf" class="graf graf--p graf-after--p">Recall what we have learned for beautifulsoup,</p><div name="4ae8" id="4ae8" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/adamedelwiess/data-acquisition-2-beautifulsoup-4e749f9cfb1b" data-href="https://medium.com/adamedelwiess/data-acquisition-2-beautifulsoup-4e749f9cfb1b" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/data-acquisition-2-beautifulsoup-4e749f9cfb1b"><strong class="markup--strong markup--mixtapeEmbed-strong">Data Acquisition 2 | Beautifulsoup</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Data Acquisition</em>medium.com</a><a href="https://medium.com/adamedelwiess/data-acquisition-2-beautifulsoup-4e749f9cfb1b" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="6a479fe9dde0401d52abe043c96192b3" data-thumbnail-img-id="1*Xyvyq3V41SSG1-Q2qtlIXw.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*Xyvyq3V41SSG1-Q2qtlIXw.png);"></a></div><p name="9705" id="9705" class="graf graf--p graf-after--mixtapeEmbed">It can be a good idea if we extract the text through it,</p><pre name="16ca" id="16ca" class="graf graf--pre graf-after--p">import sys<br>from bs4 import BeautifulSoup</pre><pre name="07dc" id="07dc" class="graf graf--pre graf-after--pre">def html2text(html_text):<br>    soup = BeautifulSoup(html_text, &#39;html.parser&#39;)<br>    text = soup.get_text()<br>    return text</pre><pre name="7451" id="7451" class="graf graf--pre graf-after--pre">with open(&quot;/tmp/TeslaIPO.html&quot;, &quot;r&quot;) as f:<br>    html_text = f.read()</pre><pre name="405f" id="405f" class="graf graf--pre graf-after--pre">tsla = html2text(html_text)</pre><p name="130c" id="130c" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(3) Tokenizing with Spacy</strong></p><p name="5c8a" id="5c8a" class="graf graf--p graf-after--p">To tokenize the text with Spacy, which is going to help us with more information about the text,</p><pre name="adbb" id="adbb" class="graf graf--pre graf-after--p">import spacy<br>nlp = spacy.load(&quot;en_core_web_sm&quot;)</pre><pre name="5db2" id="5db2" class="graf graf--pre graf-after--pre">doc = nlp(tsla)</pre><p name="389d" id="389d" class="graf graf--p graf-after--pre">then each of the items in the doc variable is a token.</p><p name="1c39" id="1c39" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Extract Parts of Speech</strong></p><pre name="2a33" id="2a33" class="graf graf--pre graf-after--p">import pandas as pd</pre><pre name="f928" id="f928" class="graf graf--pre graf-after--pre">winfo = []</pre><pre name="818f" id="818f" class="graf graf--pre graf-after--pre">for token in doc:<br>    winfo.append([token.text, token.pos_, token.is_stop])<br>    <br>pd.DataFrame(data=winfo, columns=[&#39;word&#39;,&#39;part of speech&#39;, &#39;stop word&#39;])</pre><p name="b4fa" id="b4fa" class="graf graf--p graf-after--pre">this will output a table with words in the file and their parts of speech, and whether or not this word is a stop word.</p><p name="1324" id="1324" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Extract Information of the Entities</strong></p><p name="ef1b" id="ef1b" class="graf graf--p graf-after--p">The entities are non-word instances in the text. This can include the year, date, number, organization name, people name, and etc. The following code will walk through all the entities in the text and also gives us information on what is the type for each of them (aka. label),</p><pre name="a200" id="a200" class="graf graf--pre graf-after--p">winfo = []</pre><pre name="6ed4" id="6ed4" class="graf graf--pre graf-after--pre">for ent in doc.ents:<br>    winfo.append([ent.text, ent.label_])</pre><pre name="0bec" id="0bec" class="graf graf--pre graf-after--pre">pd.DataFrame(data=winfo, columns=[&#39;word&#39;, &#39;label&#39;])</pre><p name="f1b6" id="f1b6" class="graf graf--p graf-after--pre">In order to see a full list of how we can interpret these labels, <a href="https://spacy.io/api/annotation" data-href="https://spacy.io/api/annotation" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a> is the documentation for reference. Note that there can be some mistakes in the labels.</p><p name="c91e" id="c91e" class="graf graf--p graf-after--p">Another quick way to show the information for the entities is to use the displacy model in the spacy,</p><pre name="69e0" id="69e0" class="graf graf--pre graf-after--p">from spacy import displacy<br>displacy.render(doc, style=&#39;ent&#39;)</pre><p name="4cc4" id="4cc4" class="graf graf--p graf-after--pre">This program will directly give us the entities and their entity labels.</p><p name="c0e2" id="c0e2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Calculate Word Vectors</strong></p><p name="ac9e" id="ac9e" class="graf graf--p graf-after--p">The word vectors can be used for interpreting how close the meaning of any two words. This can be measured by the distance of the two vectors. Spacy can directly give us the result of the word vector for each word,</p><pre name="e418" id="e418" class="graf graf--pre graf-after--p">winfo = []</pre><pre name="facc" id="facc" class="graf graf--pre graf-after--pre">for t in doc:<br>    winfo.append([t.text, t.vector])</pre><pre name="05f9" id="05f9" class="graf graf--pre graf-after--pre">pd.DataFrame(data=winfo, columns=[&#39;word&#39;, &#39;vector&#39;])</pre><p name="6928" id="6928" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(7) Split the Text Into Sentences</strong></p><p name="b150" id="b150" class="graf graf--p graf-after--p">Scapy can also be used to split the text by sentences in the text.</p><pre name="aea3" id="aea3" class="graf graf--pre graf-after--p">winfo = []</pre><pre name="552b" id="552b" class="graf graf--pre graf-after--pre">for s in doc.sents:<br>    winfo.append([s.text])</pre><pre name="4a09" id="4a09" class="graf graf--pre graf-after--pre graf--trailing">pd.DataFrame(data=winfo, columns=[&#39;sentence&#39;])</pre></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/259575d5dc3c"><time class="dt-published" datetime="2020-11-01T22:54:20.261Z">November 1, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/data-acquisition-3-unix-and-python-text-processing-tools-and-spacy-259575d5dc3c" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>