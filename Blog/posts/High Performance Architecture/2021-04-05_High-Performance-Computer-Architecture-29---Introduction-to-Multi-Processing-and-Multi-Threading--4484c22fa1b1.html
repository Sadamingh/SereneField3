<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>High-Performance Computer Architecture 29 | Introduction to Multi-Processing and Multi-Threading…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">High-Performance Computer Architecture 29 | Introduction to Multi-Processing and Multi-Threading…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: High-Performance Computer Architecture
</section>
<section data-field="body" class="e-content">
<section name="9380" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="108d" id="108d" class="graf graf--h3 graf--leading graf--title">High-Performance Computer Architecture 29 | <strong class="markup--strong markup--h3-strong">Introduction to Multi-Processing and Multi-Threading, UMA, NUMA, and SMT</strong></h3><figure name="81d5" id="81d5" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*EqsvVGhtbIXk8C2h.png" data-width="1446" data-height="864" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*EqsvVGhtbIXk8C2h.png"></figure><ol class="postList"><li name="761a" id="761a" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Introduction to Multi-Processing</strong></li></ol><p name="b3ae" id="b3ae" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Flynn’s Taxonomy of Parallel Machines</strong></p><p name="1b82" id="1b82" class="graf graf--p graf-after--p">We can distinguish different types of parallel machines according to how many instruction streams they have and how many data streams these instructions operate on.</p><ul class="postList"><li name="63b8" id="63b8" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Single Instruction, Single Data (SISD)</strong>: logically, this type of machine executes one instruction stream (single program counter), and each operation operates on one data stream. This is really a <strong class="markup--strong markup--li-strong">normal uniprocessor</strong>.</li><li name="c516" id="c516" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Single Instruction, Multiple Data (SIMD)</strong>: this type of machine executes one instruction stream, but there is more than one data stream they operate on. These are commonly so-called <strong class="markup--strong markup--li-strong">vector processors</strong>. Modern processors also have multimedia extension instruction such as <strong class="markup--strong markup--li-strong">MMX</strong> or <strong class="markup--strong markup--li-strong">SSE</strong> 1, 2, 3 and they are also an example of limited SIMD.</li><li name="5b03" id="5b03" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Multiple Instruction, Single Data (MISD)</strong>: this type is weird because we have multiple instruction streams means that there are several different programs executing at the same time, but they are all operating on the same stream of data. We don’t use this quite often but something like a <strong class="markup--strong markup--li-strong">stream processor</strong></li><li name="68f6" id="68f6" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Multiple Instruction, Multiple Data (MIMD)</strong>: this one is also common with more than 1 instruction stream and more than 1 data stream. These would be the normal processor where each processor has its own program that is running with its own program counter, and each of them operates on the same data. These are always our normal multiprocessors.</li></ul><p name="8e92" id="8e92" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Reasons for Multiprocessors</strong></p><p name="0d40" id="0d40" class="graf graf--p graf-after--p">There is one common confusion that could we just using the uniprocessor and improving its performance without using the multi-cores? The answer is that we can still do that. The reason why we have multi-cores is that there are actually some limits to the uniprocessor system.</p><p name="6f26" id="6f26" class="graf graf--p graf-after--p">One reason is that we can not improve the performance after we reach a threshold of width. Let’s suppose we have a 4-issue wide uniprocessor, which can execute up to 4 independent instructions parallelly. However, if we make this wider to 6 or 8 issues, although the performance will be improved, we will see <strong class="markup--strong markup--p-strong">diminishing returns</strong> because the dependent instructions will not be improved.</p><p name="b068" id="b068" class="graf graf--p graf-after--p">The other reason is that by Moore’s law, we have twice transistors in the same area every 18 months. To make the uniprocessor faster, we have to jack up the frequency and the voltage. Therefore, when we do this stuff, the power consumption grows up cubically and we may end up <strong class="markup--strong markup--p-strong">burning up the processor</strong>.</p><p name="2cb9" id="2cb9" class="graf graf--p graf-after--p">A better idea is to use a multicore system and approximately, it will have a performance similar to we have improved the unicore system. However, if we have only 1 program with only 1 thread, we can not benefit from the multiprocessors. So we need programs that are parallel in order to fully exploit the multiprocessor.</p><p name="bc04" id="bc04" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Downsides for Parallel Programs</strong></p><p name="fc21" id="fc21" class="graf graf--p graf-after--p">As we have talked about, if we want to benefit from the multiprocessor system, what we must have are parallel programs. However, because there are some downsides of the parallel programs,</p><ul class="postList"><li name="6642" id="6642" class="graf graf--li graf-after--p">single-threaded codes are much easier to develop than parallel programs</li><li name="875c" id="875c" class="graf graf--li graf-after--li">debugging parallel programs can be much more difficult</li><li name="4ad8" id="4ad8" class="graf graf--li graf-after--li">performance scaling can be very hard to get. This means that we need a lot more work (always years of hard works) if we want to achieve a result of a higher utility of the multiprocess resources</li></ul><p name="6214" id="6214" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Centralized Shared Memory (UMA)</strong></p><p name="0e7f" id="0e7f" class="graf graf--p graf-after--p">One type of multiprocessors is called the <strong class="markup--strong markup--p-strong">centralized shared memory</strong>. Let’s say we have 4 cores and their own caches are all connected to the same bus, so they can all access the same main memory and the same I/O devices. Different cores can share data by simply reading or writing to the main memory.</p><figure name="d9d9" id="d9d9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6X1UuruilokDZHA1ECXdTg.png" data-width="1522" data-height="448" src="https://cdn-images-1.medium.com/max/800/1*6X1UuruilokDZHA1ECXdTg.png"></figure><p name="7583" id="7583" class="graf graf--p graf-after--figure">This kind of multiprocessor is really what today’s multi-processors look like and it is also called <strong class="markup--strong markup--p-strong">uniform memory access time</strong> (aka. <strong class="markup--strong markup--p-strong">UMA</strong>) because all the cores are connected to the main memory in the same way. Interestingly, you are able to find this term on <a href="https://www.apple.com/mac/m1/" data-href="https://www.apple.com/mac/m1/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apple’s latest M1 chip</a>. This type is also called <strong class="markup--strong markup--p-strong">symmetric multiprocessor</strong> (aka. <strong class="markup--strong markup--p-strong">SMP</strong>).</p><p name="2a04" id="2a04" class="graf graf--p graf-after--p">However, there are some problems related to the centralized shared memory,</p><ul class="postList"><li name="35ac" id="35ac" class="graf graf--li graf-after--p">we have to keep a <strong class="markup--strong markup--li-strong">larger memory</strong> size: because more processes can be running at the same time and they are likely to contain different data. This also makes the memory access/read/write <strong class="markup--strong markup--li-strong">slow</strong>.</li><li name="2a57" id="2a57" class="graf graf--li graf-after--li">we have to improve the <strong class="markup--strong markup--li-strong">memory bandwidth</strong>: because now we have multiprocessors, we can have more access to the main memory in a time. Therefore, we must maintain a wider memory bandwidth for a larger flow of accesses or these accesses will have to wait in a queue and the multiprocessors are now <strong class="markup--strong markup--li-strong">serialized</strong>. This prevents an additional core from benefiting our performance.</li></ul><p name="2eb2" id="2eb2" class="graf graf--p graf-after--li">With these problems, the centralized shared memory has this problem so they can work only for smaller machines with 2, 4, 8, or 16 cores, and also our memory needs to be not too big and too slow.</p><p name="d69e" id="d69e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Distributed Shared Memory (NUMA)</strong></p><p name="2bbf" id="2bbf" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">distributed shared memory</strong> (aka. <strong class="markup--strong markup--p-strong">non-uniform memory access time</strong>, or <strong class="markup--strong markup--p-strong">non-uniform memory access</strong>, or <strong class="markup--strong markup--p-strong">NUMA</strong>)is another type of multiprocessors, which means that instead of caring about the shared memory problem, we can have memories assigned to each core. It has the term “shared” but this type has no relation to real shared memory. So there is no memory that is shared by all the cores. Different cores will connect with each other using a <strong class="markup--strong markup--p-strong">network interface card</strong> (aka. <strong class="markup--strong markup--p-strong">NIC</strong>) that connects to a network.</p><figure name="7a20" id="7a20" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zkM0X2gzCaN27YMeKzmo7Q.png" data-width="1522" data-height="926" src="https://cdn-images-1.medium.com/max/800/1*zkM0X2gzCaN27YMeKzmo7Q.png"></figure><p name="0b73" id="0b73" class="graf graf--p graf-after--figure">So now if two cores want to communicate with each other, they will act like independent machines that communicate through a network. One core needs to send the message to the network and another needs to properly receive the message from the network. This technique is actually called <strong class="markup--strong markup--p-strong">message passing</strong>. The only difference to the real network communication is that the NICs are much faster. For this reason, this type of multi-processor is also called a <strong class="markup--strong markup--p-strong">multi-computer </strong>or<strong class="markup--strong markup--p-strong"> cluster computer</strong>.</p><p name="3c44" id="3c44" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) UMA Vs. NUMA</strong></p><p name="811d" id="811d" class="graf graf--p graf-after--p">Now, let’s summarize the difference between message passing (NUMA) and the shared memory (UMA).</p><ul class="postList"><li name="ebc4" id="ebc4" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Communication</strong>: for message passing, the programmer is responsible for figuring out who sends/receives data. While for shared memory, the OS will automatically figure everything out</li><li name="18ab" id="18ab" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Data Distribution</strong>: for message passing, the memory is distributed. While for shared memory, the system is responsible for sending the data where it is needed</li><li name="7188" id="7188" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Hardware Support</strong>: for message passing, the HW support is relatively simple because what we have to implement is just NICs. However, for shared memory, we need extensive hardware support that automatically figures out when to send the data, to whom, etc.</li><li name="0244" id="0244" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Programming Correctness</strong>: getting correctness is relatively difficult in message passing and you can get all sorts of deadlock issues, wrong distributions, etc. But you don’t have to think about synchronization. For shared memory, although we also have to conduct synchronization, it is also much simpler than the message passing</li><li name="4282" id="4282" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Program Performance</strong>: the performance is difficult to get for message passing, but is even more difficult to get for shared memory. We have to make a lot of efforts before the program performs well</li></ul><p name="1dff" id="1dff" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Introduction to Multithreading</strong></p><p name="1385" id="1385" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Types of Shared Memory Hardware</strong></p><p name="3e41" id="3e41" class="graf graf--p graf-after--p">We have different types of shared memory hardware, and these include,</p><ul class="postList"><li name="bf70" id="bf70" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Multiple single cores</strong> to any memory address: UMA or NUMA</li><li name="0113" id="0113" class="graf graf--li graf-after--li">A <strong class="markup--strong markup--li-strong">single core with multithreading</strong>: different threads will time sharing the same core. Different threads actually have the same physical memory.</li><li name="483c" id="483c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Hardware multithreading</strong> in one core: the hardware multithreading can be either a <strong class="markup--strong markup--li-strong">coarse-grain</strong> (means changing the threads every few cycles) or a <strong class="markup--strong markup--li-strong">fine-grain</strong> (means to change threads every cycle). We can also have <strong class="markup--strong markup--li-strong">simultaneous multithreading</strong> (aka. <strong class="markup--strong markup--li-strong">SMT</strong>, or <strong class="markup--strong markup--li-strong">Hyper-threading</strong>), which means that in any given cycle, we could be doing instructions that belong to different threads.</li></ul><p name="9855" id="9855" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) The Performance of Multithreading</strong></p><p name="477e" id="477e" class="graf graf--p graf-after--p">Now, let’s see why multithreading can improve performance. Suppose we have a 4-issue wide single-threading unicore processor and the green, red, and blue blocks mean instructions from different processes. Then suppose we have the following program with three processes and they are scheduled as follows,</p><figure name="400e" id="400e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FafLgWIdbOJ12RIid6_pCg.png" data-width="1522" data-height="348" src="https://cdn-images-1.medium.com/max/800/1*FafLgWIdbOJ12RIid6_pCg.png"></figure><p name="9e63" id="9e63" class="graf graf--p graf-after--figure">If we have hardware multithreading with fine-grain, which means we can do context switches between different threads to fill the blanks. Note that we have to pay an extra cost (like x1.05) if we want to implement this. Then, we are going to have,</p><figure name="d55a" id="d55a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VhhTD2xTUhleCUzY5vsR6w.png" data-width="1522" data-height="294" src="https://cdn-images-1.medium.com/max/800/1*VhhTD2xTUhleCUzY5vsR6w.png"></figure><p name="278b" id="278b" class="graf graf--p graf-after--figure">Moreover, there are still some blanks we can fill. And the SMT can be used to fill these blanks. Although we have to pay a higher cost (like x1.5) for the hardware, the performance will be better.</p><figure name="972f" id="972f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*htiMR16PsAer-ZowRxNSUA.png" data-width="1522" data-height="294" src="https://cdn-images-1.medium.com/max/800/1*htiMR16PsAer-ZowRxNSUA.png"></figure><p name="93bf" id="93bf" class="graf graf--p graf-after--figure">There are still some blanks but we can not fill them up because these blanks are actually caused by the true data dependencies.</p><p name="fac6" id="fac6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) SMT Hardware Changes</strong></p><p name="ebc3" id="ebc3" class="graf graf--p graf-after--p">SMT is not much more expensive than a UMA or Fine-Grained Multi­threaded core. It only changes the following parts,</p><ul class="postList"><li name="9b05" id="9b05" class="graf graf--li graf-after--p">Add a program counter</li><li name="e787" id="e787" class="graf graf--li graf-after--li">Add a RAT</li><li name="2ba8" id="2ba8" class="graf graf--li graf-after--li">Add architectural registers</li></ul><p name="d587" id="d587" class="graf graf--p graf-after--li">It will not add ROB because commonly, a ROB is large enough to store all the information we need.</p><p name="62a6" id="62a6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) SMT, Data Cache, and TLB</strong></p><p name="d18f" id="d18f" class="graf graf--p graf-after--p">We can not use a VIVT machine for SMT because with a VIVT machine SMT will lead to the wrong data being used. However, with a VIPT and a PIPT machine, the TLB must know which thread belongs to which processor.</p><p name="28fc" id="28fc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) SMT and Cache Performance</strong></p><p name="6306" id="6306" class="graf graf--p graf-after--p graf--trailing">The cache of the core is shared by all the SMT threads that are currently active on it. The advantage is that we can achieve <strong class="markup--strong markup--p-strong">fast data sharing</strong>. The downside is that the cache capacity is also shared by also the threads so that if the working sets of all the currently active threads exceed the cache size, we will have a <strong class="markup--strong markup--p-strong">cache flushing</strong> problem and this problem results in lots of cache misses.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/4484c22fa1b1"><time class="dt-published" datetime="2021-04-05T04:11:18.204Z">April 5, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/high-performance-computer-architecture-29-introduction-to-multi-processing-and-multi-threading-4484c22fa1b1" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>