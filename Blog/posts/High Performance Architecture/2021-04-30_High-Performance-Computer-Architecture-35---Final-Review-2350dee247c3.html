<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>High-Performance Computer Architecture 35 | Final Review</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">High-Performance Computer Architecture 35 | Final Review</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: High-Performance Computer Architecture
</section>
<section data-field="body" class="e-content">
<section name="b90c" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="c84b" id="c84b" class="graf graf--h3 graf--leading graf--title">High-Performance Computer Architecture 35 | FinalÂ Review</h3><figure name="b92b" id="b92b" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*cNdk66i_0WzKJDv0.png" data-width="1446" data-height="864" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*cNdk66i_0WzKJDv0.png"></figure><ol class="postList"><li name="07b7" id="07b7" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Cache</strong></li></ol><p name="8de6" id="8de6" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Locality</strong></p><ul class="postList"><li name="9f25" id="9f25" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Temporal Locality</strong>: If an address has been accessed recently, it will likely be accessed again</li><li name="1239" id="1239" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Spatial Locality</strong>: If an address has been accessed, it is likely that addresses close to it will be accessed</li></ul><p name="071b" id="071b" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) AMAT</strong></p><figure name="8d94" id="8d94" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*H3EdxGR66b6jryoA.png" data-width="1356" data-height="68" src="https://cdn-images-1.medium.com/max/800/0*H3EdxGR66b6jryoA.png"></figure><p name="7ee9" id="7ee9" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Miss Time</strong></p><figure name="1d2a" id="1d2a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*YzSDAtAXh8B_6KF3.png" data-width="1262" data-height="56" src="https://cdn-images-1.medium.com/max/800/0*YzSDAtAXh8B_6KF3.png"></figure><p name="fbe5" id="fbe5" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) General AMAT</strong></p><figure name="13d7" id="13d7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*ir1HunarbcOvK7Fn.png" data-width="1262" data-height="56" src="https://cdn-images-1.medium.com/max/800/0*ir1HunarbcOvK7Fn.png"></figure><p name="af0f" id="af0f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Block Size</strong></p><p name="4edf" id="4edf" class="graf graf--p graf-after--p">Block size is the amount of data stored for each memory address.</p><p name="4b2d" id="4b2d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Block Number and Block Offset</strong></p><p name="a676" id="a676" class="graf graf--p graf-after--p">The memory address can be divide into two parts, the block number, and the block offset.</p><ul class="postList"><li name="3a7e" id="3a7e" class="graf graf--li graf-after--p">Block number is used to determine which block in the cache should be accessed.</li><li name="8d7a" id="8d7a" class="graf graf--li graf-after--li">Block offset is used to determine which part of a block should be the data of that memory address.</li></ul><figure name="06a9" id="06a9" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*g9j6_Xl2kQojom0wDeAGLA.png" data-width="1378" data-height="154" src="https://cdn-images-1.medium.com/max/800/1*g9j6_Xl2kQojom0wDeAGLA.png"></figure><p name="2497" id="2497" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(7) Tags</strong></p><p name="85b5" id="85b5" class="graf graf--p graf-after--p">Tags are used to determine which block should be accessed, it must contain at least one bit from the block number.</p><p name="f6e1" id="f6e1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Valid Bit</strong></p><p name="dee6" id="dee6" class="graf graf--p graf-after--p">The valid bit should be used in the cache for showing the validity of the cached data.</p><p name="bb77" id="bb77" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) Directed Mapped Cache (DMC)</strong></p><p name="ff14" id="ff14" class="graf graf--p graf-after--p">More than one memory block will be mapped to the same cache line.</p><ul class="postList"><li name="8202" id="8202" class="graf graf--li graf-after--p">Block offset is the data location in the cache line</li><li name="e69f" id="e69f" class="graf graf--li graf-after--li">Index decides which cache line we will direct to</li><li name="ff8f" id="ff8f" class="graf graf--li graf-after--li">Tag finds out if this line matches the current memory block</li></ul><figure name="6bb5" id="6bb5" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*GNcyy4zKm8ShTrVr.png" data-width="1328" data-height="232" src="https://cdn-images-1.medium.com/max/800/0*GNcyy4zKm8ShTrVr.png"></figure><p name="52c2" id="52c2" class="graf graf--p graf-after--figure">But this DM cache will result in a higher cache miss rate.</p><p name="e5f2" id="e5f2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) Cache Miss</strong></p><p name="bb2c" id="bb2c" class="graf graf--p graf-after--p">Because more than one block can be mapped to the same cache line, the previous data would be kicked out from the cache and thus we will have a cache miss.</p><p name="b358" id="b358" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(11) Set Associative Cache (SAC)</strong></p><p name="afbb" id="afbb" class="graf graf--p graf-after--p">In the set-associative cache, the cache lines are divided into several sets and each set has a bunch of cache lines. This means that more than one memory block will be mapped to the same cache set.</p><ul class="postList"><li name="1dec" id="1dec" class="graf graf--li graf-after--p">Block offset is the data location in the cache line</li><li name="822e" id="822e" class="graf graf--li graf-after--li">Index decides which cache set we will direct to</li><li name="2aa8" id="2aa8" class="graf graf--li graf-after--li">Tag finds out if this line matches the current memory block</li></ul><p name="7e09" id="7e09" class="graf graf--p graf-after--li">If we have N cache lines in a set, then this SAC is called an <strong class="markup--strong markup--p-strong">N-way set-associative cache</strong>. By the way, the DMC can be treated as an extreme case of <strong class="markup--strong markup--p-strong">1-way set-associative cache</strong>. When we have only a single set in the cache, and this means that each memory block can be mapped into any cache line, this SAC is called a <strong class="markup--strong markup--p-strong">fully set-associative cache</strong>.</p><p name="b623" id="b623" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(12) Cache Replacement Policies</strong></p><p name="1525" id="1525" class="graf graf--p graf-after--p">When the set is full, we have to kick out a block in a cache line to make room for the new one. The cache replacement policies are the methods for choosing the block to kick out.</p><ul class="postList"><li name="3141" id="3141" class="graf graf--li graf-after--p">Random</li><li name="3ec2" id="3ec2" class="graf graf--li graf-after--li">FIFO: track the oldest one</li><li name="716c" id="716c" class="graf graf--li graf-after--li">least recently used (LRU): add LRU counter for each cache line</li><li name="f82b" id="f82b" class="graf graf--li graf-after--li">not most recently used (NMRU): track only the MRU block, and randomly kicks out one of the NMRU lines</li><li name="c64a" id="c64a" class="graf graf--li graf-after--li">Pseudo LRU (PLRU): PLRU counters start with zeros, change to 1 if a line is recently used. Randomly kick out a 0 counter line, if all 1, then zero out all the PLRU counters.</li></ul><p name="bb90" id="bb90" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(13) LRU Implementation</strong></p><p name="9b8a" id="9b8a" class="graf graf--p graf-after--p">For the value in the LRU counter,</p><ul class="postList"><li name="4a97" id="4a97" class="graf graf--li graf-after--p">LRU = 0</li><li name="b5b5" id="b5b5" class="graf graf--li graf-after--li">MRU = N, which is the number of cache lines in this set</li></ul><p name="e2ff" id="e2ff" class="graf graf--p graf-after--li">We have to update the LRU counters every we access a set.</p><p name="3e92" id="3e92" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(14) LRU Cost</strong></p><p name="991c" id="991c" class="graf graf--p graf-after--p">For an N-way set-associative cache, the cost of each LRU counter should be <code class="markup--code markup--p-code">log2(N)</code> bits, and thus, the cost for each set should be <code class="markup--code markup--p-code">Nlog2(N)</code> bits.</p><p name="c4ef" id="c4ef" class="graf graf--p graf-after--p">If there are M sets in an N-way set-associative cache, then the general cost for maintaining the LRU policies should be <code class="markup--code markup--p-code">MNlog2(N)</code> bits.</p><p name="b363" id="b363" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(15) Write-Allocate Cache</strong></p><p name="5460" id="5460" class="graf graf--p graf-after--p">Most caches today are <strong class="markup--strong markup--p-strong">write-allocated caches</strong>. This means that the block is written to the cache after each writes if we have a cache miss. This is because we want to benefit from the locality.</p><p name="a604" id="a604" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(16) Non-Write-Allocate Cache</strong></p><p name="d8da" id="d8da" class="graf graf--p graf-after--p">If the block is not written to the cache after each writes, we will result in a non-write-allocate cache.</p><p name="f055" id="f055" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(17) Write-Through Cache</strong></p><p name="f2a8" id="f2a8" class="graf graf--p graf-after--p">If the main memory is updated immediately when the cache is updated, then we will have a write-through cache. This is unpopular because the cost and traffic on the bus will be high.</p><p name="499c" id="499c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(18) Write-Back Cache</strong></p><p name="3ebb" id="3ebb" class="graf graf--p graf-after--p">In a write-back cache, the block is only written to the memory when the block is replaced from the cache.</p><p name="6cb9" id="6cb9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(19) Pipelined Cache</strong></p><p name="7a8a" id="7a8a" class="graf graf--p graf-after--p">The pipelined cache means that we will separate the cache hit into stages so as to overlap with another cache hit. There are three stages for a cache hit,</p><ul class="postList"><li name="081d" id="081d" class="graf graf--li graf-after--p">reading the set index</li><li name="b1fe" id="b1fe" class="graf graf--li graf-after--li">determining cache hit and start data read</li><li name="0666" id="0666" class="graf graf--li graf-after--li">finish data read</li></ul><p name="ce0f" id="ce0f" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(20) VIVT Cache</strong></p><p name="707a" id="707a" class="graf graf--p graf-after--p">If the virtual address is used by the cache, then it is called a virtually accessed cache, or a virtually indexed, virtually tagged cache (or VIVT cache). Here are some features,</p><ul class="postList"><li name="6539" id="6539" class="graf graf--li graf-after--p">the virtual page number is used for tagging</li><li name="a312" id="a312" class="graf graf--li graf-after--li">the virtual page number is used for cache indexing</li><li name="8aa4" id="8aa4" class="graf graf--li graf-after--li">the virtual page number is used for TLB indexing</li></ul><figure name="8473" id="8473" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*2dvRbrU95b2MEhjw.png" data-width="2064" data-height="1208" src="https://cdn-images-1.medium.com/max/800/0*2dvRbrU95b2MEhjw.png"></figure><p name="5f52" id="5f52" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(21) PIPT Cache</strong></p><p name="83cf" id="83cf" class="graf graf--p graf-after--p">A cache that uses the physical addresses is called a physically indexed, physically tagged cache (or PIPT cache or physically accessed cache). Here are some features,</p><ul class="postList"><li name="8dc1" id="8dc1" class="graf graf--li graf-after--p">the physical page number is used for tagging</li><li name="4539" id="4539" class="graf graf--li graf-after--li">the physical page number is used for cache indexing</li><li name="818e" id="818e" class="graf graf--li graf-after--li">the virtual page number is used for TLB indexing</li></ul><figure name="ddff" id="ddff" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*6Xe-pMnMOF8IsatB.png" data-width="2090" data-height="1276" src="https://cdn-images-1.medium.com/max/800/0*6Xe-pMnMOF8IsatB.png"></figure><p name="e93c" id="e93c" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(22) VIPT Cache</strong></p><p name="198b" id="198b" class="graf graf--p graf-after--p">The VIPT cache is a combination of the VIVT and PIPT caches, which has the following features,</p><ul class="postList"><li name="2ee7" id="2ee7" class="graf graf--li graf-after--p">the physical page number is used for tagging</li><li name="89d5" id="89d5" class="graf graf--li graf-after--li">the virtual page number is used for cache indexing</li><li name="1343" id="1343" class="graf graf--li graf-after--li">the virtual page number is used for TLB indexing</li></ul><figure name="c9aa" id="c9aa" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*HuvxMTfqaQu2qasB.png" data-width="2090" data-height="1276" src="https://cdn-images-1.medium.com/max/800/0*HuvxMTfqaQu2qasB.png"></figure><p name="aee7" id="aee7" class="graf graf--p graf-after--figure">This cache is designed to conduct the cache lookup and the V2P translation at the same time. Also, the same physically tagged can be mapped to the same cache block and the cost for storage can be reduced.</p><p name="e37d" id="e37d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(23) Calculating VIPT L1 Cache Size</strong></p><ul class="postList"><li name="6fed" id="6fed" class="graf graf--li graf-after--p">Page size -&gt; Page offset bits</li><li name="07e3" id="07e3" class="graf graf--li graf-after--li">Block size -&gt; Block offset bits</li><li name="e936" id="e936" class="graf graf--li graf-after--li">Index size bits = Page offset bits - Block offset bits</li><li name="f116" id="f116" class="graf graf--li graf-after--li">Set Number = 2^Index size bits</li><li name="090b" id="090b" class="graf graf--li graf-after--li">General Size = Set Number * Block size * Cache lines per set</li></ul><p name="a6ad" id="a6ad" class="graf graf--p graf-after--li">As a quick calculation, we can use the formula,</p><figure name="2302" id="2302" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*cLuJqSYzTi1nMtmM.png" data-width="1416" data-height="62" src="https://cdn-images-1.medium.com/max/800/0*cLuJqSYzTi1nMtmM.png"></figure><p name="1ee6" id="1ee6" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(24) Way Prediction</strong></p><p name="8c7f" id="8c7f" class="graf graf--p graf-after--p">With way prediction, the processor tries to <strong class="markup--strong markup--p-strong">guess</strong> which set is most likely to be a hit. All kinds of SACs can benefit from this method but the DMC does not.</p><p name="6239" id="6239" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(25) Types of Cache Misses</strong></p><ul class="postList"><li name="b63f" id="b63f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Compulsory Miss</strong>: when a block is accessed for the first time, and this will happen even for an infinite cache</li><li name="6d9f" id="6d9f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Capacity Miss</strong>: blocks evicted when the cache is full. This is because we have a limited cache size.</li><li name="b22f" id="b22f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Conflict Miss</strong>: blocks are evicted due to associativity. This is because we have limited associativity.</li></ul><p name="6735" id="6735" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(26) Prefetching</strong></p><p name="ddc1" id="ddc1" class="graf graf--p graf-after--p">Prefetching means that we will prefetch blocks into the cache to improve the cache miss rate.</p><p name="8d9b" id="8d9b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(27) Cache Pollution</strong></p><p name="7664" id="7664" class="graf graf--p graf-after--p">When we make some bad prefetching guesses, we will result in cache pollution because we prefetch the stuff that is not useful in the future.</p><p name="dd69" id="dd69" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(28) Prefetching Methods</strong></p><p name="849d" id="849d" class="graf graf--p graf-after--p">We can use different methods for prefetching,</p><ul class="postList"><li name="aec3" id="aec3" class="graf graf--li graf-after--p">software prefetching: This means the compiler will add prefetching instructions</li><li name="3e45" id="3e45" class="graf graf--li graf-after--li">stream buffer hardware prefetching: This means simply prefetch next blocks</li><li name="db97" id="db97" class="graf graf--li graf-after--li">stride hardware prefetching: This means it will prefetch the blocks with a fixed distance</li><li name="47c1" id="47c1" class="graf graf--li graf-after--li">correlating hardware prefetching: This means it will prefetch the blocks based on some correlating rules</li></ul><p name="2ca6" id="2ca6" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(29) Loop Interchange</strong></p><p name="abe2" id="abe2" class="graf graf--p graf-after--p">This means reorganizing the loop in order to benefit from the spatial locality. This is done by the compiler.</p><p name="5400" id="5400" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(30) Overlap Misses</strong></p><p name="0dbe" id="0dbe" class="graf graf--p graf-after--p">Using non-blocking that supports hit-under-miss and miss-under-miss instead of blocking cache.</p><p name="3813" id="3813" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(31) Miss Status Handling Registers (MSHRs)</strong></p><p name="80a9" id="80a9" class="graf graf--p graf-after--p">MSHRs are the registers to determine if we have a true miss or if we have a half-miss (means that we have a miss but the data is already loaded by some other instruction, and the loading process does not finish). Here are some features,</p><ul class="postList"><li name="d908" id="d908" class="graf graf--li graf-after--p">MSHR is released when the data arrives from the memory.</li><li name="cbd6" id="cbd6" class="graf graf--li graf-after--li">Normally we would have 16â32 MSHRs</li></ul><p name="f05e" id="f05e" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(32) AMAT and Cache Hierarchy</strong></p><p name="cf4b" id="cf4b" class="graf graf--p graf-after--p">This means that we can have multi-level caches. The miss penalty for each level will be,</p><figure name="6d2e" id="6d2e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*xtA0GZnoq3r5TJHX.png" data-width="1508" data-height="58" src="https://cdn-images-1.medium.com/max/800/0*xtA0GZnoq3r5TJHX.png"></figure><p name="6d81" id="6d81" class="graf graf--p graf-after--figure">And the LLC (last level cache) AMAT should be,</p><figure name="37ce" id="37ce" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*6kBxG797hL_UVxE-.png" data-width="1508" data-height="58" src="https://cdn-images-1.medium.com/max/800/0*6kBxG797hL_UVxE-.png"></figure><p name="35c7" id="35c7" class="graf graf--p graf-after--figure">Therefore, the general AMAT will be,</p><figure name="1285" id="1285" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*r6hOID7r6WjlMGpGMMbuJw.png" data-width="1604" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*r6hOID7r6WjlMGpGMMbuJw.png"></figure><p name="b985" id="b985" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(33) Global Miss Rate of a Cache</strong></p><p name="47dd" id="47dd" class="graf graf--p graf-after--p">Memory references == Memory accesses,</p><figure name="2c32" id="2c32" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*rrlJ-53PNP7gP3-R.png" data-width="1248" data-height="108" src="https://cdn-images-1.medium.com/max/800/0*rrlJ-53PNP7gP3-R.png"></figure><p name="083d" id="083d" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(34) Local Miss Rate of a Cache</strong></p><figure name="b158" id="b158" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*3pZ9kfZRYipXNm3k.png" data-width="1126" data-height="102" src="https://cdn-images-1.medium.com/max/800/0*3pZ9kfZRYipXNm3k.png"></figure><p name="1e7a" id="1e7a" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(35) Local Hit Rate of a Cache</strong></p><figure name="011b" id="011b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*vKbZLGtNZkvDK8Yd.png" data-width="1248" data-height="108" src="https://cdn-images-1.medium.com/max/800/0*vKbZLGtNZkvDK8Yd.png"></figure><p name="ad39" id="ad39" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(36) Misses Per Thousand Instructions (MPKI)</strong></p><p name="91e7" id="91e7" class="graf graf--p graf-after--p">This is another metric for the number of misses per 1,000 instructions.</p><p name="83b2" id="83b2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(37) Inclusion Property</strong></p><p name="c57e" id="c57e" class="graf graf--p graf-after--p">The same block can be in or not in different level of caches,</p><ul class="postList"><li name="f502" id="f502" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Inclusion</strong>: means that this block is also in the L2 cache</li><li name="aff4" id="aff4" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Exclusion</strong>: means that this block is not in the L2 cache</li><li name="b1a1" id="b1a1" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Random</strong>: means that this block may or may not be in the L2 cache</li></ul><p name="56aa" id="56aa" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(38) Summary: Methods for Reducing Hit Time</strong></p><ul class="postList"><li name="fc16" id="fc16" class="graf graf--li graf-after--p">overlap cache hits by Pipelined caches</li><li name="e705" id="e705" class="graf graf--li graf-after--li">overlap cache hit with TLB hit by VIPT</li><li name="cd00" id="cd00" class="graf graf--li graf-after--li">guess the hit set by way prediction</li><li name="7c7f" id="7c7f" class="graf graf--li graf-after--li">implement replacement policies like NMRU and PLRU</li></ul><p name="b5cb" id="b5cb" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(39) Summary: Methods for Reducing Miss Rate</strong></p><ul class="postList"><li name="cfd9" id="cfd9" class="graf graf--li graf-after--p">Having large cache blocks to benefit from spatial locality</li><li name="38bf" id="38bf" class="graf graf--li graf-after--li">Prefetch blocks to reduce future cache misses</li><li name="9663" id="9663" class="graf graf--li graf-after--li">Reorganize loops to benefit from locality by loop interchange</li><li name="56be" id="56be" class="graf graf--li graf-after--li">overlap the hits and misses by non-blocking cache and MSHRs</li></ul><p name="a202" id="a202" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(40) Summary: Methods for Reducing Miss Penalty</strong></p><ul class="postList"><li name="ff38" id="ff38" class="graf graf--li graf-after--p">reduce the miss penalty overall by having cache hierarchies</li></ul><p name="0cdc" id="0cdc" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Example 1-1. SA Cache, Index, and Offsets</strong></p><p name="7533" id="7533" class="graf graf--p graf-after--p">The processor has a 32Kbyte, 32-way set associative, write-back, write-allocate, virtually indexed, physically-tagged L1 cache with 64-byte blocks. The system uses 32-bit virtual and physical addresses, with 4K byte pages.</p><p name="0865" id="0865" class="graf graf--p graf-after--p">a. Can we overlap virtual-to-physical address translation with the L1 cache access? Explain your answer.</p><p name="7a26" id="7a26" class="graf graf--p graf-after--p">b. Each time the processor accesses the cache, what is the number of tag comparisons that are needed to determine if the access is a cache hit?</p><p name="978f" id="978f" class="graf graf--p graf-after--p">c. If the processor reads memory at virtual address 0x00001234 (physical address 0xFFFF1234), what is the index that is used to access the cache?</p><p name="c471" id="c471" class="graf graf--p graf-after--p">d. If the processor reads memory at virtual address 0x00001234 (physical address 0xFFFF1234), what is the tag that is used by the cache to determine if the access is a cache hit?</p><p name="ba02" id="ba02" class="graf graf--p graf-after--p">Solution:</p><p name="431b" id="431b" class="graf graf--p graf-after--p">Summary of the cache is,</p><ul class="postList"><li name="4a1c" id="4a1c" class="graf graf--li graf-after--p">32 KB size</li><li name="a0b8" id="a0b8" class="graf graf--li graf-after--li">32-way SAC</li><li name="069c" id="069c" class="graf graf--li graf-after--li">write allocate and write back</li><li name="bb78" id="bb78" class="graf graf--li graf-after--li">VIPT</li><li name="b469" id="b469" class="graf graf--li graf-after--li">64-byte blocks: means a 6-bit block offset</li><li name="ca73" id="ca73" class="graf graf--li graf-after--li">4Kb page: means a 2-bit page offset</li><li name="d723" id="d723" class="graf graf--li graf-after--li">Index bits: 32KB / 32 way / 64 Byte = 16 sets, means we have 4 bits for index</li></ul><p name="e40d" id="e40d" class="graf graf--p graf-after--li">a. Yes. Because the block offset bits plus the number of index bits is less than the number of the page offset bits, the index can be obtained from the physical address, so overlap of address translation and cache access can occur.</p><p name="0a6f" id="0a6f" class="graf graf--p graf-after--p">b. 32, because we have a 32-way SAC means that there will be 32 block tags we need to compare</p><p name="7234" id="7234" class="graf graf--p graf-after--p">c. The page offset should be,</p><pre name="b580" id="b580" class="graf graf--pre graf-after--p">0x1234</pre><p name="7f6f" id="7f6f" class="graf graf--p graf-after--pre">The binary page offset should be,</p><pre name="f33d" id="f33d" class="graf graf--pre graf-after--p">0001 0010 0011 0100</pre><p name="d515" id="d515" class="graf graf--p graf-after--pre">The block offset should be the last 6 bits,</p><pre name="a215" id="a215" class="graf graf--pre graf-after--p">110100</pre><p name="ab6e" id="ab6e" class="graf graf--p graf-after--pre">And the index should be,</p><pre name="4dfe" id="4dfe" class="graf graf--pre graf-after--p">1000</pre><p name="9a25" id="9a25" class="graf graf--p graf-after--pre">In hexadecimal format, the index should be,</p><pre name="541a" id="541a" class="graf graf--pre graf-after--p">0x8</pre><p name="951a" id="951a" class="graf graf--p graf-after--pre">d. Except for the index and the block offset, the rest should be the tag. Because we have a VIPT cache, we need to generate tags based on the physical address, so the tag should be</p><pre name="14b9" id="14b9" class="graf graf--pre graf-after--p">1111 1111 1111 1111 0001 00</pre><p name="ced7" id="ced7" class="graf graf--p graf-after--pre">And this is also,</p><pre name="fd29" id="fd29" class="graf graf--pre graf-after--p">0011 1111 1111 1111 1100 0100</pre><p name="9ffb" id="9ffb" class="graf graf--p graf-after--pre">In hexadecimal format, the tag should be,</p><pre name="042f" id="042f" class="graf graf--pre graf-after--p">0x3FFFC4</pre><p name="47b9" id="47b9" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Example 1-2. SA Cache, Index, and Offsets</strong></p><p name="025b" id="025b" class="graf graf--p graf-after--p">Consider an 8-way set-associative 32KB, VIPT, write-back, write-allocate L1 data cache with an LRU replacement policy. Each block has a size of 32 bytes and the system uses 32-bit virtual and 56-bit physical addresses. The bits of an address are numbered from 0 (least-significant bit) to 31 (or 55 for physical address).</p><p name="6763" id="6763" class="graf graf--p graf-after--p">a. When accessing this cache, different parts of the virtual and physical address are used at different times.</p><p name="0648" id="0648" class="graf graf--p graf-after--p">First, we use the ___________ (bits ___ through ____) from the ______________ address to ______________________________.</p><p name="4132" id="4132" class="graf graf--p graf-after--p">Second, we use the _________ (bits ___ through ____) from the ______________ address to ______________________________.</p><p name="632b" id="632b" class="graf graf--p graf-after--p">Finally, we use the _________ (bits ___ through ____) from the ______________ address to ______________________________.</p><p name="2605" id="2605" class="graf graf--p graf-after--p">b. For each 32-byte block of data in the cache, some extra bits (metadata) are kept in the cache. What are these bits and how many meta-data bits altogether does each cache block have?</p><p name="197f" id="197f" class="graf graf--p graf-after--p">Solution:</p><p name="48f1" id="48f1" class="graf graf--p graf-after--p">a.</p><p name="5ccb" id="5ccb" class="graf graf--p graf-after--p">First, we use the index (bits 5 through 11) from the virtual address to find which set to look at.</p><p name="1482" id="1482" class="graf graf--p graf-after--p">Second, we use the tag (bits 12 through 55) from the physical address to check if any blocks in the set have a matching tag.</p><p name="0624" id="0624" class="graf graf--p graf-after--p">Finally, we use the offset (bits 0 through 4) from the virtual address to get the right byte in the block.</p><p name="3eaf" id="3eaf" class="graf graf--p graf-after--p">b.</p><ul class="postList"><li name="ec2e" id="ec2e" class="graf graf--li graf-after--p">Valid bit: 1 bit/line</li><li name="b679" id="b679" class="graf graf--li graf-after--li">Dirty bit: 1 bit/line</li><li name="5b60" id="5b60" class="graf graf--li graf-after--li">LRU counter: log2(8) = 3 bit/line</li><li name="d62e" id="d62e" class="graf graf--li graf-after--li">Tag: 44 bit/line (56â12 = 44 bit)</li></ul><p name="fe01" id="fe01" class="graf graf--p graf-after--li">Altogether, we have 49-bit metadata in each cache line.</p><p name="22ae" id="22ae" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 1-3. Cache Misses</strong></p><p name="3129" id="3129" class="graf graf--p graf-after--p">A processor has a 16-way set-associative, write-back, write-allocate, physically indexed, 4kB cache with 16-byte lines. Starting with an empty cache (all lines invalid), the processor begins zeroing out the elements of an array that begins at physical address 0x40fbfca8 and is 0x4000 bytes long. Pages are 1MB in size. Each element of the array is four bytes in size and the processor writes a zero into each element from first to last, without accessing any other memory locations.</p><p name="262a" id="262a" class="graf graf--p graf-after--p">a. By the time the zeroing-out is done, how many cache misses will have occurred?</p><p name="e3f0" id="e3f0" class="graf graf--p graf-after--p">b. Which set in the cache will be the first one to not have any non-valid (empty) lines left?</p><p name="848c" id="848c" class="graf graf--p graf-after--p">c. By the time the zeroing-out is done, how many write-backs will have occurred?</p><p name="ea48" id="ea48" class="graf graf--p graf-after--p">d. What is the tag of the last cache line to be written back to memory?</p><p name="2cb2" id="2cb2" class="graf graf--p graf-after--p">Solution:</p><p name="cd2f" id="cd2f" class="graf graf--p graf-after--p">The cache summary is,</p><ul class="postList"><li name="633e" id="633e" class="graf graf--li graf-after--p">4kB size</li><li name="4ef7" id="4ef7" class="graf graf--li graf-after--li">16 bytes per line, means the block size is 16 bytes</li><li name="b4b4" id="b4b4" class="graf graf--li graf-after--li">block offset is 4 bit (0~3 bit)</li><li name="9334" id="9334" class="graf graf--li graf-after--li">256 lines</li><li name="24d0" id="24d0" class="graf graf--li graf-after--li">16-way SAC</li><li name="a3d7" id="a3d7" class="graf graf--li graf-after--li">16 sets: 4-bit index (4~7 bit)</li></ul><p name="3ddf" id="3ddf" class="graf graf--p graf-after--li">a.</p><ul class="postList"><li name="0635" id="0635" class="graf graf--li graf-after--p">Array length 0x4000 bytes, all of them will be cache misses when the first access</li><li name="2940" id="2940" class="graf graf--li graf-after--li">If aligned, we can have 0x4000 / 16 = 0x400 = 1024 times cache misses</li><li name="eac8" id="eac8" class="graf graf--li graf-after--li">If not aligned, we need to have one more block, so there will be 1025 times cache misses</li></ul><p name="c7e7" id="c7e7" class="graf graf--p graf-after--li">Because the block offset is of the starting address is,</p><pre name="fd7f" id="fd7f" class="graf graf--pre graf-after--p">0x8</pre><p name="04b3" id="04b3" class="graf graf--p graf-after--pre">This means we are not aligned and there will be 1025 times cache misses.</p><p name="4523" id="4523" class="graf graf--p graf-after--p">b.</p><p name="f856" id="f856" class="graf graf--p graf-after--p">The first set that is accessed is the first one to become full because we have a round-robin set access rule. The starting address is,</p><pre name="d51b" id="d51b" class="graf graf--pre graf-after--p">0x40fbfca8</pre><p name="53ff" id="53ff" class="graf graf--p graf-after--pre">The index is,</p><pre name="573c" id="573c" class="graf graf--pre graf-after--p">0xA = 10</pre><p name="8ab9" id="8ab9" class="graf graf--p graf-after--pre">So we have the conclusion that the 10th set will be the first one to not have any non-valid (empty) lines left.</p><p name="7c12" id="7c12" class="graf graf--p graf-after--p">c.</p><p name="d72f" id="d72f" class="graf graf--p graf-after--p">In the end, the last 256 lines of data will not be kicked out and we can keep them in the cache. However, there have to be 1025 lines of data (as we have discussed in a.). So the number of writing back to the memory is,</p><pre name="061c" id="061c" class="graf graf--pre graf-after--p">1025 - 256 = 769</pre><p name="92ef" id="92ef" class="graf graf--p graf-after--pre">d.</p><p name="dc9e" id="dc9e" class="graf graf--p graf-after--p">The last cache line is the 768th line, so the starting address of this line is,</p><pre name="6f64" id="6f64" class="graf graf--pre graf-after--p">0x40fbfca8 + 768 * 16 = 0x40fbfca8 + 0x3000 = 0x40fc2ca8</pre><p name="fa5d" id="fa5d" class="graf graf--p graf-after--pre">Except for the 0~3 bit as the block offset and the 4~7 bit as the index, the rest of the address will be the tag. Therefore, the tag of the last cache line to be written back to memory is,</p><pre name="0052" id="0052" class="graf graf--pre graf-after--p">0x40fc2c</pre><p name="6959" id="6959" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Example 1-4. SA Cache, Index, and Offsets</strong></p><p name="0acb" id="0acb" class="graf graf--p graf-after--p">The Processor has a 32Kbyte, 4-way set associative, write back, write allocate, physically indexed, physically tagged L1 cache with 32-byte blocks. The system uses 32-bit virtual and physical addresses with 4 Kbyte pages.</p><p name="9f29" id="9f29" class="graf graf--p graf-after--p">a. Can we overlap virtual-to-physical address translation with the L1 cache access? Explain your answer.</p><p name="acd0" id="acd0" class="graf graf--p graf-after--p">b. The designers of this cache had a very good reason for not making it a virtually indexed, physically tagged cache. What is the reason?</p><p name="07dc" id="07dc" class="graf graf--p graf-after--p">c. If the processor reads memory at virtual address 0x00001234 (physical address 0xFFFF1234), what is the index that is used to access the cache?</p><p name="1cdf" id="1cdf" class="graf graf--p graf-after--p">d. If the processor reads memory at virtual address 0x00001234 (physical address 0xFFFF1234), what is the tag that is used by the cache to determine if the access is a cache hit?</p><p name="77e6" id="77e6" class="graf graf--p graf-after--p">e. If the processor reads memory at virtual address 0x00001234 (physical address 0xFFFF1234), how many tag comparisons are done to determine if the access is a cache hit?</p><p name="bf12" id="bf12" class="graf graf--p graf-after--p">Solution:</p><p name="5fba" id="5fba" class="graf graf--p graf-after--p">a. No. Because the cache is physically indexed and it needs a physical address to start lookup.</p><p name="d9d1" id="d9d1" class="graf graf--p graf-after--p">b. Aliasing. We will result in different values for the same memory address.</p><p name="5ca4" id="5ca4" class="graf graf--p graf-after--p">c. The physical address is,</p><pre name="b2fa" id="b2fa" class="graf graf--pre graf-after--p">1111 1111 1111 1111 0001 0010 0011 0100</pre><p name="3c12" id="3c12" class="graf graf--p graf-after--pre">The block offset should have 5 bits,</p><pre name="0106" id="0106" class="graf graf--pre graf-after--p">10100</pre><p name="215b" id="215b" class="graf graf--p graf-after--pre">There are 256 sets so the index should be an 8-bit index,</p><pre name="3cf3" id="3cf3" class="graf graf--pre graf-after--p">1 0010 001</pre><p name="18ec" id="18ec" class="graf graf--p graf-after--pre">d. The rest of the physical address should be the tag,</p><pre name="8872" id="8872" class="graf graf--pre graf-after--p">1111 1111 1111 1111 000</pre><p name="fe04" id="fe04" class="graf graf--p graf-after--pre">e. Because we have a 4-way SAC, then the tag comparison we have to done is 4.</p><p name="e34d" id="e34d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Memory and Virtual Memory</strong></p><p name="397c" id="397c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Virtual Memory</strong></p><p name="efb0" id="efb0" class="graf graf--p graf-after--p">The program thinks that it virtually has a lot of memory. But in practice, there isnât that much memory. The technique to deceive the programs that we have enough memory is called <strong class="markup--strong markup--p-strong">virtual memory</strong>.</p><p name="e83b" id="e83b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Pages</strong></p><p name="ffed" id="ffed" class="graf graf--p graf-after--p">The virtual memory is divided into small blocks (commonly 4KB per page) called pages.</p><p name="4253" id="4253" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Frames</strong></p><p name="338c" id="338c" class="graf graf--p graf-after--p">The physical memory is divided into small blocks (commonly 4KB per frame) called frames.</p><p name="d480" id="d480" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Page Number and Page Offset</strong></p><p name="62a7" id="62a7" class="graf graf--p graf-after--p">Similar to the cache blocks, the virtual address can also be divided into two parts.</p><ul class="postList"><li name="b96d" id="b96d" class="graf graf--li graf-after--p">Page offset (LSB): Page offset is the data location in the frame</li><li name="4599" id="4599" class="graf graf--li graf-after--li">Virtual page number (MSB): Virtual page number should be translated to physical frame number before we can determine which frame should be the data of that memory address</li></ul><figure name="015e" id="015e" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*b1K93sYlgI0BvLhmLyt-2g.png" data-width="1378" data-height="154" src="https://cdn-images-1.medium.com/max/800/1*b1K93sYlgI0BvLhmLyt-2g.png"></figure><p name="2fff" id="2fff" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Flat Page Table</strong></p><p name="5af6" id="5af6" class="graf graf--p graf-after--p">In order to conduct V2P translation, we can maintain a table for translating the virtual page number to the physical frame number. The simplest idea is to maintain a flat page table with all the V2P mappings kept in one single table. Here are some features of this table,</p><ul class="postList"><li name="248a" id="248a" class="graf graf--li graf-after--p">Size: # of pages in VM times the size per entry in the flat page table</li></ul><figure name="13f6" id="13f6" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*cIJ65_HjadBJXMHE.png" data-width="1422" data-height="106" src="https://cdn-images-1.medium.com/max/800/0*cIJ65_HjadBJXMHE.png"></figure><ul class="postList"><li name="216e" id="216e" class="graf graf--li graf-after--figure">Indexing: The virtual page number is used to index the flat page table</li><li name="4f14" id="4f14" class="graf graf--li graf-after--li">Advantages: Simple and easy</li><li name="e9cb" id="e9cb" class="graf graf--li graf-after--li">Disadvantages: Costly and Unnecessary</li></ul><p name="7678" id="7678" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(6) Multi-Level Page Tables (MLPT)</strong></p><p name="cc72" id="cc72" class="graf graf--p graf-after--p">Especially when we have a large virtual memory, multi-level page tables can be useful because we can hide some of the useless V2P mappings. This is also called a hierarchical page table.</p><ul class="postList"><li name="32e2" id="32e2" class="graf graf--li graf-after--p">Indexing: The page number is partitioned into the inner page number and the outer page numbers. The outer page numbers are used to index the outer page tables and the inner page number is used to index the inner page table. For instance, the following virtual address can be an example.</li></ul><figure name="5102" id="5102" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*YXzLbQNsDKiWSQUj.png" data-width="1474" data-height="248" src="https://cdn-images-1.medium.com/max/800/0*YXzLbQNsDKiWSQUj.png"></figure><ul class="postList"><li name="561a" id="561a" class="graf graf--li graf-after--figure">Advantages: The cost for storage is lower</li><li name="eb40" id="eb40" class="graf graf--li graf-after--li">Disadvantages: Slow because we have to read through the page table entries</li></ul><p name="0432" id="0432" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(7) Table Look-Aside Buffer (TLB)</strong></p><p name="6d4f" id="6d4f" class="graf graf--p graf-after--p">To speed up the V2P translation, a special, small cache is used called the <strong class="markup--strong markup--p-strong">table look-aside buffer</strong> (aka. <strong class="markup--strong markup--p-strong">TLB</strong>). This table is always fully associative with 64~512 entries.</p><p name="0423" id="0423" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Memory</strong></p><p name="f49c" id="f49c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Random Access Memory (RAM)</strong></p><p name="4996" id="4996" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Random-access memory (RAM)</strong> simply refers to the fact that we can access any memory location by address without going through all the memory locations.</p><p name="2c65" id="2c65" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Statistic Random Access Memory (SRAM)</strong></p><p name="8f0d" id="8f0d" class="graf graf--p graf-after--p">The term âstatisticâ refers to the fact that SRAM retains its data while the power is supplied. This is faster and more expensive.</p><p name="9ffa" id="9ffa" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Dynamic Random Access Memory (DRAM)</strong></p><p name="b3d0" id="b3d0" class="graf graf--p graf-after--p">The term âdynamicâ means that we will lose data unless we refresh the data. This is slower.</p><p name="35dd" id="35dd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Memory Cell</strong></p><p name="aa31" id="aa31" class="graf graf--p graf-after--p">The smallest unit in the memory we use to keep the data.</p><p name="85fe" id="85fe" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Word Lines</strong></p><p name="6a2c" id="6a2c" class="graf graf--p graf-after--p">Lines used to activate memory cells.</p><p name="3cd6" id="3cd6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Bit Lines</strong></p><p name="3aef" id="3aef" class="graf graf--p graf-after--p">Lines used to input the data to the memory cell. These lines are perpendicular to the word lines.</p><p name="1efa" id="1efa" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Row Decoder</strong></p><p name="e65a" id="e65a" class="graf graf--p graf-after--p">This decides which word line gets activated.</p><p name="4103" id="4103" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Column Decoder</strong></p><p name="cf31" id="cf31" class="graf graf--p graf-after--p">It selects the correct bits among the bit lines using the column address and then it outputs a single bit.</p><p name="6f4d" id="6f4d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) Row Address</strong></p><p name="adf5" id="adf5" class="graf graf--p graf-after--p">The input to the row decoder implies which word line to be activated.</p><p name="e424" id="e424" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) Column Address</strong></p><p name="8775" id="8775" class="graf graf--p graf-after--p">The input to the column decoder implies the correct bits among the bit lines.</p><p name="9f0c" id="9f0c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(11) SRAM Implementation</strong></p><p name="8cf2" id="8cf2" class="graf graf--p graf-after--p">In SRAM, the actual memory cell consists of two CMOS inverters.</p><figure name="3026" id="3026" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*Bbx4OORHzU9efWhD.png" data-width="1632" data-height="638" src="https://cdn-images-1.medium.com/max/800/0*Bbx4OORHzU9efWhD.png"></figure><p name="5455" id="5455" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(12) DRAM Implementation</strong></p><p name="3cf1" id="3cf1" class="graf graf--p graf-after--p">In SRAM, the memory cell only consists of a capacitor that is used to keep the value of the data.</p><figure name="edaa" id="edaa" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*e6N89ui5JBn_h5J7.png" data-width="1508" data-height="652" src="https://cdn-images-1.medium.com/max/800/0*e6N89ui5JBn_h5J7.png"></figure><p name="0b71" id="0b71" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(13) Destructive Read</strong></p><p name="edf1" id="edf1" class="graf graf--p graf-after--p">Because of the transistor leakage, the capacitor drains into the bit line but it is no longer fully charged. This is called a destructive read.</p><p name="b6dd" id="b6dd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(14) Trench Cell</strong></p><p name="79be" id="79be" class="graf graf--p graf-after--p">In real DRAM, the capacitor and the transistor are built as a signal transistor in a technology called trench cell.</p><p name="365e" id="365e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(15) Sense Amplifier</strong></p><p name="1390" id="1390" class="graf graf--p graf-after--p">This architecture connects all the bit lines. It senses the small changes on the bit lines and amplifies them.</p><p name="4d38" id="4d38" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(16) Row Buffer</strong></p><p name="6459" id="6459" class="graf graf--p graf-after--p">This a storage element that stores the correct values read from the whole row of cells after amplified by the sense amplifier.</p><p name="2e3d" id="2e3d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(17) A 16-bit Memory Chip Organization</strong></p><figure name="36f8" id="36f8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*uGjlImtBmDvwKCw6.png" data-width="1812" data-height="1176" src="https://cdn-images-1.medium.com/max/800/0*uGjlImtBmDvwKCw6.png"></figure><p name="9780" id="9780" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(18) Memory Read Pattern: Read-then-Write</strong></p><p name="8172" id="8172" class="graf graf--p graf-after--p">When we read from the memory, we will first read the data and then the data will be written back to the corresponding row. This means that after each read, the data will be refreshed to avoid destructive read.</p><p name="8560" id="8560" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(19) Memory Write Pattern: Read-then-Write</strong></p><p name="19c8" id="19c8" class="graf graf--p graf-after--p">The writing process is similar to read, and we have to read the data firstly to the row buffer, then change some values, and finally write the data back.</p><p name="7be8" id="7be8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(20) Fast-Page Mode</strong></p><p name="0425" id="0425" class="graf graf--p graf-after--p">If we first read/write a row, and then we immediately read/write it again, we will enter the fast-page mode of this row. This means that we can directly read/write from the row buffer.</p><p name="de75" id="de75" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(21) Front-Side Bus</strong></p><p name="d016" id="d016" class="graf graf--p graf-after--p">DRAM is access to the processor through the front-side bus using a memory controller.</p><p name="858e" id="858e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4. Storage and Fault Tolerance</strong></p><p name="f892" id="f892" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Types of the Storage</strong></p><ul class="postList"><li name="85d2" id="85d2" class="graf graf--li graf-after--p">Magnetic disks: HDD and floppy disks</li><li name="8494" id="8494" class="graf graf--li graf-after--li">Optical disks: such as CDs or DVDs</li><li name="51e9" id="51e9" class="graf graf--li graf-after--li">Tape: for backup</li><li name="4d64" id="4d64" class="graf graf--li graf-after--li">Flash drivers: much faster</li></ul><p name="f9d0" id="f9d0" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Platters</strong></p><p name="a912" id="a912" class="graf graf--p graf-after--p">The platters in the disks are used to keep the data on both of its surfaces.</p><p name="53e0" id="53e0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Spindle</strong></p><p name="a044" id="a044" class="graf graf--p graf-after--p">All the platters are attached to a single spindle.</p><p name="4ae9" id="4ae9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Magnetic Head</strong></p><p name="e409" id="e409" class="graf graf--p graf-after--p">Heads are used to read the data from the platters.</p><p name="339a" id="339a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Head Assembly</strong></p><p name="78b8" id="78b8" class="graf graf--p graf-after--p">All the heads are connected together to the head assembly.</p><p name="7c23" id="7c23" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Tracks</strong></p><p name="6152" id="6152" class="graf graf--p graf-after--p">Each magnetic head will be able to access a circle called track on the surface of the disk.</p><p name="a0cb" id="a0cb" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Cylinder</strong></p><p name="8039" id="8039" class="graf graf--p graf-after--p">All of the tracks at the same distance from the spindle is called a cylinder.</p><p name="5222" id="5222" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Sectors</strong></p><p name="c448" id="c448" class="graf graf--p graf-after--p">The data along with one track is divided into different sectors, and the sector is the smallest unit we can read. A sector will consist of three parts,</p><ul class="postList"><li name="4b81" id="4b81" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Preamble: </strong>Used to tell where should the sector start</li><li name="7051" id="7051" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Data</strong></li><li name="e5b4" id="e5b4" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Checksum</strong></li></ul><p name="e20d" id="e20d" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(9) Magnetic Disk Capacity</strong></p><figure name="80bc" id="80bc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*NYUlBpkarPTsCf3J.png" data-width="1298" data-height="114" src="https://cdn-images-1.medium.com/max/800/0*NYUlBpkarPTsCf3J.png"></figure><p name="a2d7" id="a2d7" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(10) Magnetic Disk Access Time</strong></p><ul class="postList"><li name="214b" id="214b" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Spinning time</strong>: time to spin the platters</li><li name="6685" id="6685" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Seek time</strong>: time to find the cylinder</li><li name="6a55" id="6a55" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Rotational latency</strong>: time to move to the correct sector</li><li name="b1a7" id="b1a7" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Data read</strong>: time to read data</li><li name="b809" id="b809" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Controller time</strong>: time to check data</li><li name="c209" id="c209" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">I/O bus time</strong>: time to send data to the processor</li><li name="1a59" id="1a59" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Queuing delay</strong>: time to wait for a previous read</li></ul><p name="51c0" id="51c0" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(11) Solid-State Disk (SSD)</strong></p><p name="9739" id="9739" class="graf graf--p graf-after--p">SSD can be a DRAM and battery. It is fast but expensive.</p><p name="786a" id="786a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(12) Flash Memory</strong></p><p name="0f62" id="0f62" class="graf graf--p graf-after--p">It works like SSD without a power supplier.</p><p name="13e5" id="13e5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(13) Hybrid Magnetic Flash</strong></p><p name="3c66" id="3c66" class="graf graf--p graf-after--p">The flash memory is combined with the traditional HDD as a cache.</p><p name="b25c" id="b25c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(14) Buses for Storage I/O</strong></p><ul class="postList"><li name="907c" id="907c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">I/O Buses</strong></li><li name="6d2d" id="6d2d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Mezzanine Bus</strong>: like PCI expresses, SATA, and SCSI</li></ul><p name="b73e" id="b73e" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(15) Dependability</strong></p><p name="1282" id="1282" class="graf graf--p graf-after--p">Qualify if a delivered service that justifies relying on the system to provide the service.</p><p name="28b1" id="28b1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(16) Specified Service Vs. Delivered Service</strong></p><ul class="postList"><li name="76a6" id="76a6" class="graf graf--li graf-after--p">Specified Service: the expected system behavior</li><li name="1792" id="1792" class="graf graf--li graf-after--li">Delivered Service: the actual system behavior</li></ul><p name="af05" id="af05" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(17) Fault</strong></p><p name="20d6" id="20d6" class="graf graf--p graf-after--p">Modules deviate from the specified behaviors. This is also called a latent error.</p><p name="1544" id="1544" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(18) Error</strong></p><p name="9f6f" id="9f6f" class="graf graf--p graf-after--p">The actual behavior within the system differs from the specified behavior. This is also called an active fault.</p><p name="f51c" id="f51c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(19) Failure</strong></p><p name="0021" id="0021" class="graf graf--p graf-after--p">The failure occurs when the system deviated from the specified behavior.</p><p name="bace" id="bace" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(20) Reliability</strong></p><p name="404a" id="404a" class="graf graf--p graf-after--p">A measure of continuous service accomplishment. This can be measured by <strong class="markup--strong markup--p-strong">mean time to failure</strong> (or <strong class="markup--strong markup--p-strong">MTTF</strong>).</p><p name="e26b" id="e26b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(21) Mean Time to Failure (MTTF)</strong></p><p name="eddf" id="eddf" class="graf graf--p graf-after--p">The MTTF is how long will the system provide service before the next service interruption.</p><p name="6011" id="6011" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(22) Availability</strong></p><p name="af95" id="af95" class="graf graf--p graf-after--p">The availability measures service accomplishment as a fraction of the overall time. This can be measured by,</p><figure name="8342" id="8342" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*gKkg_Ss5tLDscn3b.png" data-width="1256" data-height="92" src="https://cdn-images-1.medium.com/max/800/0*gKkg_Ss5tLDscn3b.png"></figure><p name="6bf8" id="6bf8" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(23) Types of Faults</strong></p><ul class="postList"><li name="021f" id="021f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Hardware (HW) faults</strong>: hardware fails to perform as designed</li><li name="7bac" id="7bac" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Design faults</strong>: software bugs or the hardware design mistakes</li><li name="f0db" id="f0db" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Operation faults</strong>: operator or the userâs mistakes</li><li name="597a" id="597a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Environmental faults</strong>: fire and the power failure</li><li name="ef85" id="ef85" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Permanent faults</strong>: once we have it, it doesnât get corrected</li><li name="52c6" id="52c6" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Intermittent faults</strong>: last for a while but recurring</li><li name="dce1" id="dce1" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Transient faults</strong>: last for a while and then go away</li></ul><p name="3177" id="3177" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(24) Checkpointing</strong></p><p name="5839" id="5839" class="graf graf--p graf-after--p">The checkpointing saves the state periodically, then if we detect the errors, we will stop the system and restore the state.</p><p name="17cd" id="17cd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(25) 2-way Redundancy (DMR)</strong></p><p name="2b44" id="2b44" class="graf graf--p graf-after--p">Two modules do the exact same work, and the outcomes are compared. Rollback and recovery if the outcomes are different.</p><p name="5065" id="5065" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(26) 3-way Redundancy (TMR)</strong></p><p name="5b36" id="5b36" class="graf graf--p graf-after--p">Three modules do the exact same work, and the outcomes are voted. Expensive but it can tolerate one fault in one module.</p><p name="b302" id="b302" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(27) 5-way Redundancy</strong></p><p name="54ca" id="54ca" class="graf graf--p graf-after--p">Five modules do the exact same work, and the outcomes are voted. Expensive but it can tolerate one fault in 2 modules.</p><p name="1e9b" id="1e9b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(28) Error Correction Methods</strong></p><ul class="postList"><li name="0607" id="0607" class="graf graf--li graf-after--p">Parity Bit: XOR all the data bits</li><li name="1378" id="1378" class="graf graf--li graf-after--li">Error-Correction Code (ECC): SECDED can correct up to 1-bit flip</li><li name="adf0" id="adf0" class="graf graf--li graf-after--li">Redundant Array of Independent Disks (RAID): a technique family for error correction</li></ul><p name="d989" id="d989" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(29) RAID Overview</strong></p><ul class="postList"><li name="8b3b" id="8b3b" class="graf graf--li graf-after--p">RAID 0: striping</li><li name="d719" id="d719" class="graf graf--li graf-after--li">RAID 1: mirroring</li><li name="3171" id="3171" class="graf graf--li graf-after--li">RAID 4: block interleaved parity (BIP)</li><li name="4f1b" id="4f1b" class="graf graf--li graf-after--li">RAID 5: distributed block interleaved parity</li><li name="0c32" id="0c32" class="graf graf--li graf-after--li">RAID 6: two parity blocks</li></ul><p name="9f17" id="9f17" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(30) Failure Rate</strong></p><p name="f98b" id="f98b" class="graf graf--p graf-after--p">In the following discussion, we will suppose the failure rate of a disk is <em class="markup--em markup--p-em">f</em>.</p><p name="03fe" id="03fe" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(31) RAID 0</strong></p><p name="3028" id="3028" class="graf graf--p graf-after--p">RAID 0 takes N disks as a single disk, and the tracks will be referred to as strips. The reliability will become worse because,</p><figure name="5316" id="5316" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*OIrvdH1_3LOS3Ef_.png" data-width="1142" data-height="94" src="https://cdn-images-1.medium.com/max/800/0*OIrvdH1_3LOS3Ef_.png"></figure><p name="0316" id="0316" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(32) RAID 1</strong></p><p name="daa3" id="daa3" class="graf graf--p graf-after--p">With RAID 1, the other disks are copies of the first disk. Suppose we have N disks with the same copy, then without replacement, the MTTDL (mean time to data loss) will be,</p><figure name="508a" id="508a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Vg8C6I1e1TP7utqzwfsDPA.png" data-width="1176" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*Vg8C6I1e1TP7utqzwfsDPA.png"></figure><p name="033e" id="033e" class="graf graf--p graf-after--figure">With replacement, the MTTDL will be,</p><figure name="a004" id="a004" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UyqsY919T9Q5L_zCERU9qw.png" data-width="1176" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*UyqsY919T9Q5L_zCERU9qw.png"></figure><p name="172a" id="172a" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(33) RAID 4</strong></p><p name="b665" id="b665" class="graf graf--p graf-after--p">It adds an additional disk used for keeping the XOR results of strips as parities. For N disks in RAID 4, there will be,</p><ul class="postList"><li name="e3f4" id="e3f4" class="graf graf--li graf-after--p">N-1 disks for striped data</li><li name="8e6a" id="8e6a" class="graf graf--li graf-after--li">1 disk for parity blocks</li></ul><p name="9f1b" id="9f1b" class="graf graf--p graf-after--li">Because now we can tolerant one disk failure, then without replacement,</p><figure name="41d6" id="41d6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pCM98WixZxs21H43o_KMcQ.png" data-width="1176" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*pCM98WixZxs21H43o_KMcQ.png"></figure><p name="ac7a" id="ac7a" class="graf graf--p graf-after--figure">With replacement, the MTTDL will be,</p><figure name="81f3" id="81f3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*l4pw-2vc1Nqzv1DeD28--Q.png" data-width="1176" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*l4pw-2vc1Nqzv1DeD28--Q.png"></figure><p name="8ae4" id="8ae4" class="graf graf--p graf-after--figure">Although the read throughput will be N-1 times a single disk, the write throughput will be 1/2 because both data and parity should be written.</p><p name="7377" id="7377" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(34) RAID 5</strong></p><p name="8452" id="8452" class="graf graf--p graf-after--p">RAID 5 is implemented with a distributed block interleaved parity. The read throughput will be N times the throughput of a single disk, and the write throughput will be N/4 times the throughput of a single disk.</p><p name="bb6a" id="bb6a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(35) RAID 6</strong></p><p name="65b5" id="65b5" class="graf graf--p graf-after--p">RAID 6 can still work with 2 failed strips. When</p><ul class="postList"><li name="260d" id="260d" class="graf graf--li graf-after--p">one disk fails: use the parity bit to recover</li><li name="8bfb" id="8bfb" class="graf graf--li graf-after--li">two disks fail: use equations to reconstruct the data</li></ul><p name="9e62" id="9e62" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Example 4-1. Disk and RAID</strong></p><p name="1ab4" id="1ab4" class="graf graf--p graf-after--p">A 6,000RPM disk drive has 5 platters, with 100,000 tracks per surface, 1,000 sectors per track, and 512 data bytes per sector. Each sector also has a 128-bit error detection code that can detect all errors in its sector but cannot correct any errors. The head takes one microsecond (one-millionth of a second) to move from one cylinder to an adjacent cylinder, and multi-cylinder movements are done at the same speed (one cylinder per microsecond). The disk controller is very fast (assume zero latency for everything it does) and the I/O bus has a very large (assume infinite) bandwidth.</p><p name="2ed8" id="2ed8" class="graf graf--p graf-after--p">a. How many heads does the disk drive have?</p><p name="d485" id="d485" class="graf graf--p graf-after--p">b. Assuming that the disk controller and the drive itself are not servicing any other requests, what is the worst-case time needed to read a sector from the disk?</p><p name="4081" id="4081" class="graf graf--p graf-after--p">c. Assuming that the disk controller and the drive itself are not servicing any other requests, what is the best-case time needed to read the entire disk?</p><p name="6d3a" id="6d3a" class="graf graf--p graf-after--p">d. If we use two of these disk drives in a RAID0 configuration, what is the total data capacity of the resulting RAID array?</p><p name="f284" id="f284" class="graf graf--p graf-after--p">e. If we use two of these disk drives in a RAID0 configuration, can we recover all data if one sector is damaged on one of the disk drives? Explain your answer.</p><p name="8aee" id="8aee" class="graf graf--p graf-after--p">f. If we use two of these disk drives in a RAID1 configuration, can we recover all data if one disk drive is accidentally dropped into an active volcano? Explain your answer.</p><p name="706a" id="706a" class="graf graf--p graf-after--p">g. If we use five of these disk drives in a RAID5 configuration, what is the total data capacity of the resulting RAID array?</p><p name="14b5" id="14b5" class="graf graf--p graf-after--p">h. If we use five of these disk drives in a RAID5 configuration, is it possible to damage only two sectors in a way that the RAID array cannot recover from? Explain your answer.</p><p name="68dd" id="68dd" class="graf graf--p graf-after--p">Solution:</p><p name="9caf" id="9caf" class="graf graf--p graf-after--p">A summary of the HDD is,</p><ul class="postList"><li name="5eec" id="5eec" class="graf graf--li graf-after--p">6,000RPM</li><li name="3402" id="3402" class="graf graf--li graf-after--li">5 platters</li><li name="aa43" id="aa43" class="graf graf--li graf-after--li">100,000 tracks per surface</li><li name="77e3" id="77e3" class="graf graf--li graf-after--li">1,000 sectors per track</li><li name="0581" id="0581" class="graf graf--li graf-after--li">Each sector contains a 128-bit ECC detecting all errors</li><li name="5202" id="5202" class="graf graf--li graf-after--li">Head move time: 1us per cylinder</li><li name="617d" id="617d" class="graf graf--li graf-after--li">infinite I/O bus bandwidth</li></ul><p name="17f5" id="17f5" class="graf graf--p graf-after--li">a. 10, because we have 5 platters and each of them has 2 sides. We need to adopt a head for each side.</p><p name="060a" id="060a" class="graf graf--p graf-after--p">b. In the worst case,</p><ul class="postList"><li name="68c5" id="68c5" class="graf graf--li graf-after--p">One track is #0 and another is #100,000. We have to move the head in 99,999 us = 0.099999 s.</li><li name="4498" id="4498" class="graf graf--li graf-after--li">We are writing after the start of the sector we want, so we have to wait for another 1000 sectors to get to the same position. This is 1 rotation. After that, we have to read the data from this sector. So we need to go through 1001 sectors and it takes 1.001 rotations. By the fact of 6000 RPM, we can calculate the time we need for this 1.001 rotation, which is 1.001 / (6000/60) = 0.01001 s.</li></ul><p name="21cc" id="21cc" class="graf graf--p graf-after--li">This in general, the overall time we need for reading this sector is</p><pre name="1fdf" id="1fdf" class="graf graf--pre graf-after--p">0.099999 + 0.01001 = 0.110009 s</pre><p name="1642" id="1642" class="graf graf--p graf-after--pre">c. In the best case of reading the entire disk,</p><ul class="postList"><li name="b2ae" id="b2ae" class="graf graf--li graf-after--p">One time one direction head movement: from track #0 to track #100,000. We have to move the head in 99,999 us = 0.099999 s.</li><li name="0017" id="0017" class="graf graf--li graf-after--li">Go through all the sectors in sequence and we have to spend 1 rotation for each track. Overall, we need to rotate 100,000 times and the time of it will be 1000 s.</li></ul><p name="ccf9" id="ccf9" class="graf graf--p graf-after--li">This in general, the overall time we need is</p><pre name="68a1" id="68a1" class="graf graf--pre graf-after--p">1000 + 0.099999 = 1000.099999 s</pre><p name="e7d7" id="e7d7" class="graf graf--p graf-after--pre">d. In RAID 0, these two disks will be treated as a whole disk. So the capacity will be 2*1000*100000*512 = 1024 * 10â¹ bytes</p><p name="836b" id="836b" class="graf graf--p graf-after--p">e. Because there is no redundancy for RAID 0, we can not recover data.</p><p name="4cb5" id="4cb5" class="graf graf--p graf-after--p">f. Yes because each of the individual disks will contain all the data. The other one acts like a redundancy.</p><p name="4f11" id="4f11" class="graf graf--p graf-after--p">g. In RAID 5, except for the disk that is used to store the parity data, the other 4 disks can be treated as a single disk. So the capacity will be 4*1000*100000*512 = 2048 * 10â¹ bytes</p><p name="9765" id="9765" class="graf graf--p graf-after--p">h. If we have one-bit flip problems on two different disks, even though we can know which disk has the error, we can not recover from this error by parity.</p><p name="d7d9" id="d7d9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 4â2. Disk and RAID</strong></p><p name="b9fe" id="b9fe" class="graf graf--p graf-after--p">A 12,000 RPM disk drive has 2 platters, with 100,000 tracks per surface, 1,000 sectors per track, and 512 data bytes per sector. Each sector also has a 128-bit error detection code that can detect all errors in its sector but cannot correct any errors. The head takes ten microseconds (a 100,000thcylinder, and multi-cylinder movements are done at the same speed (100,000 cylinders per second). The disk controller is very fast (assume zero latency for everything it does) and the I/O bus has a very large (assume infinite) bandwidth.</p><p name="7bc3" id="7bc3" class="graf graf--p graf-after--p">a. How many heads does the disk drive have?</p><p name="90e8" id="90e8" class="graf graf--p graf-after--p">b. Assuming that the disk controller and the drive itself are not servicing any other requests, what is the worst-case time needed to read a sector from the disk?</p><p name="9737" id="9737" class="graf graf--p graf-after--p">c. If we use two of these disk drives in a RAID0 configuration, and if the array is full (it contains as much data as its capacity allows), what is the best-case time needed to read all of the data stored in the array?</p><p name="9e4a" id="9e4a" class="graf graf--p graf-after--p">d. If we use five of these disk drives in a RAID5 configuration, and if the array is full (it contains as much data as its capacity allows), what is the best-case time needed to read all of the data stored in the array?</p><p name="46dc" id="46dc" class="graf graf--p graf-after--p">e. If we use three of these disk drives in a RAID5 configuration and we are extremely lucky, what is the maximum number of sectors that can be damaged in this array that still allow us to recover all of the data?</p><p name="ffa2" id="ffa2" class="graf graf--p graf-after--p">Solution:</p><p name="80af" id="80af" class="graf graf--p graf-after--p">A summary of the HDD is,</p><ul class="postList"><li name="9635" id="9635" class="graf graf--li graf-after--p">12,000 RPM</li><li name="3404" id="3404" class="graf graf--li graf-after--li">2 platters</li><li name="351a" id="351a" class="graf graf--li graf-after--li">100,000 tracks per surface</li><li name="81f5" id="81f5" class="graf graf--li graf-after--li">1,000 sectors per track</li><li name="6592" id="6592" class="graf graf--li graf-after--li">512 data per sector</li><li name="3c87" id="3c87" class="graf graf--li graf-after--li">Each sector contains a 128-bit ECC detecting all errors</li><li name="5100" id="5100" class="graf graf--li graf-after--li">Head move time: 10us per cylinder</li><li name="22cd" id="22cd" class="graf graf--li graf-after--li">infinite I/O bus bandwidth</li></ul><p name="c907" id="c907" class="graf graf--p graf-after--li">a. 4, because we have two platters</p><p name="7954" id="7954" class="graf graf--p graf-after--p">b. For the worst case,</p><ul class="postList"><li name="5e4b" id="5e4b" class="graf graf--li graf-after--p">Move from track #0 to track #100,000, which needs time for moving 99,999 tracks and the time is 999,990 us = 0.99999 s.</li><li name="4deb" id="4deb" class="graf graf--li graf-after--li">Read 1001 sectors, which means to have 1.001 rotation. The rotation time we need is 1.001 / (12,000 /60) = 1.001/200 = 0.005005 s.</li></ul><p name="522c" id="522c" class="graf graf--p graf-after--li">Thus, the overall time we need is 1.004995 s.</p><p name="c2a5" id="c2a5" class="graf graf--p graf-after--p">c. For the best case, both disks can be read simultaneously and we only have to consider one disk</p><ul class="postList"><li name="202c" id="202c" class="graf graf--li graf-after--p">Move from track #0 to track #100,000, which needs time for moving 99,999 tracks and the time is 999,990 us = 0.99999 s.</li><li name="1dcb" id="1dcb" class="graf graf--li graf-after--li">Each track has a rotation, so there will be 100,000 rotations. The time for this will be 100,000/ (12,000 /60) = 500 s</li></ul><p name="8017" id="8017" class="graf graf--p graf-after--li">Thus, the overall time we need is 500.99999 s.</p><p name="e64c" id="e64c" class="graf graf--p graf-after--p">d. In the best case,</p><ul class="postList"><li name="5cd9" id="5cd9" class="graf graf--li graf-after--p">Move from track #0 to track #100,000, which needs time for moving 99,999 tracks and the time is 999,990 us = 0.99999 s.</li><li name="3b47" id="3b47" class="graf graf--li graf-after--li">We will skip reading the parity so that only 4/5 of each disk is read. The time for this will be 4/5 * 500 = 400 s.</li></ul><p name="fc47" id="fc47" class="graf graf--p graf-after--li">Thus, the overall time we need is 400.99999 s.</p><p name="3330" id="3330" class="graf graf--p graf-after--p">e. We can recover when we have errors only in one disk. In the maximum case, all the data in that disk were damaged. So the maximum number of damaged sections we can have is the number of all the sections on a disk.</p><pre name="cab9" id="cab9" class="graf graf--pre graf-after--p">4 * 100,000 * 1,000 = 400,000,000 sectors</pre><p name="0255" id="0255" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">5. Cache Coherence</strong></p><p name="60ab" id="60ab" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Snooping</strong></p><p name="2e92" id="2e92" class="graf graf--p graf-after--p">All writes are put on the shared bus, and the cores snoop the bus for updating caches.</p><p name="3538" id="3538" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Directory</strong></p><p name="18b3" id="18b3" class="graf graf--p graf-after--p">Each block state is maintained by a directory, which reflects the state change after each writes. This is useful for more than 8 cores. A directory has the following features,</p><ul class="postList"><li name="6975" id="6975" class="graf graf--li graf-after--p">distribute across all cores</li><li name="72e2" id="72e2" class="graf graf--li graf-after--li">each core has its own slice</li><li name="6250" id="6250" class="graf graf--li graf-after--li">each slice serve a set of blocks</li></ul><p name="4185" id="4185" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Write-Updating</strong></p><p name="8119" id="8119" class="graf graf--p graf-after--p">Broadcast all write values so other cores can update the cache lines.</p><p name="48cf" id="48cf" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Write-Invalidate</strong></p><p name="3430" id="3430" class="graf graf--p graf-after--p">Broadcast all writes to make other copies of that data invalid in the other caches.</p><p name="5c8a" id="5c8a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Dirty Bit</strong></p><p name="1557" id="1557" class="graf graf--p graf-after--p">Dirty bit is used to show that the current modification of the cache line hasnât been written to the memory.</p><p name="c1de" id="c1de" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Valid Bit</strong></p><p name="353d" id="353d" class="graf graf--p graf-after--p">The valid bit is used to show whether the current cache is valid for reading.</p><p name="8da3" id="8da3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Shared Bit</strong></p><p name="093f" id="093f" class="graf graf--p graf-after--p">The shared bit is used to show whether the data in the current cache line is shared with some other caches.</p><p name="c7ad" id="c7ad" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) MSI Coherence Protocol</strong></p><p name="97c0" id="97c0" class="graf graf--p graf-after--p">In MSI, a block can be in one of three states in a cache,</p><ul class="postList"><li name="e091" id="e091" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Invalid state (I)</strong>: Valid bit = 0, when the block is not in any caches</li><li name="0782" id="0782" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Shared state (S)</strong>: Valid bit = 1; Dirty bit = 0, when the block is in the cache and not modified</li><li name="ff62" id="ff62" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Modified state (M)</strong>: Valid bit = 1; Dirty bit = 1, then the block is in the cache and it is modified</li></ul><p name="9469" id="9469" class="graf graf--p graf-after--li">When a read happens, we are expecting,</p><pre name="81f8" id="81f8" class="graf graf--pre graf-after--p">I -&gt; S, S -&gt; S, M -&gt; M</pre><p name="994e" id="994e" class="graf graf--p graf-after--pre">When a write happens, we are expecting,</p><pre name="63ca" id="63ca" class="graf graf--pre graf-after--p">S -&gt; I, I -&gt; M, S -&gt; M, M -&gt; M</pre><p name="e89d" id="e89d" class="graf graf--p graf-after--pre">When writing back to the memory, we are expecting,</p><pre name="c961" id="c961" class="graf graf--pre graf-after--p">M -&gt; I</pre><p name="0aa3" id="0aa3" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(9) MOSI Coherence Protocol</strong></p><p name="6d60" id="6d60" class="graf graf--p graf-after--p">MOSI is implemented to perform the cache to cache transformation and avoid writebacks to the memory for read-after-write.</p><ul class="postList"><li name="6a13" id="6a13" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Owner state (O)</strong>: this means that the block is in the cache and is modified. It will provide this data to the other caches instead of from the memory.</li></ul><p name="c5fd" id="c5fd" class="graf graf--p graf-after--li">So when a read happens, we are expecting,</p><pre name="89a1" id="89a1" class="graf graf--pre graf-after--p">I -&gt; S, S -&gt; S, M -&gt; O, O -&gt; O</pre><p name="4910" id="4910" class="graf graf--p graf-after--pre">When a write happens, we are expecting,</p><pre name="d322" id="d322" class="graf graf--pre graf-after--p">S -&gt; I, O -&gt; I, I -&gt; M, S -&gt; M, M -&gt; M</pre><p name="5ef0" id="5ef0" class="graf graf--p graf-after--pre">When writing back to the memory, we are expecting,</p><pre name="f312" id="f312" class="graf graf--pre graf-after--p">M -&gt; I, O -&gt; I</pre><p name="89e9" id="89e9" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(10) MOESI Coherence Protocol</strong></p><p name="b4b7" id="b4b7" class="graf graf--p graf-after--p">MOESI is implemented to unnecessary traffic for <code class="markup--code markup--p-code">S -&gt; I</code> when a block write happens.</p><ul class="postList"><li name="f99a" id="f99a" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Exclusive State (E)</strong>: this means that the block is in the cache and is modified. But it is not shared with any other caches so we donât have to send invalidation traffics on the bus.</li></ul><p name="4837" id="4837" class="graf graf--p graf-after--li">So when a read happens, we are expecting,</p><pre name="1401" id="1401" class="graf graf--pre graf-after--p">I -&gt; E, I -&gt; S, S -&gt; S, M -&gt; O, O -&gt; O, E -&gt; E, E -&gt; S</pre><p name="44e1" id="44e1" class="graf graf--p graf-after--pre">When a write happens, we are expecting,</p><pre name="5ecd" id="5ecd" class="graf graf--pre graf-after--p">S -&gt; I, O -&gt; I, I -&gt; M, S -&gt; M, M -&gt; M, E -&gt; M</pre><p name="505f" id="505f" class="graf graf--p graf-after--pre">When writing back to the memory, we are expecting,</p><pre name="d917" id="d917" class="graf graf--pre graf-after--p">M -&gt; I, O -&gt; I</pre><p name="cf1b" id="cf1b" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(11) Directory Entry</strong></p><p name="ecae" id="ecae" class="graf graf--p graf-after--p">A directory entry has,</p><ul class="postList"><li name="90e5" id="90e5" class="graf graf--li graf-after--p">1 dirty bit</li><li name="3fc8" id="3fc8" class="graf graf--li graf-after--li">1 bit for showing the cache is present (1) or not present (0)</li></ul><p name="5e4c" id="5e4c" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(12) Coherence Miss</strong></p><p name="248c" id="248c" class="graf graf--p graf-after--p">The coherence miss happens because we invalidate some blocks in the cache to maintain the cache coherence.</p><p name="12a2" id="12a2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(13) True Coherence Miss</strong></p><p name="fdbd" id="fdbd" class="graf graf--p graf-after--p">When different cores accessing the same data, we can have true coherence miss.</p><p name="7c5f" id="7c5f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(14) False Coherence Miss</strong></p><p name="00d0" id="00d0" class="graf graf--p graf-after--p">When different cores access different data and there should not be any coherence messages because of that, except for these data items are in the same block.</p><p name="d792" id="d792" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 5-1. MESI Protocol</strong></p><p name="dfb6" id="dfb6" class="graf graf--p graf-after--p">A shared-memory bus-based multiprocessor has two processors. Each processor has a direct-mapped L1 cache, with only sixteen 256-byte blocks in each cache. All addresses in this problem are physical addresses, and all caches are physically indexed and tagged. The MESI coherence protocol is used in this multiprocessor. The current state of the two caches is:</p><figure name="cf5b" id="cf5b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*at__NDK5FLe3NsJKgzQYWw.png" data-width="1142" data-height="525" src="https://cdn-images-1.medium.com/max/800/1*at__NDK5FLe3NsJKgzQYWw.png"></figure><p name="2a09" id="2a09" class="graf graf--p graf-after--figure">If the next access is each one of the following instructions, answer do we need to access result in a bus broadcast and what will be the new content of the corresponding lines in P0 and P1 caches.</p><p name="3787" id="3787" class="graf graf--p graf-after--p">a. a P0 access which executes SB (store byte) to address 0xFFFFFCFF</p><p name="be67" id="be67" class="graf graf--p graf-after--p">b. a P1 access which executes SB (store byte) to address 0xFFFFF5FF</p><p name="b233" id="b233" class="graf graf--p graf-after--p">c. a P1 access which executes LB (load byte) to address 0xFFFFF3FF</p><p name="4fe5" id="4fe5" class="graf graf--p graf-after--p">d. a P0 access which executes SB (store byte) to address 0xFFFFFD00</p><p name="b24b" id="b24b" class="graf graf--p graf-after--p">e. a P0 access which executes SB (store byte) to address 0xFFFFF700</p><p name="6a4e" id="6a4e" class="graf graf--p graf-after--p">Solution:</p><p name="664a" id="664a" class="graf graf--p graf-after--p">The cache summary,</p><ul class="postList"><li name="085d" id="085d" class="graf graf--li graf-after--p">256-byte block size: 8-bit block offset</li><li name="d3ce" id="d3ce" class="graf graf--li graf-after--li">16 blocks in cache: 4-bit index</li><li name="3b45" id="3b45" class="graf graf--li graf-after--li">2 processors</li><li name="35c9" id="35c9" class="graf graf--li graf-after--li">only L1 DM Cache</li><li name="1c3f" id="1c3f" class="graf graf--li graf-after--li">PIPT</li><li name="05e2" id="05e2" class="graf graf--li graf-after--li">MESI</li></ul><p name="2b0a" id="2b0a" class="graf graf--p graf-after--li">a. Yes, because S-&gt;M must invalidate others.</p><pre name="b4e2" id="b4e2" class="graf graf--pre graf-after--p">P0<br>--------------------<br>C     M      0xFFFFF</pre><pre name="eda6" id="eda6" class="graf graf--pre graf-after--pre">P1<br>--------------------<br>C     I      0xFFFFF</pre><p name="597b" id="597b" class="graf graf--p graf-after--pre">b. Yes, because write back to memory</p><pre name="005f" id="005f" class="graf graf--pre graf-after--p">P0<br>--------------------<br>5     M      0xFFFF0</pre><pre name="b529" id="b529" class="graf graf--pre graf-after--pre">P1<br>--------------------<br>5     M      0xFFFFF</pre><p name="26eb" id="26eb" class="graf graf--p graf-after--pre">c. No, S-&gt;S with no need to broadcast on the bus</p><pre name="1513" id="1513" class="graf graf--pre graf-after--p">P0<br>--------------------<br>3     S      0xFFFFF</pre><pre name="23bf" id="23bf" class="graf graf--pre graf-after--pre">P1<br>--------------------<br>3     S      0xFFFFF</pre><p name="bc0b" id="bc0b" class="graf graf--p graf-after--pre">d. No, E-&gt;M with no need to broadcast because the current block is exclusive</p><pre name="df77" id="df77" class="graf graf--pre graf-after--p">P0<br>--------------------<br>D     M      0xFFFFF</pre><pre name="e927" id="e927" class="graf graf--pre graf-after--pre">P1<br>--------------------<br>D     I      0xFFFF1</pre><p name="265d" id="265d" class="graf graf--p graf-after--pre">e. Yes, S-&gt;M and we have to invalidate others</p><pre name="4a7f" id="4a7f" class="graf graf--pre graf-after--p">P0<br>--------------------<br>7     M      0xFFFFF</pre><pre name="68ac" id="68ac" class="graf graf--pre graf-after--pre">P1<br>--------------------<br>7     I      0xFFFFF</pre><p name="c72a" id="c72a" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Example 5â2. MESI Protocol</strong></p><p name="ac23" id="ac23" class="graf graf--p graf-after--p">A shared-memory bus-based multiprocessor has two processors. Each processor has a small direct-mapped L1 cache, with only two 8-byte blocks in each cache. All addresses in this problem are 12-bit physical addresses, and all caches are physically indexed and tagged. The MESI coherence protocol is used in this multiprocessor. The initial state for the two caches is with tags of 0 and in <code class="markup--code markup--p-code">I</code> state.</p><p name="ba74" id="ba74" class="graf graf--p graf-after--p">a. What is the shortest sequence of accesses that will start with the initial state and result in the following cache state:</p><figure name="b3e0" id="b3e0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*aPcnDthOD_JmYklwPwEmGg.png" data-width="726" data-height="92" src="https://cdn-images-1.medium.com/max/800/1*aPcnDthOD_JmYklwPwEmGg.png"></figure><p name="a9b4" id="a9b4" class="graf graf--p graf-after--figure">b. What is the shortest sequence of accesses that will start with the initial state and result in the following cache state:</p><figure name="24dc" id="24dc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Xt8TrcauJe6Gr4FgIqAfmg.png" data-width="1746" data-height="206" src="https://cdn-images-1.medium.com/max/800/1*Xt8TrcauJe6Gr4FgIqAfmg.png"></figure><p name="a3f2" id="a3f2" class="graf graf--p graf-after--figure">c. What is the shortest sequence of accesses that will start with the initial state and result in the following cache state:</p><figure name="4ea6" id="4ea6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nPm2LCUnVbX8yzPHFUD-DA.png" data-width="780" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*nPm2LCUnVbX8yzPHFUD-DA.png"></figure><p name="efe0" id="efe0" class="graf graf--p graf-after--figure">d. What is the shortest sequence of accesses that will start with the initial state and result in the following cache state:</p><figure name="ab10" id="ab10" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*N-xgFt_NNVO2JsS-P6OfoA.png" data-width="1608" data-height="190" src="https://cdn-images-1.medium.com/max/800/1*N-xgFt_NNVO2JsS-P6OfoA.png"></figure><p name="7610" id="7610" class="graf graf--p graf-after--figure">e. Explain why the following state can not happen,</p><figure name="08df" id="08df" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6pz1suZwn1vVOcWSCyCQRw.png" data-width="1494" data-height="200" src="https://cdn-images-1.medium.com/max/800/1*6pz1suZwn1vVOcWSCyCQRw.png"></figure><p name="e129" id="e129" class="graf graf--p graf-after--figure">f. Explain why the following state can not happen,</p><figure name="b659" id="b659" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xKIoKooNFaTN1UfxkVPWaw.png" data-width="1494" data-height="200" src="https://cdn-images-1.medium.com/max/800/1*xKIoKooNFaTN1UfxkVPWaw.png"></figure><p name="a391" id="a391" class="graf graf--p graf-after--figure">Solution:</p><p name="4115" id="4115" class="graf graf--p graf-after--p">Summary of the cache,</p><ul class="postList"><li name="5521" id="5521" class="graf graf--li graf-after--p">L1 DMC</li><li name="645c" id="645c" class="graf graf--li graf-after--li">8-byte block size: 3-bit block offset</li><li name="70a0" id="70a0" class="graf graf--li graf-after--li">PIPT with 12-bit physical addresses</li><li name="3d77" id="3d77" class="graf graf--li graf-after--li">MESI</li><li name="d8c6" id="d8c6" class="graf graf--li graf-after--li">2 lines per cache: 1-bit index</li></ul><p name="3f85" id="3f85" class="graf graf--p graf-after--li">a. X means any value</p><pre name="61cb" id="61cb" class="graf graf--pre graf-after--p">P0     RD     0000 0001 0XXX<br>P1     RD     0000 0001 0XXX</pre><p name="b005" id="b005" class="graf graf--p graf-after--pre">b.</p><pre name="f38e" id="f38e" class="graf graf--pre graf-after--p">P0     RD     0000 0001 0XXX<br>P1     WR     0000 0001 0XXX</pre><p name="cb9b" id="cb9b" class="graf graf--p graf-after--pre">c. This requires us first write back, then read again from the memory</p><pre name="4d97" id="4d97" class="graf graf--pre graf-after--p">P0     RD     0000 0001 0XXX<br>P1     WR     0000 0001 0XXX<br>P0     RD     0000 0010 0XXX     // kick out line 0<br>P0     RD     0000 0001 0XXX     // kick out line 0</pre><p name="ca08" id="ca08" class="graf graf--p graf-after--pre">d.</p><pre name="4731" id="4731" class="graf graf--pre graf-after--p">P0     RD     0000 0001 0XXX<br>P1     RD     0000 0001 0XXX<br>P1     RD     0000 0010 0XXX</pre><p name="4772" id="4772" class="graf graf--p graf-after--pre">e. To have a <code class="markup--code markup--p-code">I</code> state in the P1 cache, the line 0 in P0 must be in either <code class="markup--code markup--p-code">M</code> state (means that it validate line 0 in P1 cache like b.) or <code class="markup--code markup--p-code">E</code> state (means the data is written to the memory and then read again like c.)</p><p name="604f" id="604f" class="graf graf--p graf-after--p">f. To have <code class="markup--code markup--p-code">I</code> state in both cache lines, we need to have a write operation in another core. Because there are only two cores, then this case is impossible.</p><p name="966e" id="966e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">6. Memory Consistency</strong></p><p name="e8af" id="e8af" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Memory Consistency</strong></p><p name="0b9b" id="0b9b" class="graf graf--p graf-after--p">Memory consistency determines the order of accesses by different cores, and it ensures that the program is the same as the execution order.</p><p name="6604" id="6604" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Sequential Consistency (SC)</strong></p><p name="3180" id="3180" class="graf graf--p graf-after--p">This is the simplest idea to make all the memory accesses in the program order by each processor. This will hurt the performance.</p><p name="98ca" id="98ca" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Relaxed Consistency (RC)</strong></p><p name="4b5c" id="4b5c" class="graf graf--p graf-after--p">We can release some of the rules in SC like a read-after-read operation. This is then called a relaxed consistency model.</p><p name="5096" id="5096" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) </strong><code class="markup--code markup--p-code"><strong class="markup--strong markup--p-strong">msync</strong></code><strong class="markup--strong markup--p-strong"> Instruction</strong></p><p name="647e" id="647e" class="graf graf--p graf-after--p">X86 <code class="markup--code markup--p-code">msync</code> instruction can be used as a boundary for maintaining memory consistency.</p><p name="b4e1" id="b4e1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Data Race</strong></p><p name="c3b8" id="c3b8" class="graf graf--p graf-after--p">This means the accesses to the same address by different cores that are not ordered by synchronization.</p><p name="8a05" id="8a05" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Data-Â­RaceÂ­-Free Program</strong></p><p name="fb7e" id="fb7e" class="graf graf--p graf-after--p">A data-race-free program is a program that can not create any data races in its executions with proper synchronizations.</p><p name="9a9b" id="9a9b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">7. Multiprocessing, Synchronization, Many Cores</strong></p><p name="4e14" id="4e14" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Flynnâs Taxonomy of Parallel Machines</strong></p><ul class="postList"><li name="3367" id="3367" class="graf graf--li graf-after--p">Single Instruction, Single Data (SISD): normal uniprocessor</li><li name="09ae" id="09ae" class="graf graf--li graf-after--li">Single Instruction, Multiple Data (SIMD): vector processor</li><li name="424c" id="424c" class="graf graf--li graf-after--li">Multiple Instruction, Single Data (MISD): stream processor</li><li name="98b2" id="98b2" class="graf graf--li graf-after--li">Multiple Instruction, Multiple Data (MIMD): normal multiprocessor.</li></ul><p name="2fc0" id="2fc0" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Uniform Memory Access Time (UMA): sharing main memory</strong></p><p name="4499" id="4499" class="graf graf--p graf-after--p">All cores have caches, but they will share the same main memory. Also called symmetric multiprocessor (SMP) or centralized shared memory. It works well for a small number of cores (&lt;16).</p><p name="2c4b" id="2c4b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Non-Uniform Memory Access Time (NUMA): distributed memory</strong></p><p name="f20e" id="f20e" class="graf graf--p graf-after--p">Each core will have its own cache and a slice of the main memory. Also called distributed shared memory.</p><p name="7c17" id="7c17" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Message Passing</strong></p><p name="7818" id="7818" class="graf graf--p graf-after--p">One way to communicate between processes is by passing messages. The hardware for this can be simple but programming can be hard.</p><p name="e7ee" id="e7ee" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Shared Memory</strong></p><p name="387c" id="387c" class="graf graf--p graf-after--p">Different cores can also communicate via the shared memory. We need synchronization (i.e. locking and unlocking) for the shared memory.</p><p name="8cb8" id="8cb8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Hardware Multithreading</strong></p><p name="b488" id="b488" class="graf graf--p graf-after--p">The hardware multithreading can be,</p><ul class="postList"><li name="faa1" id="faa1" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">coarse-grain: </strong>means changing the threads every few cycles</li><li name="7170" id="7170" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">fine-grain: </strong>means to change threads every cycle</li><li name="fb39" id="fb39" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">simultaneous multithreading</strong> (<strong class="markup--strong markup--li-strong">SMT</strong>, or <strong class="markup--strong markup--li-strong">Hyper-threading</strong>): means that in any given cycle, we could be doing instructions that belong to different threads.</li></ul><figure name="e201" id="e201" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*GJHas7gm2FrsXpzU.png" data-width="1522" data-height="348" src="https://cdn-images-1.medium.com/max/800/0*GJHas7gm2FrsXpzU.png"></figure><figure name="c38b" id="c38b" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="0*wL5MSNKQNAMfOtS1.png" data-width="1522" data-height="294" src="https://cdn-images-1.medium.com/max/800/0*wL5MSNKQNAMfOtS1.png"></figure><figure name="1841" id="1841" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="0*-04jeVmOQ5_0-TPX.png" data-width="1522" data-height="294" src="https://cdn-images-1.medium.com/max/800/0*-04jeVmOQ5_0-TPX.png"></figure><p name="59da" id="59da" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(7) SMT Hardware Requirements</strong></p><ul class="postList"><li name="654f" id="654f" class="graf graf--li graf-after--p">Add a program counter</li><li name="4da4" id="4da4" class="graf graf--li graf-after--li">Add a RAT</li><li name="2d5c" id="2d5c" class="graf graf--li graf-after--li">Add architectural registers</li><li name="6da4" id="6da4" class="graf graf--li graf-after--li">Can not use VIVT</li></ul><p name="fa94" id="fa94" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(8) Atomic Code Sections (Critical Sections)</strong></p><p name="b981" id="b981" class="graf graf--p graf-after--p">These are the sections of code that must be executed completely one at a time.</p><p name="c735" id="c735" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) Mutual Exclusion (Mutex)</strong></p><p name="4761" id="4761" class="graf graf--p graf-after--p">A type of synchronization used for atomic sections.</p><p name="7984" id="7984" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) Atomic Instructions</strong></p><p name="c860" id="c860" class="graf graf--p graf-after--p">To implement a mutex, we must use some hardware-supported atomic instructions. There are mainly three types of them,</p><ul class="postList"><li name="8a22" id="8a22" class="graf graf--li graf-after--p">Atomic exchange instructions</li><li name="c691" id="c691" class="graf graf--li graf-after--li">Test-then-write family</li><li name="58f1" id="58f1" class="graf graf--li graf-after--li">Load Link/Store Conditional (LL/SC) instructions</li></ul><p name="d463" id="d463" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(11) Barrier</strong></p><p name="9a8f" id="9a8f" class="graf graf--p graf-after--p">The barrier is another way to maintain synchronization, and this will be used when several threads are allowed to access an atomic section at a time. A typical barrier implementation has two variables,</p><ul class="postList"><li name="3e10" id="3e10" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">A counter</strong>: counts how many threads arrived</li><li name="4339" id="4339" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">A flag</strong>: gets set when the counter reaches N</li></ul><pre name="abd3" id="abd3" class="graf graf--pre graf-after--li">typedef struct barrier_t {<br>    int count;<br>    int flag;<br>} barrier_t;</pre><p name="2b91" id="2b91" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(12) Reuse Barrier: Flippable Local Sense</strong></p><p name="47ca" id="47ca" class="graf graf--p graf-after--p">To solve the deadlock problem, we must reuse the barrier with flippable local sense. This means that after passing the first barrier, the local sense for the flag will be flipped.</p><p name="bbf1" id="bbf1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(13) Many-Core Challenges</strong></p><ul class="postList"><li name="65ae" id="65ae" class="graf graf--li graf-after--p">Bus Bottleneck: we need efficient bus designs like mesh</li><li name="7499" id="7499" class="graf graf--li graf-after--li">Off-Chip Traffic Bottleneck: we have to share the LLC or make it distributed</li><li name="d46d" id="d46d" class="graf graf--li graf-after--li">Over Large Coherence Directory: conduct directory slicing or partial directory</li><li name="3bb8" id="3bb8" class="graf graf--li graf-after--li">Power Budget for Cores: boost the frequency of the active cores</li><li name="26e2" id="26e2" class="graf graf--li graf-after--li">OS Confusion: beyond our scope of discussion</li></ul><p name="d1f6" id="d1f6" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Example 7-1. Parallel Processing</strong></p><p name="a1ee" id="a1ee" class="graf graf--p graf-after--p">A shared-memory system with two processors is executing the following two code fragments in parallel (one on each processor). All variables are shared and are initially zeros. The system uses sequential consistency.</p><p name="a9a2" id="a9a2" class="graf graf--p graf-after--p">Processor 0 executes,</p><pre name="4d84" id="4d84" class="graf graf--pre graf-after--p">A = 10;<br>Atmp = A;<br>B = Atmp - 2;<br>while (C == 0);<br>C = 6;<br>printf(&quot;%d %d %d\n&quot;, A, B, C);</pre><p name="fd53" id="fd53" class="graf graf--p graf-after--pre">Processor 1 executes,</p><pre name="d9bc" id="d9bc" class="graf graf--pre graf-after--p">Btmp = B;<br>C = Btmp;<br>if(C == 0)<br>    C = 3;<br>A = 3;</pre><p name="8a28" id="8a28" class="graf graf--p graf-after--pre">a. Is it possible for the printout in P0 to be <code class="markup--code markup--p-code">10 8 6</code>. If yes, show the execution interleaving that produces this printout. If no, explain.</p><p name="7fe9" id="7fe9" class="graf graf--p graf-after--p">b. Is it possible for P0 to get stuck and never print out anything? If yes, show the execution interleaving that produces this printout. If no, explain.</p><p name="0e03" id="0e03" class="graf graf--p graf-after--p">c. Is it possible for P0 to print 3 as the value of C? If yes, show the execution interleaving that produces this printout. If no, explain.</p><p name="0664" id="0664" class="graf graf--p graf-after--p">d. Is it possible for P0 to print a value of C other than 3 or 6? If yes, show the execution interleaving that produces this printout. If no, explain.</p><p name="ab0d" id="ab0d" class="graf graf--p graf-after--p">e. Is it possible for P0 to print a value of B other than 8? If yes, show the execution interleaving that produces this printout. If no, explain.</p><p name="fe64" id="fe64" class="graf graf--p graf-after--p">Solution:</p><p name="a059" id="a059" class="graf graf--p graf-after--p">a. Yes. A possible interleaving can be,</p><pre name="db30" id="db30" class="graf graf--pre graf-after--p">A = 10;                            // A = 10<br>Atmp = A;<br>B = Atmp - 2;                      // B = 8<br>Btmp = B;                          <br>C = Btmp;                          // C = 8<br>if(C == 0)<br>    C = 3;  <br>while (C == 0);<br>C = 6;                             // C = 6<br>printf(&quot;%d %d %d\n&quot;, A, B, C);     // print<br>A = 3;</pre><p name="010a" id="010a" class="graf graf--p graf-after--pre">b. No. This is not possible because the P1 program will not be stuck and if it finds a C = 0, C will be assigned to 3. So that P0 can no longer be stuck in the while loop after that.</p><p name="7d80" id="7d80" class="graf graf--p graf-after--p">c. No. C will become 3 only if in the program P0 C is 0 before the if statement. But if C is not 0, it will pass the while loop and finally be set to 6 in the end. Therefore, we can not achieve a printout of 3 for C.</p><p name="e38a" id="e38a" class="graf graf--p graf-after--p">d. No. As we have said in C if C begins with value 0, we can not achieve a printout of C other than 6.</p><p name="d1b7" id="d1b7" class="graf graf--p graf-after--p">e. Yes. A possible interleaving can be,</p><pre name="ec9b" id="ec9b" class="graf graf--pre graf-after--p">A = 10;               // A = 10<br>Btmp = B;<br>C = Btmp;             // C = 0<br>if(C == 0)<br>    C = 3;            // C = 3<br>A = 3;                // A = 3<br>Atmp = A;             <br>B = Atmp - 2;         // B = 1<br>while (C == 0);<br>C = 6;                // C = 6<br>printf(&quot;%d %d %d\n&quot;, A, B, C);         // print</pre><p name="4d81" id="4d81" class="graf graf--p graf-after--pre">In this case, the printout for B in P0 should be 1.</p><p name="e4c9" id="e4c9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 7-2. Barrier</strong></p><p name="d152" id="d152" class="graf graf--p graf-after--p">Two shared-memory systems use different implementations of the sense-reversal barrier:</p><figure name="4272" id="4272" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ixr86Qu_4T8WtHh-Ycct9A.png" data-width="1344" data-height="582" src="https://cdn-images-1.medium.com/max/800/1*Ixr86Qu_4T8WtHh-Ycct9A.png"></figure><p name="a644" id="a644" class="graf graf--p graf-after--figure">Variable <code class="markup--code markup--p-code">locSens</code> is local in each thread, and variable <code class="markup--code markup--p-code">cntlock</code>, <code class="markup--code markup--p-code">cnt</code>, <code class="markup--code markup--p-code">tot</code>, and <code class="markup--code markup--p-code">rel</code> are global variables shared by all threads. All these variables start out with zero values, except <code class="markup--code markup--p-code">cntlock</code> (which is correctly initialized using <code class="markup--code markup--p-code">pthread_mutex_init</code>).</p><p name="3d15" id="3d15" class="graf graf--p graf-after--p">Whereas <strong class="markup--strong markup--p-strong">Implementation 1</strong> is correct, <strong class="markup--strong markup--p-strong">Implementation 2</strong> does not. Show a (sequentially consistent) execution order for two threads (<code class="markup--code markup--p-code">tot</code> is 2) in Implementation 2 that leads to incorrect behavior.</p><p name="d545" id="d545" class="graf graf--p graf-after--p">An execution order (interleaving) can be represented as a sequence of [Core #: Line #] events, using âWorkâ instead of a line number for actual work between calls to the barrier, e.g. [C1: L1], [C2: L1], [C1: L2,L3,L4], [C2: L2,L3,L4], [C1: L5], [C2: L5], [C1: L6], [C2: L6], [C1: L7], [C2: L7], [C1: Work], [C2: Work], [C1: L1], [C2: L1], etc. leads to correct behavior.</p><p name="d712" id="d712" class="graf graf--p graf-after--p">Solution:</p><pre name="014b" id="014b" class="graf graf--pre graf-after--p">[C1: L1, L2, L3, L4, L5, L9]      // C1 waiting<br>[C2: L1, L2, L3, L4, L5, L6]      // C2 release the rel<br>[C1: L9]                          // C1 continue<br>[C1: Work]                        // C1 actual work<br>[C1: L1, L2, L3, L4, L5, L9]      // cnt == 3 without reset by C2<br>[C2: L7]                          // cnt == 0 reset by C2<br>[C2: Work]                        // C2 actual work<br>[C2: L1, L2, L3, L4, L5, L9]      // cnt == 1 so C2 have to wait</pre><pre name="45bd" id="45bd" class="graf graf--pre graf-after--pre">Because now both are waiting for each other at L9, this leads to incorrect behavior.</pre><p name="1229" id="1229" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Final Example. Potpourri</strong></p><p name="39ef" id="39ef" class="graf graf--p graf-after--p">True or False</p><ul class="postList"><li name="7a71" id="7a71" class="graf graf--li graf-after--p">Increasing cache associativity reduces compulsory misses.</li><li name="5163" id="5163" class="graf graf--li graf-after--li">A page fault requires a page table walk.</li><li name="2df2" id="2df2" class="graf graf--li graf-after--li">Context switch requires TLB flush.</li><li name="27a1" id="27a1" class="graf graf--li graf-after--li">For a highly irregular memory access pattern, the open-page DRAM policy will decrease the average DRAM access time.</li><li name="aeab" id="aeab" class="graf graf--li graf-after--li">A system with an inclusive L2 cache has a higher effective capacity than a system with a non-inclusive L2 of the same size.</li></ul><p name="b98b" id="b98b" class="graf graf--p graf-after--li">Solution:</p><ul class="postList"><li name="2355" id="2355" class="graf graf--li graf-after--p">False. It will reduce conflict misses.</li><li name="80d9" id="80d9" class="graf graf--li graf-after--li">False. A page fault means the page is not in the table and requires OS intervention to bring that page to DRAM after the page table walk.</li><li name="a7d4" id="a7d4" class="graf graf--li graf-after--li">True. When we do context switch, we need to flush the whole TLB.</li><li name="abdf" id="abdf" class="graf graf--li graf-after--li">False. An irregular memory access pattern will increase the average DRAM access time.</li><li name="d3f6" id="d3f6" class="graf graf--li graf-after--li graf--trailing">False. An inclusive L2 cache has a lower effective capacity because it contains all the cache lines in the L1 cache.</li></ul></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/2350dee247c3"><time class="dt-published" datetime="2021-04-30T20:22:45.739Z">April 30, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/high-performance-computer-architecture-35-final-review-2350dee247c3" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>