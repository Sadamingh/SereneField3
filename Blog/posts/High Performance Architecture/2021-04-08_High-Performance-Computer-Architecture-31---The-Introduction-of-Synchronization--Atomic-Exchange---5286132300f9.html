<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>High-Performance Computer Architecture 31 | The Introduction of Synchronization, Atomic Exchange …</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">High-Performance Computer Architecture 31 | The Introduction of Synchronization, Atomic Exchange …</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: High-Performance Computer Architecture
</section>
<section data-field="body" class="e-content">
<section name="d938" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="c4e8" id="c4e8" class="graf graf--h3 graf--leading graf--title">High-Performance Computer Architecture 31 | <strong class="markup--strong markup--h3-strong">The Introduction of Synchronization, Atomic Exchange Instructions, Test-then-Write Family, Load Link, Store Conditional, Barrier Synchronization, and Flippable Local Sense</strong></h3><figure name="f6e0" id="f6e0" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*SPxRHxMEeP0r3GJU.png" data-width="1446" data-height="864" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*SPxRHxMEeP0r3GJU.png"></figure><ol class="postList"><li name="ff92" id="ff92" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">The Introduction of Synchronization</strong></li></ol><p name="881f" id="881f" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) The Definition of Synchronizations</strong></p><p name="4a52" id="4a52" class="graf graf--p graf-after--p">Because we can have multicores and multi-threads that are trying to work together in the same program, we will need synchronization. For example, let’s say we have two threads and what they are doing is counting the occurrences of different letters in an essay. The first thread is called thread A, which will be counting the first half of the document, and the second one is called B, which will count the second half.</p><figure name="e934" id="e934" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*oKY4PvT0aVPwtPaPBRueGg.png" data-width="1432" data-height="544" src="https://cdn-images-1.medium.com/max/800/1*oKY4PvT0aVPwtPaPBRueGg.png"></figure><p name="1ba4" id="1ba4" class="graf graf--p graf-after--figure">Suppose the programs we have for thread A and B are,</p><pre name="ec08" id="ec08" class="graf graf--pre graf-after--p">// for thread A<br>LW  I, 0(R1)<br>LW  C, Count[I]<br>ADD C, C, I<br>SW  C, Count[I]</pre><pre name="9435" id="9435" class="graf graf--pre graf-after--pre">// for thread B<br>LW  I, 0(R1)<br>LW  C, Count[I]<br>ADD C, C, I<br>SW  C, Count[I]</pre><p name="3030" id="3030" class="graf graf--p graf-after--pre">And it is clear to find out that they are the same because we are operating on the same <code class="markup--code markup--p-code">Count[i]</code> array. Let’s suppose that at a time, both of the threads A and B read from the document and get the same latter A, then they will simultaneously index to the same position and load the same counting value. Suppose we have 15 in the <code class="markup--code markup--p-code">Count</code> array before reading these two letter <code class="markup--code markup--p-code">A</code>s,</p><pre name="73ae" id="73ae" class="graf graf--pre graf-after--p">// for thread A<br>LW  I, 0(R1)          // -&gt; I = &#39;A&#39;<br>LW  C, Count[I]       // get C = 15<br>ADD C, C, I           // C = 16<br>SW  C, Count[I]       // Count[&#39;A&#39;] = 16</pre><pre name="c6d5" id="c6d5" class="graf graf--pre graf-after--pre">// for thread B<br>LW  I, 0(R1)          // -&gt; I = &#39;A&#39;<br>LW  C, Count[I]       // get C = 15<br>ADD C, C, I           // C = 16<br>SW  C, Count[I]       // Count[&#39;A&#39;] = 16</pre><p name="ef74" id="ef74" class="graf graf--p graf-after--pre">However, both of these two threads will write 16 back to the program and this is not correct. What we actually need is that we want to increase letter A in sequence and get a final result of 17. Therefore, this is definitely an incorrect behavior against what we expect.</p><p name="7274" id="7274" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Synchronization Example</strong></p><p name="0fe6" id="0fe6" class="graf graf--p graf-after--p">Now, let’s see a real program that shows us why should we maintain synchronization for a multithreading program. If you can not understand the following code, the following articles may help.</p><div name="5658" id="5658" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/adamedelwiess/operating-system-8-a-tutorial-for-the-pthread-programming-bf8004e36e70" data-href="https://medium.com/adamedelwiess/operating-system-8-a-tutorial-for-the-pthread-programming-bf8004e36e70" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/operating-system-8-a-tutorial-for-the-pthread-programming-bf8004e36e70"><strong class="markup--strong markup--mixtapeEmbed-strong">Operating System 8 | A Tutorial for the PThread Programming</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Operating System</em>medium.com</a><a href="https://medium.com/adamedelwiess/operating-system-8-a-tutorial-for-the-pthread-programming-bf8004e36e70" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="1ef7ba5cd8172d0815935b9bf5680ef0" data-thumbnail-img-id="0*q3b7G95zDTLqCKx1.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*q3b7G95zDTLqCKx1.png);"></a></div><div name="008b" id="008b" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/adamedelwiess/operating-system-9-file-i-o-experiment-d4f1191bd358" data-href="https://medium.com/adamedelwiess/operating-system-9-file-i-o-experiment-d4f1191bd358" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/operating-system-9-file-i-o-experiment-d4f1191bd358"><strong class="markup--strong markup--mixtapeEmbed-strong">Operating System 10 | File I/O Experiment</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Operating System</em>medium.com</a><a href="https://medium.com/adamedelwiess/operating-system-9-file-i-o-experiment-d4f1191bd358" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="ba708c364222c1879cd29c8d60fccb91" data-thumbnail-img-id="0*T7MhJKENM9uZ7CC9.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*T7MhJKENM9uZ7CC9.png);"></a></div><p name="a3cf" id="a3cf" class="graf graf--p graf-after--mixtapeEmbed">What we are going to do is that we would like to implement the program we have discussed. We are going to create 2 threads and one thread counts the first half of the file and the second one counts the second half of the file. To make things clear, we will have 320 letter A in the document called <code class="markup--code markup--p-code">text.txt</code> with the first half and the second half separated by an escape character <code class="markup--code markup--p-code">\n</code>. We expect to count 320 of A as a result, but if we have a number smaller than that, then we probably have some synchronization problems.</p><p name="8a33" id="8a33" class="graf graf--p graf-after--p">The document should be,</p><figure name="1c90" id="1c90" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/96852eb8780f52993e22902e481e2148.js"></script></figure><p name="bb4e" id="bb4e" class="graf graf--p graf-after--figure">And the program should be,</p><figure name="bae0" id="bae0" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/aa725de54605876cd27d44f84a206140.js"></script></figure><p name="ff0a" id="ff0a" class="graf graf--p graf-after--figure">If we run the code above several times, we may probably get the following results,</p><pre name="b1af" id="b1af" class="graf graf--pre graf-after--p">A: 320<br>A: 320<br><strong class="markup--strong markup--pre-strong">A: 302</strong><br>A: 320<br><strong class="markup--strong markup--pre-strong">A: 293</strong><br>...</pre><p name="cbef" id="cbef" class="graf graf--p graf-after--pre">You can find out that even though some of them count the write value as we expect, there can be some cases we have a synchronization problem.</p><p name="7c00" id="7c00" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) The Definition of Atomic (Critical) Section</strong></p><p name="4848" id="4848" class="graf graf--p graf-after--p">In the last example, we don’t want both of the threads loading from the memory and get a value of 15. What we want is that these two threads can execute in sequence. Therefore, we may want to exclusively execute some of the code in the program as follows,</p><pre name="7187" id="7187" class="graf graf--pre graf-after--p">LW  I, 0(R1)<br>// exclusive ============<strong class="markup--strong markup--pre-strong"><br>LW  C, Count[I]<br>ADD C, C, I<br>SW  C, Count[I]<br></strong>// exclusive end ========</pre><p name="7114" id="7114" class="graf graf--p graf-after--pre">If the bold codes are exclusively executed among threads, we can avoid the interleaved sections of those two threads, and they should happen one at a time. The code section that can only happen one at a time is called the <strong class="markup--strong markup--p-strong">critical section</strong> or the <strong class="markup--strong markup--p-strong">atomic section</strong>.</p><p name="e008" id="e008" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) The Definition of Mutex (Lock)</strong></p><p name="514e" id="514e" class="graf graf--p graf-after--p">To implement a critical section, we have to use synchronization called <strong class="markup--strong markup--p-strong">mutual exclusive </strong>(aka. <strong class="markup--strong markup--p-strong">mutex</strong>, or <strong class="markup--strong markup--p-strong">lock</strong>). So what we really need before we get into the critical section, we need to lock a <strong class="markup--strong markup--p-strong">global mutex </strong>(means that this mutex should be in the shared memory of all threads that requires this lock), which can be seen by all the other threads, so that if there are any other threads who would like to lock the mutex will be blocked and these threads will spin there until the mutex becomes unlocked. Once we can grab the mutex, we are able to enter the critical section. In the previous example, we are going to maintain an array of mutexes <code class="markup--code markup--p-code">Countlock</code> for each counter. Therefore, the code for the two threads above should be,</p><pre name="3285" id="3285" class="graf graf--pre graf-after--p">LW  I, 0(R1)<br>lock Countlock[I]      // lock<strong class="markup--strong markup--pre-strong"><br>LW  C, Count[I]<br>ADD C, C, I<br>SW  C, Count[I]<br></strong>unlock Countlock[I]    // unlock</pre><p name="16fa" id="16fa" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(5) Mutex Implementation: A Paradox</strong></p><p name="d1b9" id="d1b9" class="graf graf--p graf-after--p">Now, let’s see how we can implement a mutex. As we have said, the mutex is only a global data structure in the shared memory as a normal variable, and it can be used to control whether we can access a thread or not. Therefore, we can use a simple <code class="markup--code markup--p-code">int</code> type to implement a mutex, and also we will write some functions for it. In our example, we will initialize mutex as 0 in the beginning, which means that we can lock and access the critical section. After the mutex is set to 1, the other threads acquire the lock will be blocked.</p><p name="2f50" id="2f50" class="graf graf--p graf-after--p">So the <code class="markup--code markup--p-code">mutex_type</code> is defined by,</p><pre name="b639" id="b639" class="graf graf--pre graf-after--p">typedef int mutex_type;</pre><p name="577a" id="577a" class="graf graf--p graf-after--pre">And the initialization function should be,</p><pre name="085b" id="085b" class="graf graf--pre graf-after--p">void mutex_init(mutex_type *mu) {<br>    *mu = 0;<br>}</pre><p name="77cd" id="77cd" class="graf graf--p graf-after--pre">The locking function should be,</p><pre name="3ff1" id="3ff1" class="graf graf--pre graf-after--p">void mutex_lock(mutex_type *mu) {<br>    while (*mu == 1) {}<br>    *mu = 1;<br>}</pre><p name="a144" id="a144" class="graf graf--p graf-after--pre">And the unlock function should be,</p><pre name="cdf4" id="cdf4" class="graf graf--pre graf-after--p">void mutex_unlock(mutex_type *mu) {<br>    *mu = 0;<br>}</pre><p name="7855" id="7855" class="graf graf--p graf-after--pre">After implementing these in our previous code, we can have,</p><figure name="6063" id="6063" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/70676b5bd5c2ffd3cd4d521b83fda0b0.js"></script></figure><p name="51f5" id="51f5" class="graf graf--p graf-after--figure">If we run the code above several times, we may probably get the following results,</p><pre name="2112" id="2112" class="graf graf--pre graf-after--p">A: 320<br><strong class="markup--strong markup--pre-strong">A: 294<br>A: 299</strong><br>A: 320<br><strong class="markup--strong markup--pre-strong">A: 319</strong><br>...</pre><p name="c1c5" id="c1c5" class="graf graf--p graf-after--pre">It seems that our mutex is not working and we still have synchronization problems. Well, how does this happen? To answer this question, let’s review the <code class="markup--code markup--p-code">mutex_lock</code> function.</p><pre name="a8a0" id="a8a0" class="graf graf--pre graf-after--p">void mutex_lock(mutex_type *mu) {<br>    while (*mu == 1) {}<br>    *mu = 1;<br>}</pre><p name="b37e" id="b37e" class="graf graf--p graf-after--pre">Suppose we have two threads now and when they execute <code class="markup--code markup--p-code">while (*mu == 1) {}</code> simultaneously. Then both of them will consider mu = 0 and they will not be locked. Therefore, we will result in some synchronization problems.</p><p name="133d" id="133d" class="graf graf--p graf-after--p">In order to deal with this, we should have a <strong class="markup--strong markup--p-strong">magic lock</strong> that does not rely on the mutex, and it will make the <code class="markup--code markup--p-code">while</code> statement mutually exclusive among all the threads.</p><pre name="5f4f" id="5f4f" class="graf graf--pre graf-after--p">void mutex_lock(mutex_type *mu) {<br>    <strong class="markup--strong markup--pre-strong">magic_lock(lock);</strong><br>    while (*mu == 1) {}<br>    *mu = 1;<br>    <strong class="markup--strong markup--pre-strong">magic_unlock(lock);</strong><br>}</pre><p name="2383" id="2383" class="graf graf--p graf-after--pre">So now here’s the paradox. If we have this magic lock, we don’t have to implement a mutex because the magic lock can be used as the mutex. However, if we don’t have this magic lock, we can not implement the mutex either. So in conclusion, we need a mutex to implement a mutex, which can be impossible for us, and what’s more, there is also no magic.</p><p name="5610" id="5610" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Mutex Implementation: Solutions</strong></p><p name="db95" id="db95" class="graf graf--p graf-after--p">Now, let’s see what do we do if there’s no magic. There are several ways to get the effect that the magic lock will give us,</p><ul class="postList"><li name="dac1" id="dac1" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Lamport’s Bakery Algorithm</strong>: this algorithm is able to use normal loads and saves to replace the magic lock. But this algorithm can be too complicated with tens of instructions per lock. So it can be <strong class="markup--strong markup--li-strong">slow</strong> and <strong class="markup--strong markup--li-strong">expensive</strong>.</li><li name="fa69" id="fa69" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Special Atomic R/W Instructions</strong>: another way is to use so-called special atomic read and write instructions. So there would be an instruction that does this check and set to 1 and that will allow us to have only one instruction access the critical section without needing the magic. These instructions can be briefly called <strong class="markup--strong markup--li-strong">atomic instructions</strong>.</li></ul><p name="3eb9" id="3eb9" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(7) Common Atomic Instructions</strong></p><p name="712e" id="712e" class="graf graf--p graf-after--p">There are three main types of atomic instructions. These include atomic exchange instructions, test-then-write instruction family, and LL/SC instructions.</p><p name="b7e6" id="b7e6" class="graf graf--p graf-after--p">We are going to introduce them in the following sections.</p><p name="b6dc" id="b6dc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Atomic Exchange Instructions</strong></p><p name="6bc5" id="6bc5" class="graf graf--p graf-after--p">For example,</p><pre name="1496" id="1496" class="graf graf--pre graf-after--p">EXCH R1, 8(R2)</pre><p name="b1d4" id="b1d4" class="graf graf--p graf-after--pre">this instruction means that we are going to swap the data in R1 and in the address of <code class="markup--code markup--p-code">8(R2)</code> in the memory. Therefore, we will conduct a load instruction from the memory and write to R1, and we will also conduction a save instruction from R1 to <code class="markup--code markup--p-code">8(R2)</code>. When we have this instruction, what we can do is that, we will write 1 to the R1 register and we will looply check the value of R1. Suppose we have two threads acquiring the same mutex = 0, we will do one more check on the R1 after it gets the value of 0 from mu. Therefore, only one thread can be unlocked and we will lock the mutex properly.</p><pre name="9c00" id="9c00" class="graf graf--pre graf-after--p">R1 = 1;<br>while (R1 == 1) {<br>    EXCH R1, mu;<br>}</pre><p name="397b" id="397b" class="graf graf--p graf-after--pre">The drawback of this is it continues to swap while waiting to acquire the lock and we can avoid the unnecessary swaps by the test-then-write instructions.</p><p name="f717" id="f717" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) Test-then-Write Family</strong></p><p name="9f0a" id="9f0a" class="graf graf--p graf-after--p">We also have a test-then-write atomic instruction family, which will test some conditions before we conduct the instructions. For example,</p><pre name="926e" id="926e" class="graf graf--pre graf-after--p">TSTSW R1, 8(R2)</pre><p name="be29" id="be29" class="graf graf--p graf-after--pre">This means that before we store the value of R1 to <code class="markup--code markup--p-code">8(R2)</code>, we will first test whether <code class="markup--code markup--p-code">8(R2) = 0</code>. If it is not 0, we will not write to it. So this method corrects the drawback of the Atomic Exchange.</p><p name="c0ff" id="c0ff" class="graf graf--p graf-after--p">For example, if we have the <code class="markup--code markup--p-code">TSET</code> instruction of,</p><pre name="1a22" id="1a22" class="graf graf--pre graf-after--p">TSET R1, Addr</pre><p name="614a" id="614a" class="graf graf--p graf-after--pre">which is the same as,</p><pre name="e298" id="e298" class="graf graf--pre graf-after--p">if (Mem[Addr] == 0) {<br>    Mem[Addr] = 1;<br>    R1 = 1;<br>} else {<br>    R1 = 0;<br>}</pre><p name="15c7" id="15c7" class="graf graf--p graf-after--pre">Then the mutex lock can be implemented by,</p><pre name="981b" id="981b" class="graf graf--pre graf-after--p">void mutex_lock(mutex_type *mu) {<br>    R1 = 0;<br>    while (R1 == 0) {<br>        TSET R1, mu;<br>    }<br>}</pre><p name="c629" id="c629" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(10) Load Linked (LL)</strong> / <strong class="markup--strong markup--p-strong">Store Conditional (SC)</strong></p><p name="2922" id="2922" class="graf graf--p graf-after--p">The test-then-write instructions are good but it is hard to implement because it is actually a strange instruction, and it is neither a store or load. It would be ideal for processor design if we could do something like load&amp;store yet behaves in the way of a test-then-write instruction. This brings us to the <strong class="markup--strong markup--p-strong">linked load </strong>(<strong class="markup--strong markup--p-strong">LL</strong>)<strong class="markup--strong markup--p-strong"> </strong>and the <strong class="markup--strong markup--p-strong">store conditional</strong> (<strong class="markup--strong markup--p-strong">SC</strong>) instructions.</p><p name="d13f" id="d13f" class="graf graf--p graf-after--p">The reason why this is bad for the processor design is that this is bad for the pipeline. Suppose we have a normal five-stage pipeline and we need to implement atomic read and write in the instruction, we can not finish both load and store in a single M stage. Therefore, in order to implement this, we need to have more M stages (e.g. M1 and M2) as follows,</p><pre name="02ab" id="02ab" class="graf graf--pre graf-after--p">F D A M W -&gt; F D A M1 M2 W</pre><p name="da92" id="da92" class="graf graf--p graf-after--pre">So the basic idea is that we should only one thread can read from the mutex at a time. After the mutex is read by one thread, all the other threads should know that it is already read by some information accessible for all the other threads. The structure LL and SC use to share the information is called the <strong class="markup--strong markup--p-strong">link register</strong>. Let’s now see an example. Suppose we have the following code,</p><pre name="6382" id="6382" class="graf graf--pre graf-after--p">LL R1, mu<br>SC R1, mu</pre><p name="4050" id="4050" class="graf graf--p graf-after--pre">What we need is that if the LL finds the lock variable mu available, and we try to do an SC, then the SC will only succeed when if the lock is still available at that time. The key is that if we snoop a write to the lock variable after LL, we will put a 0 into the link register. Then if the LL loads the lock variable and we see that it is free, and then we will try to do an SC. If some other thread beats us to it, then SC will fail because the link register will not match the local variable.</p><p name="f126" id="f126" class="graf graf--p graf-after--p">Let’s suppose we have to read 12 from the memory, increase by 1, and put it back to the memory. Assume we have two threads, and they will load from the memory simultaneously. Then we have,</p><figure name="54d7" id="54d7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Lze2HUOawXvQX2W4bVR0Fg.png" data-width="1432" data-height="320" src="https://cdn-images-1.medium.com/max/800/1*Lze2HUOawXvQX2W4bVR0Fg.png"></figure><p name="7d9d" id="7d9d" class="graf graf--p graf-after--figure">At the beginning, both of the threads will LL 12 from the memory to R1 because the mutex is initially 0. After the load, the mutex will be set to 1 in order to block the other threads.</p><figure name="055c" id="055c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*myOjfceYGcVyHJgM5xNkTQ.png" data-width="1432" data-height="320" src="https://cdn-images-1.medium.com/max/800/1*myOjfceYGcVyHJgM5xNkTQ.png"></figure><p name="5edb" id="5edb" class="graf graf--p graf-after--figure">For normal loads and saves, we will then increase 12 to 13 and write it back to the memory. It is obvious that we will have a synchronization problem. However, for LL and SC, after we increase the value in R1 from 12 to 13, the first thread writes to the memory will first check if the mutex matches the linked register. If these values match, we can then save the value to the memory,</p><figure name="05c4" id="05c4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vTQ3yykLmD_loP7ZpwSCZQ.png" data-width="1432" data-height="320" src="https://cdn-images-1.medium.com/max/800/1*vTQ3yykLmD_loP7ZpwSCZQ.png"></figure><p name="29cd" id="29cd" class="graf graf--p graf-after--figure">After the first thread writes to the memory, the link register will be set to 0. Then before the second thread writes to the memory, it will also check if the mutex matches the linked register. However, because the link register is set to 0 because of the first thread, the value in the mutex doesn’t match the value in the link register. Then the SC instruction figures out we have a synchronization problem. So what we are going to do next is that we are going to abort the value in R1 and retry LL from the memory.</p><figure name="88ed" id="88ed" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6qh2vCG93Yf1zCtRK7qRQw.png" data-width="1432" data-height="320" src="https://cdn-images-1.medium.com/max/800/1*6qh2vCG93Yf1zCtRK7qRQw.png"></figure><p name="2eea" id="2eea" class="graf graf--p graf-after--figure">Now, if we continue the second thread, we will get the correct value and then the correct value will be written to the memory. Now, we perfectly deal with the synchronization problem.</p><figure name="72df" id="72df" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3ieW30nUsYBycJkBcaDxHA.png" data-width="1432" data-height="320" src="https://cdn-images-1.medium.com/max/800/1*3ieW30nUsYBycJkBcaDxHA.png"></figure><p name="a114" id="a114" class="graf graf--p graf-after--figure">So the pseudocode for implementing this LL/SC approach is,</p><pre name="b3d8" id="b3d8" class="graf graf--pre graf-after--p">void mutex_lock(mutex_type *mu) {<br>    trylock:<br>        MOV    R1, 1;          // set a variable = 1 <br>        LL     R2, mu;         // load mu to R2<br>        SC     R1, mu;         // check whether mu = R1<br>        BNEZ   R2, trylock;    // if mu = 0, we should retry<br>        BNEZ   R1, trylock;    // if not match, we should retry<br>}</pre><p name="8b2d" id="8b2d" class="graf graf--p graf-after--pre">Note that we can not explicitly read from the index register.</p><p name="d474" id="d474" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(11) Lock Performance</strong></p><p name="ab37" id="ab37" class="graf graf--p graf-after--p">Let’s now consider an example of how the atomic exchange instructions interact with coherence protocols and what is the performance of the lock. Initially, the mutex is initialized to 0, and the R1 register in each core is set to 1. All the caches are empty in the beginning.</p><figure name="a501" id="a501" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*hxMj4uw30D0lp4IC0VY8Gg.png" data-width="1616" data-height="510" src="https://cdn-images-1.medium.com/max/800/1*hxMj4uw30D0lp4IC0VY8Gg.png"></figure><p name="81b9" id="81b9" class="graf graf--p graf-after--figure">Let’s say that core 0 tries to grab the mutex first and successes. Because the mutex is initially in the memory with a value of 0, we have to read this value to cache #1. After that, we will exchange the value in R1 and mutex mu so that the block mu will be in the M state.</p><figure name="bf79" id="bf79" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*H1DlYRmMGNedQE_jfz1Ing.png" data-width="1616" data-height="618" src="https://cdn-images-1.medium.com/max/800/1*H1DlYRmMGNedQE_jfz1Ing.png"></figure><p name="c8b0" id="c8b0" class="graf graf--p graf-after--figure">Before mu is unlocked by core #1, let’s suppose then core 2 requires the lock and tries to enter the critical section. However, this will fail because at this time, we will not require the mutex from the memory. Instead, there will be a cache-to-cache transfer from cache #1 to cache #2. After this transfer, the corresponding block in cache #1 will be set to the I state and the corresponding block in cache #2 becomes the M state.</p><figure name="3fda" id="3fda" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*r-LBcogoPF-Whn4iahcYYw.png" data-width="1616" data-height="702" src="https://cdn-images-1.medium.com/max/800/1*r-LBcogoPF-Whn4iahcYYw.png"></figure><p name="41dd" id="41dd" class="graf graf--p graf-after--figure">Suppose core #3 then acquires the same mutex mu and then it can block similar to core #2.</p><figure name="6794" id="6794" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1Zuv55_qOFC0_3Y8VUO_tA.png" data-width="1616" data-height="764" src="https://cdn-images-1.medium.com/max/800/1*1Zuv55_qOFC0_3Y8VUO_tA.png"></figure><p name="67f2" id="67f2" class="graf graf--p graf-after--figure">As long as the lock is still busy, core #2 and core #3 will be spinning in the loop trying to acquire the mutex mu. So what will happen is that cache #2 to cache #3 transfers and cache #3 to cache #2 transfers are going to happen many many times. So finally, what we can view is an I-M state swap of these two blocks. Each time the state swaps, there will be communications on the shared bus and also we will spend a lot of power.</p><figure name="60cf" id="60cf" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DI-Gx-zxo-nXKsYBl1XDJg.png" data-width="1616" data-height="992" src="https://cdn-images-1.medium.com/max/800/1*DI-Gx-zxo-nXKsYBl1XDJg.png"></figure><p name="9304" id="9304" class="graf graf--p graf-after--figure">None of the swaps will successfully grab the block until the mutex mu is unlocked and the value of it is reset to 0. So at some point, core #2 and core #3 will have a race condition and one of them could acquire the lock successfully.</p><figure name="f37f" id="f37f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FbRS73DwKhktWcA6s16bhg.png" data-width="1616" data-height="1072" src="https://cdn-images-1.medium.com/max/800/1*FbRS73DwKhktWcA6s16bhg.png"></figure><p name="7289" id="7289" class="graf graf--p graf-after--figure">As we have discussed, the atomic read-write operations repeated even while waiting for the lock is inefficient in terms of energy and it actually slows down the thread that is in the critical section.</p><p name="e0ad" id="e0ad" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(12) Test-and-Atomic-Operation Lock</strong></p><p name="0097" id="0097" class="graf graf--p graf-after--p">Because we have the inefficiency of repeated r/w operations with the mutex, we may want to have it improved. One way to make our lock more efficient is by an approach called <strong class="markup--strong markup--p-strong">test-and-atomic-operation</strong>. So before we have the <code class="markup--code markup--p-code">EXCH</code> ,</p><pre name="ad7c" id="ad7c" class="graf graf--pre graf-after--p">R1 = 1;<br>while (R1 == 1) {<br>    while (mu == 1) <br>        EXCH R1, mu;<br>}</pre><p name="293b" id="293b" class="graf graf--p graf-after--pre">In the beginning, we still have cache #1 load from the memory, and the corresponding block is in the M state.</p><figure name="fe13" id="fe13" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ifhfbsk11f5W7gzUFFMOPA.png" data-width="1616" data-height="626" src="https://cdn-images-1.medium.com/max/800/1*Ifhfbsk11f5W7gzUFFMOPA.png"></figure><p name="41f2" id="41f2" class="graf graf--p graf-after--figure">Because then core #2 and core #3 only read from the mu block in core #1 without writing, the block in core #1 will be in the O state. Both the blocks in cache #2 and cache #3 will be in the S state.</p><figure name="5bd8" id="5bd8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NqAF81sa0QBr7r8yPKSZQg.png" data-width="1616" data-height="718" src="https://cdn-images-1.medium.com/max/800/1*NqAF81sa0QBr7r8yPKSZQg.png"></figure><p name="42fa" id="42fa" class="graf graf--p graf-after--figure">Then in the following loops, we will have cache hits. So we can directly read from the current cache instead of doing the I/M swaps between two caches. And this results in a better performance.</p><figure name="a3e0" id="a3e0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dtpBDMIYbKZcQ4B28QVPcw.png" data-width="1616" data-height="1072" src="https://cdn-images-1.medium.com/max/800/1*dtpBDMIYbKZcQ4B28QVPcw.png"></figure><p name="bfaf" id="bfaf" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">2. Barrier Synchronization</strong></p><p name="475e" id="475e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of Barrier Synchronization</strong></p><p name="0af7" id="0af7" class="graf graf--p graf-after--p">Another type of synchronization is called <strong class="markup--strong markup--p-strong">barrier synchronization</strong>. This often occurs when there is a parallel section where several threads are doing something independent of each other. For example, suppose each thread is adding up the numbers in its own part of a shared array, and eventually, we want to know the total sum. What needs to happen is that we need to make sure that everybody is finishing their own adding up and that’s why we need a barrier.</p><p name="32b0" id="32b0" class="graf graf--p graf-after--p">So the <strong class="markup--strong markup--p-strong">barrier</strong> is pretty much a global wait that ensures that all threads have entered the barrier before any of them can proceed the past barrier. The barrier synchronization ensures that all threads that need to arrive before any can leave the barrier.</p><p name="33a8" id="33a8" class="graf graf--p graf-after--p">A typical barrier implementation has two variables,</p><ul class="postList"><li name="7168" id="7168" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">A counter</strong>: counts how many threads arrived</li><li name="d206" id="d206" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">A flag</strong>: gets set when the counter reaches N</li></ul><pre name="b065" id="b065" class="graf graf--pre graf-after--li">typedef struct barrier_t {<br>    int count;<br>    int flag;<br>} barrier_t;</pre><p name="7b89" id="7b89" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(2) Simple Barrier Implementation</strong></p><p name="df4f" id="df4f" class="graf graf--p graf-after--p">Let’s see how can we implement a barrier in a thread,</p><pre name="43a2" id="43a2" class="graf graf--pre graf-after--p">barrier_t *barrier = (barrier_t *) malloc(sizeof(barrier_t));</pre><pre name="52d8" id="52d8" class="graf graf--pre graf-after--pre">lock(mu_barrier);<br>if (barrier-&gt;count == 0) barrier-&gt;flag = 0;     // initialize flag<br>barrier-&gt;count ++;                              // count thread<br>unlock(mu_barrier);</pre><pre name="d37a" id="d37a" class="graf graf--pre graf-after--pre">if (barrier-&gt;count == NUM_THREADS) {<br>    barrier-&gt;count = 0;       // reinitialized the counter<br>    barrier-&gt;flag = 1;        // the flag implies we can continue<br>} else {<br>    spin(barrier-&gt;flag != 1);  // wait for threads until flag set<br>}</pre><p name="0cbe" id="0cbe" class="graf graf--p graf-after--pre">Because the <code class="markup--code markup--p-code">barrier</code> can be accessed by all the other threads, when we update the value of the counter, we have to lock the barrier for synchronization purposes.</p><p name="78d8" id="78d8" class="graf graf--p graf-after--p">If the counter achieves the number of the threads <code class="markup--code markup--p-code">NUM_THREADS</code> we are waiting for, we have to reinitialize th counter to 0 for future use. And the flag should be set to 1, which shows all the other threads that are waiting to continue execution. However, if we have a counter smaller than the number of the threads, we have to wait for the other threads, and the current threads need to spin when the flag is not set.</p><p name="bb52" id="bb52" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Simple Barrier Implementation: Exception Situation</strong></p><p name="b7ec" id="b7ec" class="graf graf--p graf-after--p">For the simple barrier we have discussed, there can be an exception. Let’s say we have two threads. Thread A will be executed first but it will spin and wait for the thread B. When thread B finishes, the flag will be set to 1 both of them can continue executing,</p><figure name="7c35" id="7c35" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*khCsno2Ajlq9GXbCMTbNow.png" data-width="1298" data-height="564" src="https://cdn-images-1.medium.com/max/800/1*khCsno2Ajlq9GXbCMTbNow.png"></figure><p name="a8eb" id="a8eb" class="graf graf--p graf-after--figure">However, when thread A is spinning, we may probably context switch to some other programs and it may take a while for us to come back. So when thread B sets the flag to 1, thread A may not continue executing immediately. Therefore, the flag from B shared to A will be delayed.</p><figure name="ee53" id="ee53" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pF_xVsW2uL2aD20YbXdsnw.png" data-width="1298" data-height="884" src="https://cdn-images-1.medium.com/max/800/1*pF_xVsW2uL2aD20YbXdsnw.png"></figure><p name="5e37" id="5e37" class="graf graf--p graf-after--figure">This will not be a problem until if in the delayed time, thread B keeps executing and reaches another new barrier. Then sadly, the flag will be cleared to 0 before thread A grab its value. When thread A switches back to the program, it will still get a flag of 0, and both of the threads are spinning now. Therefore, we have both of the threads waiting for each other but no one can continue, which is quite similar to a deadlock problem.</p><figure name="bfc5" id="bfc5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*oG0Ib3PIm2c5tnXaiDBQwg.png" data-width="1298" data-height="884" src="https://cdn-images-1.medium.com/max/800/1*oG0Ib3PIm2c5tnXaiDBQwg.png"></figure><p name="b371" id="b371" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Simple Barrier Exception Solution: Flippable Local Sense</strong></p><p name="57a3" id="57a3" class="graf graf--p graf-after--p">So the last thing we are going to discuss is how we can reuse the barrier properly and this is done by assigning the flag with the <strong class="markup--strong markup--p-strong">flippable local sense</strong>. The exception is caused because we have changed the value in the barrier before another thread reads from it. Although we can maintain new barriers to avoid this problem, it is simpler if we can reuse the barriers.</p><p name="ec0d" id="ec0d" class="graf graf--p graf-after--p">For reusing the barrier in the proper way, we can assign the flag with a local sense variable instead of a constant. The modified code should be,</p><pre name="3fb5" id="3fb5" class="graf graf--pre graf-after--p">barrier_t *barrier = (barrier_t *) malloc(sizeof(barrier_t));<br>int localsense = 1;</pre><pre name="16f9" id="16f9" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">localsense = !localsense;     <br></strong>lock(mu_barrier);<br>if (barrier-&gt;count == 0) barrier-&gt;flag = <strong class="markup--strong markup--pre-strong">localsense</strong>; <br>barrier-&gt;count ++;<br>unlock(mu_barrier);</pre><pre name="90b0" id="90b0" class="graf graf--pre graf-after--pre">if (barrier-&gt;count == NUM_THREADS) {<br>    barrier-&gt;count = 0; <br>    barrier-&gt;flag = <strong class="markup--strong markup--pre-strong">!localsense</strong>;<br>} else {<br>    spin(barrier-&gt;flag != <strong class="markup--strong markup--pre-strong">localsense</strong>);<br>}</pre><p name="d759" id="d759" class="graf graf--p graf-after--pre">Initially, the <code class="markup--code markup--p-code">localsense</code> is set to 0 in the first barrier, and this means that 0 means we have to wait for the other threads, while 1 means that we can continue executing. After passing the first barrier, the <code class="markup--code markup--p-code">localsense</code> will be flipped to 1, and this means that 1 means we have to wait for the other threads, while 0 means that we can continue executing.</p><p name="e3e8" id="e3e8" class="graf graf--p graf-after--p">Therefore, thread A can still get a flag of 1 from B even if B reuses the barrier,</p><figure name="d4f5" id="d4f5" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*DnYTacMlyTPqQ-vUY41bfA.png" data-width="1298" data-height="1046" src="https://cdn-images-1.medium.com/max/800/1*DnYTacMlyTPqQ-vUY41bfA.png"></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/5286132300f9"><time class="dt-published" datetime="2021-04-08T17:35:55.320Z">April 8, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/high-performance-computer-architecture-31-the-introduction-of-synchronization-atomic-exchange-5286132300f9" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>