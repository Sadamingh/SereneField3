<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Linear Algebra 9 | Trace, Eigenspace, Eigendecomposition, Similarity, and Diagonalizable Matrix</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Linear Algebra 9 | Trace, Eigenspace, Eigendecomposition, Similarity, and Diagonalizable Matrix</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Linear Algebra
</section>
<section data-field="body" class="e-content">
<section name="ad66" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="9e8a" id="9e8a" class="graf graf--h3 graf--leading graf--title">Linear Algebra 9 | <strong class="markup--strong markup--h3-strong">Trace, Eigenspace, Eigendecomposition, Similarity, and Diagonalizable Matrix</strong></h3><figure name="98f2" id="98f2" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*Yj4DEKztkbPyDrnciYX9dQ.jpeg" data-width="1955" data-height="1296" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*Yj4DEKztkbPyDrnciYX9dQ.jpeg"></figure><ol class="postList"><li name="ec39" id="ec39" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Recall: Eigenvalue and Eigenvector</strong></li></ol><p name="a67d" id="a67d" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) The Definition of the Eigenvector and the Eigenvalue</strong></p><p name="78d6" id="78d6" class="graf graf--p graf-after--p">Let matrix <strong class="markup--strong markup--p-strong">A</strong> be an <em class="markup--em markup--p-em">n</em> × <em class="markup--em markup--p-em">n </em>square matrix. Suppose we have a vector <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">x</em></strong> ≠ <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">0</em></strong>. If there’s a constant λ satisfies,</p><figure name="dd6e" id="dd6e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*PZ0K01DyXD7irbs4.png" data-width="1248" data-height="56" src="https://cdn-images-1.medium.com/max/800/0*PZ0K01DyXD7irbs4.png"></figure><p name="58a0" id="58a0" class="graf graf--p graf-after--figure">then <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">x</em></strong> is an eigenvector for <strong class="markup--strong markup--p-strong">A</strong> corresponding to the eigenvalue λ.</p><p name="b39c" id="b39c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Calculate the Eigenvector and the Eigenvalue</strong></p><p name="9216" id="9216" class="graf graf--p graf-after--p">If we want to calculate the eigenvalue of <strong class="markup--strong markup--p-strong">A</strong>, we can construct the matrix <strong class="markup--strong markup--p-strong">A</strong>-λ<strong class="markup--strong markup--p-strong">I</strong> and then assign its determinate to zero.</p><figure name="2211" id="2211" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*XTJwv877zQbiCWgf.png" data-width="1248" data-height="56" src="https://cdn-images-1.medium.com/max/800/0*XTJwv877zQbiCWgf.png"></figure><p name="8b5d" id="8b5d" class="graf graf--p graf-after--figure">Then, we can assign back the λ to <strong class="markup--strong markup--p-strong">A</strong>-λ<strong class="markup--strong markup--p-strong">I</strong> respectively, and then calculate the non-trivial solution of (<strong class="markup--strong markup--p-strong">A</strong>-λ<strong class="markup--strong markup--p-strong">I</strong>)<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">x</em></strong> = <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">0</em></strong> in order to solve the eigenvector <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">x</em></strong>.</p><p name="8a50" id="8a50" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) The Definition of Algebraic Multiplicity</strong></p><p name="67eb" id="67eb" class="graf graf--p graf-after--p">The number of times of λ appears as a root of,</p><figure name="983e" id="983e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*XTJwv877zQbiCWgf.png" data-width="1248" data-height="56" src="https://cdn-images-1.medium.com/max/800/0*XTJwv877zQbiCWgf.png"></figure><p name="c41c" id="c41c" class="graf graf--p graf-after--figure">is called the algebraic multiplicity.</p><p name="3b11" id="3b11" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) The Definition of Geometric Multiplicity</strong></p><p name="4d0e" id="4d0e" class="graf graf--p graf-after--p">The dimension of the subspace N(<strong class="markup--strong markup--p-strong">A</strong>-λ<strong class="markup--strong markup--p-strong">I</strong>) is called geometric multiply. The algebraic multiplicity and the geometric multiplicity can be equal sometimes but they are not always equal to each other.</p><p name="c0a6" id="c0a6" class="graf graf--p graf-after--p">Geometric multiplicity is also known as the dimension of the eigenspace of λ.</p><p name="f601" id="f601" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) The Definition of Trace</strong></p><p name="2e96" id="2e96" class="graf graf--p graf-after--p">The trace of a matrix is the summation of the main diagonal entries, which is,</p><figure name="298b" id="298b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YwBa82hgnfvzyfj842X63w.png" data-width="1374" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*YwBa82hgnfvzyfj842X63w.png"></figure><p name="db02" id="db02" class="graf graf--p graf-after--figure">Why this is an important conclusion is because the trace of A equals the summation of all the eigenvalues of A. This is also to say that,</p><figure name="651d" id="651d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gdCiFPWdTsJUPvYBGul47A.png" data-width="1374" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*gdCiFPWdTsJUPvYBGul47A.png"></figure><p name="21ce" id="21ce" class="graf graf--p graf-after--figure">Proof:</p><p name="1b61" id="1b61" class="graf graf--p graf-after--p">Suppose we have matrix A as</p><figure name="e75a" id="e75a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0xOzSDw58lVAhOuvLxJp9Q.png" data-width="1374" data-height="186" src="https://cdn-images-1.medium.com/max/800/1*0xOzSDw58lVAhOuvLxJp9Q.png"></figure><p name="a9ae" id="a9ae" class="graf graf--p graf-after--figure">then the determinate of A is to calculate,</p><figure name="24c1" id="24c1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4Javn6zg3Omtx5HASJh7Qg.png" data-width="1374" data-height="186" src="https://cdn-images-1.medium.com/max/800/1*4Javn6zg3Omtx5HASJh7Qg.png"></figure><p name="daa1" id="daa1" class="graf graf--p graf-after--figure">by computational formula,</p><figure name="18f6" id="18f6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*augUgv35PJ57BGxRn3L6VA.png" data-width="1374" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*augUgv35PJ57BGxRn3L6VA.png"></figure><p name="eac6" id="eac6" class="graf graf--p graf-after--figure">Alternatively, we suppose λ is an eigenvalue of A, then it must satisfy,</p><figure name="d23a" id="d23a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zB1lFyvnqM0oSmSK5VDqYA.png" data-width="1374" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*zB1lFyvnqM0oSmSK5VDqYA.png"></figure><p name="c6ee" id="c6ee" class="graf graf--p graf-after--figure">then, we can have,</p><figure name="549c" id="549c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AqZr-pDexlfSA68tu3Ab9Q.png" data-width="1374" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*AqZr-pDexlfSA68tu3Ab9Q.png"></figure><p name="8a89" id="8a89" class="graf graf--p graf-after--figure">Because</p><figure name="520e" id="520e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WI8rrG8IClaU1G1pQJlVpA.png" data-width="1374" data-height="192" src="https://cdn-images-1.medium.com/max/800/1*WI8rrG8IClaU1G1pQJlVpA.png"></figure><p name="b1dc" id="b1dc" class="graf graf--p graf-after--figure">thus,</p><figure name="06a3" id="06a3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*H61iITrIxGoU66pijwInzQ.png" data-width="1374" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*H61iITrIxGoU66pijwInzQ.png"></figure><p name="b39b" id="b39b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">2. Recall: Orthonormal Transformation</strong></p><p name="d535" id="d535" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Orthonormal Transformation</strong></p><p name="19ea" id="19ea" class="graf graf--p graf-after--p">If matrix <strong class="markup--strong markup--p-strong">Q</strong> has orthonormal columns,</p><figure name="9671" id="9671" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*Nv7buig5CObnhV8S.png" data-width="1450" data-height="58" src="https://cdn-images-1.medium.com/max/800/0*Nv7buig5CObnhV8S.png"></figure><p name="abaf" id="abaf" class="graf graf--p graf-after--figure">So we can see that the orthonormal matrix <strong class="markup--strong markup--p-strong">Q</strong> on <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">x</em></strong> preserves its length.</p><p name="2256" id="2256" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Orthogonal Transformation</strong></p><p name="31c4" id="31c4" class="graf graf--p graf-after--p">If matrix <strong class="markup--strong markup--p-strong">U</strong> has orthogonal columns, and suppose we have,</p><figure name="f847" id="f847" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*U19aJMhC5BgkMrrwdxyhOw.png" data-width="1374" data-height="188" src="https://cdn-images-1.medium.com/max/800/1*U19aJMhC5BgkMrrwdxyhOw.png"></figure><p name="f9ec" id="f9ec" class="graf graf--p graf-after--figure">then,</p><figure name="6fb1" id="6fb1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*URFlOkDuVWgKeMHcyZAUQA.png" data-width="1562" data-height="186" src="https://cdn-images-1.medium.com/max/800/1*URFlOkDuVWgKeMHcyZAUQA.png"></figure><p name="c4a0" id="c4a0" class="graf graf--p graf-after--figure">Thus the orthogonal matrix <strong class="markup--strong markup--p-strong">U</strong> on <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">x</em></strong> will not preserve its length. So it is obviously not a good idea to use orthogonal vectors for transformation.</p><p name="3005" id="3005" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Eigenspace and Eigendecomposition</strong></p><p name="45a8" id="45a8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of the Eigenspace</strong></p><p name="6a1c" id="6a1c" class="graf graf--p graf-after--p">The eigenspace is a subspace whose basis corresponding to the span of the set of all eigenvectors of <strong class="markup--strong markup--p-strong">A</strong>. This is to say, suppose we have the set of all eigenvectors of <strong class="markup--strong markup--p-strong">A </strong>as {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>n}, then if,</p><figure name="ce06" id="ce06" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*XN8-Wb_jL52hzoNVV_b3Ww.png" data-width="1248" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*XN8-Wb_jL52hzoNVV_b3Ww.png"></figure><p name="4a60" id="4a60" class="graf graf--p graf-after--figure">this is known as the eigenspace of matrix <strong class="markup--strong markup--p-strong">A</strong>.</p><p name="7c5c" id="7c5c" class="graf graf--p graf-after--p">Proof of linear independence:</p><p name="afe0" id="afe0" class="graf graf--p graf-after--p">Generally, this is to prove that eigenvectors corresponding to distinct eigenvalues are linearly independent.</p><p name="2da5" id="2da5" class="graf graf--p graf-after--p">Suppose we have a set of all eigenvectors of <strong class="markup--strong markup--p-strong">A </strong>as {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>n}, this is to show that we have only a trivial solution (<em class="markup--em markup--p-em">c</em>1 = <em class="markup--em markup--p-em">c</em>2 = … = 0) if we want to have,</p><figure name="965d" id="965d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4DLST7qQ8xO_O185nT76BA.png" data-width="1296" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*4DLST7qQ8xO_O185nT76BA.png"></figure><p name="9776" id="9776" class="graf graf--p graf-after--figure">we assume that <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>n is in the linear combination of all the other eigenvectors, and all the other eigenvectors are linearly independent, then,</p><figure name="ef74" id="ef74" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*LkJUmXCZDfusEmykQB8ZRg.png" data-width="1296" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*LkJUmXCZDfusEmykQB8ZRg.png"></figure><p name="a97e" id="a97e" class="graf graf--p graf-after--figure">define that,</p><figure name="17b7" id="17b7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ug1w-hziuiFrgwh2-3RRqg.png" data-width="1296" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*ug1w-hziuiFrgwh2-3RRqg.png"></figure><p name="f72b" id="f72b" class="graf graf--p graf-after--figure">then,</p><figure name="0d7e" id="0d7e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jFbGYjdlYBikCFIEBnqPlA.png" data-width="1296" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*jFbGYjdlYBikCFIEBnqPlA.png"></figure><p name="c2dd" id="c2dd" class="graf graf--p graf-after--figure">multiply A on the left of both sides of (*),</p><figure name="5b9c" id="5b9c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*QNdUX3gttZbUo5ZH3QwQLQ.png" data-width="1296" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*QNdUX3gttZbUo5ZH3QwQLQ.png"></figure><p name="0788" id="0788" class="graf graf--p graf-after--figure">then,</p><figure name="bb9a" id="bb9a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WJVaTh-kZ2VmhPw0YTqwzg.png" data-width="1296" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*WJVaTh-kZ2VmhPw0YTqwzg.png"></figure><p name="a184" id="a184" class="graf graf--p graf-after--figure">also, multiply λn on the left of both sides of (*),</p><figure name="e8d6" id="e8d6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nhnypnY3JlaV95ZtDHOsMw.png" data-width="1296" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*nhnypnY3JlaV95ZtDHOsMw.png"></figure><p name="b98e" id="b98e" class="graf graf--p graf-after--figure">let (***) minus (**), then,</p><figure name="dea9" id="dea9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*z7qqd9c_YA3uYIWbdwnwfQ.png" data-width="1296" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*z7qqd9c_YA3uYIWbdwnwfQ.png"></figure><p name="3adb" id="3adb" class="graf graf--p graf-after--figure">because <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>n is in the linear combination of all the other eigenvectors,</p><figure name="e7bf" id="e7bf" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mudcINpUCdh3F7xBKFmBfA.png" data-width="1296" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*mudcINpUCdh3F7xBKFmBfA.png"></figure><p name="6822" id="6822" class="graf graf--p graf-after--figure">also, we have eigenvectors corresponding to distinct eigenvalues are linearly independent, so that,</p><figure name="0bb4" id="0bb4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*uApz5TDIB5gg-aw2ATX3hg.png" data-width="1296" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*uApz5TDIB5gg-aw2ATX3hg.png"></figure><p name="c6c0" id="c6c0" class="graf graf--p graf-after--figure">therefore, for the set {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>n-1}, these vectors are not linearly independent. This caused a contradiction to our assumption. Therefore, all the eigenvectors corresponding to distinct eigenvalues are linearly independent.</p><p name="85c3" id="85c3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Definition of the Eigenmatrix</strong></p><p name="dd5b" id="dd5b" class="graf graf--p graf-after--p">Suppose we have an <em class="markup--em markup--p-em">n </em>× <em class="markup--em markup--p-em">n </em>diagonal matrix whose main diagonal is composed of eigenvalues of <strong class="markup--strong markup--p-strong">A</strong>, then this matrix is called the eigenmatrix of <strong class="markup--strong markup--p-strong">A</strong>, and it is noted as Λ.</p><figure name="397e" id="397e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*eM_gjQoQBMuYIYhFdU7uSw.png" data-width="1248" data-height="186" src="https://cdn-images-1.medium.com/max/800/1*eM_gjQoQBMuYIYhFdU7uSw.png"></figure><p name="2728" id="2728" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) The Definition of the Eigendecomposition</strong></p><p name="4079" id="4079" class="graf graf--p graf-after--p">Suppose we have a full-ranked <em class="markup--em markup--p-em">n </em>× <em class="markup--em markup--p-em">n </em>matrix <strong class="markup--strong markup--p-strong">S</strong> whose column space is the eigenspace of <strong class="markup--strong markup--p-strong">A</strong>, which is to say that,</p><figure name="46bd" id="46bd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*98JFc1cxgV55t4fV8yYnSw.png" data-width="1248" data-height="164" src="https://cdn-images-1.medium.com/max/800/1*98JFc1cxgV55t4fV8yYnSw.png"></figure><p name="68bc" id="68bc" class="graf graf--p graf-after--figure">Then we can have the conclusion that,</p><figure name="f9b0" id="f9b0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xe1PNcSUq6XHiSMCgA0P0g.png" data-width="1248" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*xe1PNcSUq6XHiSMCgA0P0g.png"></figure><p name="4e3f" id="4e3f" class="graf graf--p graf-after--figure">This is known as the eigendecomposition of <strong class="markup--strong markup--p-strong">A</strong>.</p><p name="4e34" id="4e34" class="graf graf--p graf-after--p">Proof:</p><p name="c2d0" id="c2d0" class="graf graf--p graf-after--p">Because {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>n} is a basis of ℝⁿ and ∀ <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>i ∈ {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>n}, by definition of the eigenvalue and eigenvector,</p><figure name="98a9" id="98a9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*LIvuW3zeNu3W0niEEBOa6Q.png" data-width="1248" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*LIvuW3zeNu3W0niEEBOa6Q.png"></figure><p name="7a26" id="7a26" class="graf graf--p graf-after--figure">then,</p><figure name="45ef" id="45ef" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5ZOb3xz-UzAeLMDrfocNLA.png" data-width="1248" data-height="160" src="https://cdn-images-1.medium.com/max/800/1*5ZOb3xz-UzAeLMDrfocNLA.png"></figure><p name="dbf3" id="dbf3" class="graf graf--p graf-after--figure">then,</p><figure name="3698" id="3698" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kS_eRbb28PhcThHuKRomDQ.png" data-width="1248" data-height="160" src="https://cdn-images-1.medium.com/max/800/1*kS_eRbb28PhcThHuKRomDQ.png"></figure><p name="127c" id="127c" class="graf graf--p graf-after--figure">then,</p><figure name="f599" id="f599" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vu2_ENQtcIugX4SPZjSlXA.png" data-width="1248" data-height="180" src="https://cdn-images-1.medium.com/max/800/1*vu2_ENQtcIugX4SPZjSlXA.png"></figure><p name="1ca1" id="1ca1" class="graf graf--p graf-after--figure">then,</p><figure name="4891" id="4891" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*aC8WxZxZOvf5CuJ-UcQMDw.png" data-width="1254" data-height="50" src="https://cdn-images-1.medium.com/max/800/1*aC8WxZxZOvf5CuJ-UcQMDw.png"></figure><p name="3f5c" id="3f5c" class="graf graf--p graf-after--figure">then,</p><figure name="e871" id="e871" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gN7IOU5GwYwJT0Bmtj6sVw.png" data-width="1248" data-height="50" src="https://cdn-images-1.medium.com/max/800/1*gN7IOU5GwYwJT0Bmtj6sVw.png"></figure><p name="3f12" id="3f12" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Eigendecomposition in an Orthonormal Way for the Symmetric Matrix (The Spectral Theorem)</strong></p><p name="3c4a" id="3c4a" class="graf graf--p graf-after--p">In the definition of eigendecomposition above, we had the matrix <strong class="markup--strong markup--p-strong">S</strong> as a matrix whose column space is the eigenspace of <strong class="markup--strong markup--p-strong">A</strong>. For the symmetric matrix, we can always find an orthogonal eigenvector matrix (we are going to talk about this later). What if we normalize this matrix S and make it an orthonormal matrix? Suppose we have the set of all eigenvectors of <strong class="markup--strong markup--p-strong">A </strong>as {<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>1, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>2, …, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">v</em></strong>n}, then if we define,</p><figure name="f63a" id="f63a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Am7UG7d9w6GLOk2Jew85Vg.png" data-width="1248" data-height="372" src="https://cdn-images-1.medium.com/max/800/1*Am7UG7d9w6GLOk2Jew85Vg.png"></figure><p name="a38d" id="a38d" class="graf graf--p graf-after--figure">then we can construct matrix Q as</p><figure name="392f" id="392f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*adD5Axg5oRjHC0bE0x12ZA.png" data-width="1248" data-height="166" src="https://cdn-images-1.medium.com/max/800/1*adD5Axg5oRjHC0bE0x12ZA.png"></figure><p name="4522" id="4522" class="graf graf--p graf-after--figure">thus, we have,</p><figure name="0c49" id="0c49" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ex_cYaqUgUR3MFm-OU59kg.png" data-width="1248" data-height="52" src="https://cdn-images-1.medium.com/max/800/1*ex_cYaqUgUR3MFm-OU59kg.png"></figure><p name="74a4" id="74a4" class="graf graf--p graf-after--figure">We are going to analyze this orthonormality for the symmetric matrix later.</p><p name="a2e1" id="a2e1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Geometrical Meaning of Eigendecomposition</strong></p><p name="fde2" id="fde2" class="graf graf--p graf-after--p">Suppose we conduct transformation <strong class="markup--strong markup--p-strong">A</strong>: ℝⁿ → ℝⁿ on the eigenvector <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">x</em></strong>, then we can have <strong class="markup--strong markup--p-strong">A<em class="markup--em markup--p-em">x</em> </strong>equivalent to,</p><figure name="7f90" id="7f90" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ObNz3WylcgGeS-D1VAUgpw.png" data-width="1248" data-height="52" src="https://cdn-images-1.medium.com/max/800/1*ObNz3WylcgGeS-D1VAUgpw.png"></figure><p name="0354" id="0354" class="graf graf--p graf-after--figure">By the geometrical meaning of eigenvector matrix <strong class="markup--strong markup--p-strong">Q </strong>(<strong class="markup--strong markup--p-strong">Not orthonormal in this case</strong>), we know that both <strong class="markup--strong markup--p-strong">Q</strong> and <strong class="markup--strong markup--p-strong">Q</strong> inverse is to rotate the vector <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">x</em></strong> for some degrees with its length remains, while the geometrical meaning of eigenmatrix <strong class="markup--strong markup--p-strong">Λ</strong> is only to scale the vector with its direction remains as the same. This is to say that,</p><figure name="d701" id="d701" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xImSkHCX1nORhZJDeFXDUw.png" data-width="1214" data-height="572" src="https://cdn-images-1.medium.com/max/800/1*xImSkHCX1nORhZJDeFXDUw.png"></figure><p name="ba90" id="ba90" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(6) The Definition of Similar Matrix</strong></p><p name="df59" id="df59" class="graf graf--p graf-after--p">If two matrix <strong class="markup--strong markup--p-strong">A</strong> and<strong class="markup--strong markup--p-strong"> B</strong> are similar, then there’s an invertible matrix <strong class="markup--strong markup--p-strong">S</strong> satisfies,</p><figure name="a2e2" id="a2e2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*K_jy8lP2R8CheuJLW1i_vQ.png" data-width="1296" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*K_jy8lP2R8CheuJLW1i_vQ.png"></figure><p name="a1df" id="a1df" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(7) The Definition of Diagonalizable Matrix</strong></p><p name="945b" id="945b" class="graf graf--p graf-after--p">The matrix <strong class="markup--strong markup--p-strong">A</strong> is said to be diagonalizable if <strong class="markup--strong markup--p-strong">A</strong> is similar to a diagonal matrix. This is also to say that there is an invertible matrix <strong class="markup--strong markup--p-strong">S</strong> so that,</p><figure name="b4a0" id="b4a0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xmBskLcj4dYMvIqCp33UBg.png" data-width="1296" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*xmBskLcj4dYMvIqCp33UBg.png"></figure><p name="d602" id="d602" class="graf graf--p graf-after--figure">where <strong class="markup--strong markup--p-strong">D</strong> is a diagonal matrix.</p><p name="b43e" id="b43e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) The Definition of the Defective Matrix</strong></p><p name="7214" id="7214" class="graf graf--p graf-after--p">Suppose we have a matrix <strong class="markup--strong markup--p-strong">A</strong> whose total algebraic multiplicity is greater than the geometric multiplicity, then we can&#39;t have enough eigenvector for the eigendecomposition. Therefore, the matrix <strong class="markup--strong markup--p-strong">A </strong>in this case is not diagonalizable, and matrix <strong class="markup--strong markup--p-strong">A</strong> is called a defective matrix. For example, suppose we have matrix <strong class="markup--strong markup--p-strong">A </strong>as</p><figure name="9024" id="9024" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*B5IENVxCwIGow4H-lYh8Yg.png" data-width="1296" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*B5IENVxCwIGow4H-lYh8Yg.png"></figure><p name="6453" id="6453" class="graf graf--p graf-after--figure">then the eigenvalue of <strong class="markup--strong markup--p-strong">A</strong> is,</p><figure name="ba35" id="ba35" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xFVNEUs307MHyYvcRfaxUw.png" data-width="1296" data-height="48" src="https://cdn-images-1.medium.com/max/800/1*xFVNEUs307MHyYvcRfaxUw.png"></figure><p name="cb0b" id="cb0b" class="graf graf--p graf-after--figure">then,</p><figure name="cafc" id="cafc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*OiDgPe0sNnpWbFzVUVSqpQ.png" data-width="1428" data-height="86" src="https://cdn-images-1.medium.com/max/800/1*OiDgPe0sNnpWbFzVUVSqpQ.png"></figure><p name="bfa1" id="bfa1" class="graf graf--p graf-after--figure">Solving (<strong class="markup--strong markup--p-strong">A</strong>-<strong class="markup--strong markup--p-strong">I</strong>)<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">x</em></strong> = <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">0</em></strong>, then the eigenvector is,</p><figure name="f61d" id="f61d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*G8iLB3DH9Z49bkw8G82qTg.png" data-width="1248" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*G8iLB3DH9Z49bkw8G82qTg.png"></figure><p name="7cbf" id="7cbf" class="graf graf--p graf-after--figure">therefore, we can not construct a diagonal matrix in this case.</p><p name="3c0c" id="3c0c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) Ways to Know Diagonalizablity</strong></p><p name="5632" id="5632" class="graf graf--p graf-after--p">There are three ways to know whether a matrix is diagonalizable,</p><ul class="postList"><li name="6f0c" id="6f0c" class="graf graf--li graf-after--p">Eigenvectors corresponding to distinct eigenvalues are linearly independent. (If matrix <strong class="markup--strong markup--li-strong">A </strong>has <em class="markup--em markup--li-em">n</em> distinct eigenvalues, then it’s diagonalizable.)</li><li name="d39e" id="d39e" class="graf graf--li graf-after--li">If <strong class="markup--strong markup--li-strong">A</strong> and <strong class="markup--strong markup--li-strong">B</strong> are similar, then they have the same eigenvalues, and they have the same diagonal matrix.</li><li name="01a2" id="01a2" class="graf graf--li graf-after--li">The eigenvalues of <strong class="markup--strong markup--li-strong">A</strong> equal the eigenvalues of <strong class="markup--strong markup--li-strong">A</strong> transpose.</li></ul><p name="21e5" id="21e5" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(10) Complex Eigenvalues</strong></p><p name="f096" id="f096" class="graf graf--p graf-after--p">Suppose we have matrix <strong class="markup--strong markup--p-strong">A</strong> as</p><figure name="1eeb" id="1eeb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fOqGP-JnaFEoJnAPMS7QtQ.png" data-width="1248" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*fOqGP-JnaFEoJnAPMS7QtQ.png"></figure><p name="a40d" id="a40d" class="graf graf--p graf-after--figure">then, we can solve the eigenvalues for,</p><figure name="5660" id="5660" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*N76smVJ1DZrI5Q72LshdbA.png" data-width="1248" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*N76smVJ1DZrI5Q72LshdbA.png"></figure><p name="fb55" id="fb55" class="graf graf--p graf-after--figure graf--trailing">we are not going to talk about it into details here, and just reserve if for later discussions.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/f2627ff4d00b"><time class="dt-published" datetime="2020-09-18T10:39:17.188Z">September 18, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/linear-algebra-9-trace-eigenspace-eigendecomposition-similarity-and-diagonalizable-matrix-f2627ff4d00b" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>