<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Data Pipeline 1 | Apache Airflow Overview</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Data Pipeline 1 | Apache Airflow Overview</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Data Pipeline
</section>
<section data-field="body" class="e-content">
<section name="10ce" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="869e" id="869e" class="graf graf--h3 graf--leading graf--title">Data Pipeline 1 | Apache Airflow Overview</h3><figure name="88b2" id="88b2" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*D7GccUttMtNxf5Rvi9kzQw.png" data-width="1352" data-height="622" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*D7GccUttMtNxf5Rvi9kzQw.png"></figure><ol class="postList"><li name="e810" id="e810" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Apache Airflow</strong></li></ol><p name="f5ca" id="f5ca" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) What is Apache Airflow?</strong></p><p name="9632" id="9632" class="graf graf--p graf-after--p">Apache Airflow is an open-source workflow management platform. It started at Airbnb in October 2014 as a solution to manage the company’s increasingly complex workflows.</p><p name="aea6" id="aea6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Definition of Workflow</strong></p><p name="a9a7" id="a9a7" class="graf graf--p graf-after--p">The workflow is defined as a sequence of tasks that start on scheduled data or are triggered by an event. They are currently broadly used to handle the big data processing pipelines.</p><p name="a11d" id="a11d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Traditional ETL Workflow</strong></p><p name="fa00" id="fa00" class="graf graf--p graf-after--p">Now, let’s think about a traditional ETL workflow. In a naive approach, the APP generates data to the database and then the engineers write a script for pulling data from the database and then send it to HDFS. Then Hadoop or Spark are used for distributed computing on the data retrieved from the database. The workflows are scheduled by <code class="markup--code markup--p-code">crontab</code> in Unix-like systems.</p><p name="957f" id="957f" class="graf graf--p graf-after--p">However, this approach has the following defects,</p><ul class="postList"><li name="4f35" id="4f35" class="graf graf--li graf-after--p">Difficult to manage retries after failures</li><li name="8c8b" id="8c8b" class="graf graf--li graf-after--li">Hard to monitor the time and status of the process</li><li name="1084" id="1084" class="graf graf--li graf-after--li">Difficult to manage task dependencies or data dependencies</li><li name="3b87" id="3b87" class="graf graf--li graf-after--li">etc.</li></ul><p name="5191" id="5191" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(5) Benefits of Using Apache Airflow</strong></p><ul class="postList"><li name="d8ac" id="d8ac" class="graf graf--li graf-after--p">DAGs (Directed Acyclic Graph) is written in Python language and it is easy to learn. The DAGs are a kind of workflow process that we can define, and we will take about it later.</li><li name="f382" id="f382" class="graf graf--li graf-after--li">Apache Airflow is supported by a huge open-source community</li><li name="751e" id="751e" class="graf graf--li graf-after--li">Apache Airflow is capable of creating complex workflows</li><li name="ba4e" id="ba4e" class="graf graf--li graf-after--li">Apache Airflow has a richer API with a better UI</li><li name="01a7" id="01a7" class="graf graf--li graf-after--li">Apache Airflow uses Jinja as a Python template engine for rendering</li></ul><p name="91a5" id="91a5" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Airflow Installation</strong></p><p name="d9d1" id="d9d1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Airflow Installation</strong></p><p name="0b05" id="0b05" class="graf graf--p graf-after--p">We should use <code class="markup--code markup--p-code">pip</code> to install the Apache Airflow,</p><pre name="d280" id="d280" class="graf graf--pre graf-after--p">$ pip install apache-airflow</pre><p name="1c07" id="1c07" class="graf graf--p graf-after--pre">To make it support AWS S3 and PostgreSQL, we should also install some extra functions by,</p><pre name="3f21" id="3f21" class="graf graf--pre graf-after--p">$ pip install apache-airflow[postgres,s3]</pre><p name="da99" id="da99" class="graf graf--p graf-after--pre">Then if we would like to see the version of the current airflow we have, we can run,</p><pre name="2bbf" id="2bbf" class="graf graf--pre graf-after--p">$ airflow version</pre><p name="690c" id="690c" class="graf graf--p graf-after--pre">We can simply start all the components by <code class="markup--code markup--p-code">standalone</code> option,</p><pre name="5d3d" id="5d3d" class="graf graf--pre graf-after--p">$ airflow standalone</pre><p name="9bc3" id="9bc3" class="graf graf--p graf-after--pre">This all-in-one <code class="markup--code markup--p-code">standalone</code> command actually runs the individual parts of Airflow manually. The Apache Airflow requires a database running before we run the tasks, and here we will just use the default SQLite database if we run <code class="markup--code markup--p-code">db init</code> by default.</p><pre name="68f2" id="68f2" class="graf graf--pre graf-after--p">$ airflow db init<br>$ airflow users create \<br>      --username admin \<br>      --firstname FIRST_NAME \<br>      --lastname LAST_NAME \<br>      --role Admin \<br>      --email admin@example.org<br>$ airflow webserver --port 8080<br>$ airflow scheduler</pre><p name="564d" id="564d" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(2) Setting Up a DAG Directory</strong></p><p name="1d3c" id="1d3c" class="graf graf--p graf-after--p">After we run,</p><pre name="bdad" id="bdad" class="graf graf--pre graf-after--p">$ airflow standalone</pre><p name="08e9" id="08e9" class="graf graf--p graf-after--pre">We are expected to get the following information,</p><pre name="64ed" id="64ed" class="graf graf--pre graf-after--p">...<br>standalone | Airflow is ready<br>standalone | Login with <strong class="markup--strong markup--pre-strong">username: admin</strong>  <strong class="markup--strong markup--pre-strong">password: ...</strong><br>standalone | Airflow Standalone is for development purposes only. Do not use this in production!<br>...</pre><p name="e4f0" id="e4f0" class="graf graf--p graf-after--pre">Then we can go to the 8080 port of localhost,</p><pre name="c505" id="c505" class="graf graf--pre graf-after--p">GOTO: localhost:8080</pre><p name="cdf4" id="cdf4" class="graf graf--p graf-after--pre">And we should be able to log in with the <code class="markup--code markup--p-code">username</code> and <code class="markup--code markup--p-code">password</code> generated above. We will by default have some example DAGs.</p><figure name="f6e3" id="f6e3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ml6Gio19mJwc8lBJUL2faw.png" data-width="2780" data-height="952" src="https://cdn-images-1.medium.com/max/800/1*Ml6Gio19mJwc8lBJUL2faw.png"></figure><p name="f2e7" id="f2e7" class="graf graf--p graf-after--figure">We can get this list from the command line by,</p><pre name="d1f4" id="d1f4" class="graf graf--pre graf-after--p">$ airflow dags list</pre><p name="d679" id="d679" class="graf graf--p graf-after--pre">Now, suppose we want to create our new DAGs and put them in another directory <code class="markup--code markup--p-code">DAGs</code>. Suppose we would like to add a new folder in the home directory for the current user.</p><pre name="12ff" id="12ff" class="graf graf--pre graf-after--p">$ cd ~<br>$ mkdir DAGs</pre><p name="bea9" id="bea9" class="graf graf--p graf-after--pre">And then, we will make it a path that will be used by the Airflow. The correct path to your DAG folder is set in <code class="markup--code markup--p-code">airflow.cfg</code> file is located at,</p><pre name="983c" id="983c" class="graf graf--pre graf-after--p">$ cd ~/airflow/</pre><p name="0572" id="0572" class="graf graf--p graf-after--pre">And we can choose to open it then by,</p><pre name="d8cd" id="d8cd" class="graf graf--pre graf-after--p">$ vi airflow.cfg</pre><p name="af80" id="af80" class="graf graf--p graf-after--pre">The variables <code class="markup--code markup--p-code">dags_folder</code> actually holds the place that we should use for our new directory. According to the document, the folder where your airflow pipelines live, most likely a subfolder in a code repository. Therefore, we should change the value of it by,</p><pre name="5e50" id="5e50" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">dags_folder = ~/</code>DAGs</pre><p name="f5a9" id="f5a9" class="graf graf--p graf-after--pre">Then we should be able to create our own DAGs in this directory <code class="markup--code markup--p-code">~/DAGs</code> .</p><p name="0b97" id="0b97" class="graf graf--p graf-after--p">Also, we can stop the service at any time by pressing <code class="markup--code markup--p-code">^c</code>.</p><p name="bde3" id="bde3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Create a Hello World DAG</strong></p><p name="67ee" id="67ee" class="graf graf--p graf-after--p">Now, let’s use a completed baby Hello World DAG for our testing. You can download the following script and then put it in the directory we have assigned to <code class="markup--code markup--p-code">dags_folder</code> . Currently, we don’t dive deeply into this script but we can give it a general introduction.</p><figure name="d5aa" id="d5aa" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/e300ab81ba40c65f02edf10abee9fcef.js"></script></figure><p name="1e9b" id="1e9b" class="graf graf--p graf-after--figure">In this script, we have three tasks <code class="markup--code markup--p-code">t1</code>, <code class="markup--code markup--p-code">t2</code>, and <code class="markup--code markup--p-code">t3</code> and they are all bash commands,</p><ul class="postList"><li name="8a41" id="8a41" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">t1</code>: create a new file in <code class="markup--code markup--li-code">/tmp</code> named <code class="markup--code markup--li-code">test.txt</code></li><li name="3d54" id="3d54" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">t2</code>: write <code class="markup--code markup--li-code">Testing.</code> to this file <code class="markup--code markup--li-code">test.txt</code></li><li name="d1cc" id="d1cc" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">t3</code>: read from file <code class="markup--code markup--li-code">test.txt</code> and print it to standard output</li></ul><p name="4ebb" id="4ebb" class="graf graf--p graf-after--li">They are organized in the running order of,</p><pre name="8306" id="8306" class="graf graf--pre graf-after--p">t1 &gt;&gt; t2 &gt;&gt; t3</pre><p name="c7ff" id="c7ff" class="graf graf--p graf-after--pre">Then we should start the Airflow by.</p><pre name="6c22" id="6c22" class="graf graf--pre graf-after--p">$ airflow standalone</pre><p name="9f04" id="9f04" class="graf graf--p graf-after--pre">Then go to the 8080 port and we should be able to find the DAG with the name <code class="markup--code markup--p-code">example_testing</code> , and it is going to be the DAG we are going to run.</p><figure name="dabe" id="dabe" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*l4-9hGiXXGuM9K2O1XMxHQ.png" data-width="2780" data-height="618" src="https://cdn-images-1.medium.com/max/800/1*l4-9hGiXXGuM9K2O1XMxHQ.png"><figcaption class="imageCaption">Find example_testing DAG</figcaption></figure><p name="0eb6" id="0eb6" class="graf graf--p graf-after--figure">We can click on the name of the DAG and then select <code class="markup--code markup--p-code">Graph</code> tag in the next view. From here, we can check out the current directed acyclic graph. Also, we can check out the code by selecting the <code class="markup--code markup--p-code">Code</code> tag.</p><figure name="efe6" id="efe6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zapiCtph5hxGpp-vQcZXpw.png" data-width="2786" data-height="654" src="https://cdn-images-1.medium.com/max/800/1*zapiCtph5hxGpp-vQcZXpw.png"></figure><p name="4a7b" id="4a7b" class="graf graf--p graf-after--figure">To start a DAG, we have to turn on the switch of <code class="markup--code markup--p-code">Pause/Unpause DAG</code> behind the name of the DAG. Then after a second, we can find out that all the tasks are successfully executed.</p><figure name="3cc3" id="3cc3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*XJjmNM1WkPW1HkQb23s3aA.png" data-width="2786" data-height="736" src="https://cdn-images-1.medium.com/max/800/1*XJjmNM1WkPW1HkQb23s3aA.png"></figure><p name="26d5" id="26d5" class="graf graf--p graf-after--figure">We can check out the logs for each task. For example, we can check out the log for task <code class="markup--code markup--p-code">t3</code> and see if it prints <code class="markup--code markup--p-code">Testing.</code> to stdout. To do so, we should first select the task <code class="markup--code markup--p-code">read_from_file</code> (i.e. <code class="markup--code markup--p-code">t3</code>), and then in the following view, select <code class="markup--code markup--p-code">Log</code> .</p><figure name="fc53" id="fc53" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5h3kCaxMHV2DETbIXL8m8g.png" data-width="2776" data-height="1166" src="https://cdn-images-1.medium.com/max/800/1*5h3kCaxMHV2DETbIXL8m8g.png"></figure><p name="bfba" id="bfba" class="graf graf--p graf-after--figure">From the logs, we are able to find this output information at the bottom,</p><pre name="a9e6" id="a9e6" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code u-paddingRight0 u-marginRight0">...<br>[2021-11-30, 16:25:10 UTC] {subprocess.py:74} INFO - Running command: [&#39;bash&#39;, &#39;-c&#39;, &#39;cat /tmp/test.txt&#39;]<br><strong class="markup--strong markup--pre-strong">[2021-11-30, 16:25:10 UTC] {subprocess.py:85} INFO - Output:<br>[2021-11-30, 16:25:10 UTC] {subprocess.py:89} INFO - Testing.</strong><br>[2021-11-30, 16:25:10 UTC] {subprocess.py:93} INFO - Command exited with return code 0<br>...</code></pre><p name="d370" id="d370" class="graf graf--p graf-after--pre">And this indicates that we successfully output the content of the message.</p><p name="9320" id="9320" class="graf graf--p graf-after--p">We can also check out if we have this <code class="markup--code markup--p-code">test.txt</code> file created by,</p><pre name="40df" id="40df" class="graf graf--pre graf-after--p graf--trailing">$ cat /tmp/<code class="markup--code markup--pre-code">test.txt<br></code>Testing.</pre></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/3cc421a9e650"><time class="dt-published" datetime="2021-12-01T00:53:08.651Z">December 1, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/data-pipeline-1-apache-airflow-overview-3cc421a9e650" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>