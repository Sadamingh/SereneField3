<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Intro to Machine Learning 7 | Introduction to Deep Learning</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Intro to Machine Learning 7 | Introduction to Deep Learning</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Intro to Machine Learning
</section>
<section data-field="body" class="e-content">
<section name="4855" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="e8fd" id="e8fd" class="graf graf--h3 graf--leading graf--title">Intro to Machine Learning 7 | Introduction to Deep Learning</h3><figure name="e636" id="e636" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*dpqFJwXZ6uwkJjNo.png" data-width="1250" data-height="700" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*dpqFJwXZ6uwkJjNo.png"></figure><ol class="postList"><li name="ea1b" id="ea1b" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Activation Functions</strong></li></ol><p name="8e7d" id="8e7d" class="graf graf--p graf-after--li">An activation function is a function used in artificial neural networks which outputs a small value for small inputs, and a larger value if its inputs exceed a threshold. If the inputs are large enough, the activation function fires, otherwise it does nothing. In other words, an activation function is like a gate that checks that an incoming value is greater than a critical number.</p><p name="a5bb" id="a5bb" class="graf graf--p graf-after--p">Some common non-linear activation functions are,</p><ul class="postList"><li name="8437" id="8437" class="graf graf--li graf-after--p">Threshold step function</li></ul><figure name="376a" id="376a" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*2ZVLc1aTB7ClbbOAxu_gtw.png" data-width="1346" data-height="104" src="https://cdn-images-1.medium.com/max/800/1*2ZVLc1aTB7ClbbOAxu_gtw.png"></figure><ul class="postList"><li name="a6a5" id="a6a5" class="graf graf--li graf-after--figure">Rectified linear unit (ReLU)</li></ul><figure name="6ae2" id="6ae2" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*Qgs6UwjdAVStley1Tf94fQ.png" data-width="1346" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*Qgs6UwjdAVStley1Tf94fQ.png"></figure><ul class="postList"><li name="2f27" id="2f27" class="graf graf--li graf-after--figure">Parametric rectified linear unit (PReLU)</li></ul><figure name="5d9e" id="5d9e" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*md-QQuVdx5ze_KNsB_BPbw.png" data-width="1346" data-height="104" src="https://cdn-images-1.medium.com/max/800/1*md-QQuVdx5ze_KNsB_BPbw.png"></figure><ul class="postList"><li name="f325" id="f325" class="graf graf--li graf-after--figure">Sigmoid function</li></ul><figure name="f457" id="f457" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*xe7JOmHZ4_vcLvZz_AqNCQ.png" data-width="1346" data-height="104" src="https://cdn-images-1.medium.com/max/800/1*xe7JOmHZ4_vcLvZz_AqNCQ.png"></figure><ul class="postList"><li name="5976" id="5976" class="graf graf--li graf-after--figure">Hyperbolic tangent (tanh)</li></ul><figure name="9218" id="9218" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*YPBUtr3YQN1czHqQw3uyQQ.png" data-width="1346" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*YPBUtr3YQN1czHqQw3uyQQ.png"></figure><ul class="postList"><li name="68d3" id="68d3" class="graf graf--li graf-after--figure">Softmax</li></ul><figure name="2e4e" id="2e4e" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*hvdSVumRA4Ofs-dPbcE3OA.png" data-width="1346" data-height="110" src="https://cdn-images-1.medium.com/max/800/1*hvdSVumRA4Ofs-dPbcE3OA.png"></figure><p name="bf84" id="bf84" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">2. Epoch, Batch, Minibatch, Iterations</strong></p><ul class="postList"><li name="2a75" id="2a75" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">epoch</code> : An epoch means passing the entire X training matrix in the neural network once for gradient descent. <code class="markup--code markup--li-code">nepochs</code> means the total number of for gradient decent. Each update of the model parameters occurs after considering all training instances or after each epoch.</li><li name="5727" id="5727" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">batch</code> : Sometimes we have a large scale of data and it can not be passed to the network in a single batch. Therefore, we split this training data in chunks and then pass them to the network. <code class="markup--code markup--li-code">batch_size</code> is the data size of each batch and <code class="markup--code markup--li-code">nbatches</code> equals to <code class="markup--code markup--li-code">entire X size / batch size</code>.</li><li name="fc40" id="fc40" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">mini-batch</code> : Note that <code class="markup--code markup--li-code">mini-batch</code> is different from <code class="markup--code markup--li-code">batch</code> . The former one trains the neural network on the entire X training matrix, while the latter trains the neural network on different small data chunks of X separately. The mini-batch method takes the average of computed gradients across all the batches and then update the weight in an epoch. Next, we shuffle the batches, and iterate until convergence. The gradient descent based on mini-batch is also called <strong class="markup--strong markup--li-strong">mini-batch gradient descent</strong>, or more commonly <strong class="markup--strong markup--li-strong">stochastic gradient descent</strong> (SGD). It is called stochastic because a) Randomness; b) Imprecision introduced by computation of gradients on a subset of the training data.</li><li name="50af" id="50af" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">iteration</code> : Iterations are the number of batches or mini-batches we have to complete training in one epoch. The total iterations means the number of iterations in one epoch times the number of epoches.</li></ul><p name="1b23" id="1b23" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">3. PyTorch Basics</strong></p><p name="9455" id="9455" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) PyTorch For Linear Regression</strong></p><ul class="postList"><li name="7142" id="7142" class="graf graf--li graf-after--p">Import PyTorch</li></ul><pre name="ac31" id="ac31" class="graf graf--pre graf-after--li">import torch</pre><ul class="postList"><li name="fefd" id="fefd" class="graf graf--li graf-after--pre">Set features and labels as tensors</li></ul><pre name="d23e" id="d23e" class="graf graf--pre graf-after--li">n = len(df)<br>m = # of features</pre><pre name="b54b" id="b54b" class="graf graf--pre graf-after--pre">X = torch.tensor(df.x).float().reshape(n, m)<br>y = torch.tensor(df.y).float().reshape(n, 1)</pre><ul class="postList"><li name="a1a7" id="a1a7" class="graf graf--li graf-after--pre">Create a random tensor of <code class="markup--code markup--li-code">dim=(1,1)</code></li></ul><pre name="4ed4" id="4ed4" class="graf graf--pre graf-after--li">torch.randn(1, 1)</pre><ul class="postList"><li name="cd71" id="cd71" class="graf graf--li graf-after--pre">Create a randomly started parameter requires autograd of <code class="markup--code markup--li-code">dim=(1,1)</code></li></ul><pre name="82b0" id="82b0" class="graf graf--pre graf-after--li">torch.randn(1, 1, requires_grad=True)</pre><ul class="postList"><li name="7104" id="7104" class="graf graf--li graf-after--pre">Create a linear model, where <code class="markup--code markup--li-code">m</code> and <code class="markup--code markup--li-code">b</code> are randomly started parameters,</li></ul><pre name="cbcb" id="cbcb" class="graf graf--pre graf-after--li">y_pred = m * X + b</pre><ul class="postList"><li name="e53d" id="e53d" class="graf graf--li graf-after--pre">Linear loss function based on RMSE</li></ul><pre name="0d0f" id="0d0f" class="graf graf--pre graf-after--li">loss = torch.mean((y_pred - y)**2)</pre><ul class="postList"><li name="26f5" id="26f5" class="graf graf--li graf-after--pre">Computes the gradient of <code class="markup--code markup--li-code">loss</code> with respect to all variables with <code class="markup--code markup--li-code">requires_grad=True</code>.</li></ul><pre name="4fd1" id="4fd1" class="graf graf--pre graf-after--li">loss.backward()</pre><ul class="postList"><li name="a23a" id="a23a" class="graf graf--li graf-after--pre">The gradient of loss with respect to parameter <code class="markup--code markup--li-code">m</code> after calling <code class="markup--code markup--li-code">backward</code></li></ul><pre name="1082" id="1082" class="graf graf--pre graf-after--li">m.grad</pre><ul class="postList"><li name="6e42" id="6e42" class="graf graf--li graf-after--pre">Reset the gradient of loss with respect to parameter <code class="markup--code markup--li-code">m</code> to 0</li></ul><pre name="0ca0" id="0ca0" class="graf graf--pre graf-after--li">m.grad.zero_()</pre><ul class="postList"><li name="2218" id="2218" class="graf graf--li graf-after--pre">Inbuilt Adam optimizer for all the model parameters with <code class="markup--code markup--li-code">requires_grad=True</code></li></ul><pre name="c7dd" id="c7dd" class="graf graf--pre graf-after--li">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</pre><ul class="postList"><li name="e86d" id="e86d" class="graf graf--li graf-after--pre">Zero out all the model parameters with <code class="markup--code markup--li-code">requires_grad=True</code></li></ul><pre name="9373" id="9373" class="graf graf--pre graf-after--li">optimizer.zero_grad()</pre><ul class="postList"><li name="be0c" id="be0c" class="graf graf--li graf-after--pre">Take one step in gradient descent based on the gradient of loss with respect to parameters</li></ul><pre name="6c49" id="6c49" class="graf graf--pre graf-after--li">optimizer.step()</pre><p name="c572" id="c572" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(2) PyTorch For Frank Rosenblatt Perceptron</strong></p><ul class="postList"><li name="b964" id="b964" class="graf graf--li graf-after--p">PyTorch in-built linear model</li></ul><pre name="df28" id="df28" class="graf graf--pre graf-after--li">nn.Linear(1, 1)</pre><ul class="postList"><li name="8a11" id="8a11" class="graf graf--li graf-after--pre">PyTorch in-built ReLU</li></ul><pre name="9cc2" id="9cc2" class="graf graf--pre graf-after--li">nn.ReLU()</pre><ul class="postList"><li name="cc1b" id="cc1b" class="graf graf--li graf-after--pre">PyTorch in-built Sigmoid</li></ul><pre name="07eb" id="07eb" class="graf graf--pre graf-after--li">nn.Sigmoid()</pre><ul class="postList"><li name="d77b" id="d77b" class="graf graf--li graf-after--pre">PyTorch in-built function used to glue models together</li></ul><pre name="81df" id="81df" class="graf graf--pre graf-after--li">nn.Sequential()</pre><ul class="postList"><li name="d275" id="d275" class="graf graf--li graf-after--pre">Frank Rosenblatt Perceptron (similar to Logistic Regression for statistics, see a difference <a href="https://stats.stackexchange.com/questions/162257/whats-the-difference-between-logistic-regression-and-perceptron" data-href="https://stats.stackexchange.com/questions/162257/whats-the-difference-between-logistic-regression-and-perceptron" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">here</a>)</li></ul><pre name="56ca" id="56ca" class="graf graf--pre graf-after--li">model = nn.Sequential(<br>    nn.Linear(100, 1),<br>    nn.Sigmoid()<br>)</pre><ul class="postList"><li name="c0f2" id="c0f2" class="graf graf--li graf-after--pre">Multi-neuron Frank Rosenblatt Perceptron with a 1000-neuron layer</li></ul><pre name="a550" id="a550" class="graf graf--pre graf-after--li">model = nn.Sequential(<br>    nn.Linear(100, 1000),      # hidden 1000-neuron layer<br>    nn.Sigmoid(),              # activation function<br>    nn.Linear(1000, 1)         # output layer<br>)</pre><p name="2672" id="2672" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(3) PyTorch For Classification</strong></p><ul class="postList"><li name="d62f" id="d62f" class="graf graf--li graf-after--p">Regressors typically use mean squared error, but classifiers typically used log loss (cross-entropy loss).</li></ul><pre name="be66" id="be66" class="graf graf--pre graf-after--li">import torch.nn.functional as F</pre><pre name="7c5c" id="7c5c" class="graf graf--pre graf-after--pre">loss = F.binary_cross_entropy(y_pred, y)<br>loss = F.cross_entropy(y_pred, y)</pre><ul class="postList"><li name="1dee" id="1dee" class="graf graf--li graf-after--pre">Note that the output layer for classification should be a Sigmoid activation function or a Softmax activation function or something like that</li></ul><pre name="85b3" id="85b3" class="graf graf--pre graf-after--li">nn.Sigmoid()              # for binary class classification<br>nn.Softmax(dim=1)         # for multiclass classification</pre><p name="a1ba" id="a1ba" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(4) Running PyTorch on GPU</strong></p><ul class="postList"><li name="856f" id="856f" class="graf graf--li graf-after--p">Find if we have a CUDA device (i.e. GPU from Nvidia), if we don’t have it, just name it as <code class="markup--code markup--li-code">cpu</code></li></ul><pre name="4ff7" id="4ff7" class="graf graf--pre graf-after--li">device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)</pre><ul class="postList"><li name="827c" id="827c" class="graf graf--li graf-after--pre">Move the tensors with data to the GPU/CPU device</li></ul><pre name="8eb9" id="8eb9" class="graf graf--pre graf-after--li">X = torch.tensor(X).float().to(device)<br>y = torch.tensor(y).to(device)</pre><ul class="postList"><li name="435a" id="435a" class="graf graf--li graf-after--pre">Also, make sure the model runs on GPU</li></ul><pre name="82f0" id="82f0" class="graf graf--pre graf-after--li">model = model.to(device)</pre><p name="265c" id="265c" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">4. Validation for Deep Learning Models</strong></p><p name="2536" id="2536" class="graf graf--p graf-after--p">Deep learning models often have so many parameters that we can drive training loss to zero, but unfortunately, the validation loss usually grows as the model overfits. For example, we are expected to see the following plots when training,</p><figure name="95be" id="95be" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AX4k0xf9SBQnumIfEWIvkw.png" data-width="705" data-height="154" src="https://cdn-images-1.medium.com/max/800/1*AX4k0xf9SBQnumIfEWIvkw.png"><figcaption class="imageCaption">these plots also imply the nepochs we should select</figcaption></figure><p name="234d" id="234d" class="graf graf--p graf-after--figure">One way to reduce overfitting is to use weight decay, which is actually the same as the L2 regularization we have discussed before. Without constraints, model parameters can get very large, which typically leads to a lack of generality. Using the <code class="markup--code markup--p-code">Adam</code> optimizer, we turn on weight decay with parameter <code class="markup--code markup--p-code">weight_decay</code>, but otherwise, the training loop is the same. For example,</p><pre name="2c50" id="2c50" class="graf graf--pre graf-after--p">optimizer = torch.optim.Adam(model.parameters(),<br>                             lr=learning_rate,<br>                             weight_decay=weight_decay)</pre><p name="bbea" id="bbea" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">5. Deep Learning Frequently Asked Questions</strong></p><ul class="postList"><li name="49b6" id="49b6" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Why do we have to normalize before we train a neural network?</strong></li></ul><p name="bd23" id="bd23" class="graf graf--p graf-after--li">Or it would be hard to find a suitable learning rate for all the parameters.</p><ul class="postList"><li name="54b5" id="54b5" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">How to prevent overfitting a neural network?</strong></li></ul><p name="397a" id="397a" class="graf graf--p graf-after--li">Introduce weight decay to the optimizer or simply find the <code class="markup--code markup--p-code">nepochs</code> with the lowest validation loss.</p><ul class="postList"><li name="d247" id="d247" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Why does the training loss sometimes pop up and then go back down? Why is it not monotonically decreasing?</strong></li></ul><p name="d7b1" id="d7b1" class="graf graf--p graf-after--li">It is likely the learning rate is too high and therefore, as we approach the minimum of the lost function, our steps are too big. We are jumping back and forth across the location of the minimum in parameter space.</p><ul class="postList"><li name="6180" id="6180" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">What happens if we set a small learning rate but keep the same number of epochs?</strong></li></ul><p name="4e00" id="4e00" class="graf graf--p graf-after--li">The training loss continues to decrease but it will stop at a relatively high training loss. The training loss stays at a high level because we don’t have enough iterations for it to converge.</p><ul class="postList"><li name="17e3" id="17e3" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">What happens if we set a small learning rate and also increase the number of epochs?</strong></li></ul><p name="9e14" id="9e14" class="graf graf--p graf-after--li">The training will be very slow because we have more iterations and a smaller learning rate. The training loss continues to decrease but much lower than before and stops long before reaching a loss very near zero.</p><ul class="postList"><li name="045d" id="045d" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">How do we know what the right value of the weight decay is for our optimizer?</strong></li></ul><p name="3b41" id="3b41" class="graf graf--p graf-after--li graf--trailing">Typically we try a variety of weight decay values and then see which one gives us the best validation error.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/ad84e1838f07"><time class="dt-published" datetime="2021-12-15T17:59:21.505Z">December 15, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/intro-to-machine-learning-7-introduction-to-deep-learning-ad84e1838f07" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>