<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Intro to Machine Learning 5 | Random Forest</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Intro to Machine Learning 5 | Random Forest</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Intro to Machine Learning
</section>
<section data-field="body" class="e-content">
<section name="7dc4" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="6968" id="6968" class="graf graf--h3 graf--leading graf--title">Intro to Machine Learning 5 | Random¬†Forest</h3><figure name="4a09" id="4a09" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*G2559Svt3s6y3FZm.png" data-width="1250" data-height="700" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*G2559Svt3s6y3FZm.png"></figure><ol class="postList"><li name="e771" id="e771" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Improving Decision Tree Generality</strong></li></ol><p name="9104" id="9104" class="graf graf--p graf-after--li">Now we have known that the decision tree can have high accuracy, but it may overfit like crazy. So our goal for the new model is to <strong class="markup--strong markup--p-strong">keep this high accuracy but increase the generality</strong>. However, trees are very stable and they will grow to the exact same model if we start from the same root set. So two techniques we use for creating independent trees are,</p><ul class="postList"><li name="ef87" id="ef87" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Bootstrapping/Bagging</strong>: Weaken trees by training on the randomly selected subset.</li><li name="add3" id="add3" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Amnesia</strong>: Forgetting some features as we create decision nodes. This is because if there is one strongly predictive var out of ùëù, then all trees would be similar.</li></ul><p name="b32a" id="b32a" class="graf graf--p graf-after--li">Another technique is to use extremely random forests, which will not do bagging and Amnesia. What it is going to do is to randomly split with the most predictive feature in each decision node.</p><p name="7e00" id="7e00" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Amnesia in Sklearn</strong></p><p name="5a81" id="5a81" class="graf graf--p graf-after--p">In sci-kit learn, we use the hyperparameter <code class="markup--code markup--p-code">max_features</code> for amnesia. If <code class="markup--code markup--p-code">int</code>, then it considers <code class="markup--code markup--p-code">max_features</code> features at each split. If <code class="markup--code markup--p-code">float</code>, then <code class="markup--code markup--p-code">max_features</code> is a fraction and <code class="markup--code markup--p-code">round(max_features * n_features)</code> features are considered at each split.</p><ul class="postList"><li name="9200" id="9200" class="graf graf--li graf-after--p">If <code class="markup--code markup--li-code">max_features</code> is too high, we may bind to similar trees trained on the strongly predictive variable.</li><li name="6efc" id="6efc" class="graf graf--li graf-after--li">If <code class="markup--code markup--li-code">max_features</code> is too low, we can have bad accuracy because we are forgetting too many features in each node.</li></ul><p name="0dd3" id="0dd3" class="graf graf--p graf-after--li">Therefore, we need to perform hyperparameter tuning for <code class="markup--code markup--p-code">max_features</code>.</p><p name="a733" id="a733" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Properties of Random Forest</strong></p><ul class="postList"><li name="514c" id="514c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Accuracy initially improves greatly as we add trees.</strong></li></ul><p name="7349" id="7349" class="graf graf--p graf-after--li">This is because each tree sees only 2/3 of data by bagging so adding bootstrapped trees increases the use of training data.</p><ul class="postList"><li name="5ee0" id="5ee0" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Accuracy asymptotically approaches a minimum instead of continual improvement.</strong></li></ul><p name="261c" id="261c" class="graf graf--p graf-after--li">With enough trees, the ensemble sees 100% of the training data, so it‚Äôs approaching the accuracy of the single decision tree in the ideal world.</p><ul class="postList"><li name="c03c" id="c03c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">RF does not overfit as more trees are added.</strong></li></ul><p name="e29e" id="e29e" class="graf graf--p graf-after--li">New trees <strong class="markup--strong markup--p-strong">balance each other out</strong>, one might be too high, another too low, so new trees get averaged in so each additional tree has a less individual effect.</p><ul class="postList"><li name="4eb2" id="4eb2" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">RF is relatively robust to ùë¶ outliers.</strong></li></ul><p name="2fef" id="2fef" class="graf graf--p graf-after--li">The ùë¶ outliers get shunted to their <strong class="markup--strong markup--p-strong">own leaf</strong> since doing so reduces loss function, particularly if squared-error is used.</p><ul class="postList"><li name="3591" id="3591" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">RF is relatively robust to ùëã noise.</strong></li></ul><p name="d1a0" id="d1a0" class="graf graf--p graf-after--li">Noise variables in <em class="markup--em markup--p-em">X</em> aren‚Äôt predictive so not chosen as split variables.</p><ul class="postList"><li name="e232" id="e232" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">RF is relatively robust to ùëã multicollinearity.</strong></li></ul><p name="e9b7" id="e9b7" class="graf graf--p graf-after--li">The forest will randomly choose to split between two collinear columns so multicollinearity is not a problem.</p><ul class="postList"><li name="3f46" id="3f46" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">RF is impacted by falsely-predictive features like sales ID.</strong></li></ul><p name="56bb" id="56bb" class="graf graf--p graf-after--li">Even though RF is robust to X noises, it can be highly impacted by the falsely-predictive features. So dropping useless features also often gives a small bump.</p><ul class="postList"><li name="a724" id="a724" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Bagging helps more, the more unstable the model.</strong></li></ul><p name="63a4" id="63a4" class="graf graf--p graf-after--li">Averaging is a smoothing operator, which squeezes predictions to the centroid. If the model is low variance already, there is no point in bagging.</p><ul class="postList"><li name="d4c7" id="d4c7" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">RFs are scale and range insensitive in features and target ùë¶.</strong></li></ul><p name="826a" id="826a" class="graf graf--p graf-after--li">This is because RFs are quite simple in each node. It compares feature values in decision nodes but does not do maths on them. Besides, the target <em class="markup--em markup--p-em">y</em> is computed by its means or modes for predictions, which have no use of features.</p><p name="52a4" id="52a4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4. Bootstrapping Vs. Subsampling</strong></p><ul class="postList"><li name="076d" id="076d" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Bootstrapping</strong>: Popular and safer to use.</li><li name="ab18" id="ab18" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Subsampling</strong>: A kind of new theory that can get the same performance on n/2 subsamples. It also improves the generality if we have a smaller fraction of <em class="markup--em markup--li-em">n</em>.</li></ul><p name="a1ad" id="a1ad" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">5. Goal of RF Tuning Strategy</strong></p><p name="c47b" id="c47b" class="graf graf--p graf-after--p">The general goal is to do something that can minimize the validation error and then stop.</p><ul class="postList"><li name="c0e0" id="c0e0" class="graf graf--li graf-after--p">Start with 20 trees and work upwards until <strong class="markup--strong markup--li-strong">validation error</strong> stops getting better.</li><li name="6915" id="6915" class="graf graf--li graf-after--li">Sklearn <code class="markup--code markup--li-code">uses max_features= sqrt(p)</code> by default, so try dropping this to <code class="markup--code markup--li-code">log(p)</code>, or <code class="markup--code markup--li-code">p/3</code> for regression and <code class="markup--code markup--li-code">sqrt(p)</code> for classification.</li></ul><p name="76f2" id="76f2" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">6. Out-Of-Bag (OOB) Samples</strong></p><p name="f457" id="f457" class="graf graf--p graf-after--p">Bootstrap aggregating is also called bagging, and that means training an ensemble of models based on bootstrapping. Random forest applies bootstrapping to reduce overfitting, and the trees in a random forest are called <strong class="markup--strong markup--p-strong">bagged trees</strong>. For a specific bagged tree, the set that is not used for training this tree is called an <strong class="markup--strong markup--p-strong">out-of-sample sample</strong> (about 37%).</p><ul class="postList"><li name="670d" id="670d" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Out-Of-Bag Predictions</strong>: get OOB predictions by averaging (for regression) or voting (for classification) estimates from the bagged trees with that record in their OOB samples.</li><li name="b1b3" id="b1b3" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Out-Of-Bag Score</strong>: get R¬≤ score for regression or accuracy score for classification on the label y and its OOB predictions. Ignore or skip the records that never appear in any OOB samples.</li><li name="6014" id="6014" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Out-Of-Bag Error</strong>: equals to <code class="markup--code markup--li-code">1 - OOB_score</code> according to sci-kit learn.</li></ul><p name="be61" id="be61" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">7. OOB Frequently Asked Questions</strong></p><ul class="postList"><li name="5080" id="5080" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Why does OOB error slightly overestimate testing/validation error? Or when does OOB score sightly underestimate testing/validation score?</strong></li></ul><p name="1d9e" id="1d9e" class="graf graf--p graf-after--li">Because OOB samples are not predicted with all trees in the forest whereas the test/validation set uses the whole forest, which presumably has lower noise/variation.</p><ul class="postList"><li name="c6ee" id="c6ee" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Why OOB Error should not be used with time-sensitive datasets?</strong></li></ul><p name="4e96" id="4e96" class="graf graf--p graf-after--li">The validation set for time-sensitive data can‚Äôt be split randomly. Probably think about one-step validation.</p><ul class="postList"><li name="6cd4" id="6cd4" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">When OOB error is lower than validation/testing error? Or when the OOB score is higher than the validation/testing score?</strong></li></ul><p name="daa3" id="daa3" class="graf graf--p graf-after--li">Consider the following reasons,</p><ul class="postList"><li name="824b" id="824b" class="graf graf--li graf-after--p">The validation set is drawn from a different distribution than the training set</li><li name="52b6" id="52b6" class="graf graf--li graf-after--li">It is a time-sensitive data set</li><li name="6a16" id="6a16" class="graf graf--li graf-after--li">We didn‚Äôt extract the validation set properly</li><li name="3659" id="3659" class="graf graf--li graf-after--li">The model is overfitted to the data in the training set, focusing on relationships that are not relevant to the test set (e.g. sales ID case)</li></ul><p name="4953" id="4953" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">8. Common Feature Importance Techniques</strong></p><ul class="postList"><li name="8e50" id="8e50" class="graf graf--li graf-after--p">Spearman‚Äôs rank correlation</li><li name="71d8" id="71d8" class="graf graf--li graf-after--li">Principle component analysis (PCA)</li><li name="17ae" id="17ae" class="graf graf--li graf-after--li">Minimal-redundancy maximal-relevance (mRMR)</li></ul><p name="8761" id="8761" class="graf graf--p graf-after--li">Note that these techniques should only be used on strong and stable models.</p><p name="dd67" id="dd67" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">9. Feature Importance For Random Forest</strong></p><ul class="postList"><li name="20f0" id="20f0" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">sklearn</code> default: <strong class="markup--strong markup--li-strong">Gini/MSE Drop</strong>. However, this gives us biased feature importance. This common mechanism for computing feature importance is biased, and it tends to inflate the importance of continuous or high-cardinality categorical variables.</li><li name="f1dc" id="f1dc" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Drop-Column Importance</strong>: brute force and easy, however, it is very expensive and slow because it means retraining the model <em class="markup--em markup--li-em">p</em> times for <em class="markup--em markup--li-em">p</em> features.</li><li name="3758" id="3758" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Permutation Importance</strong>: easy and no need to retrain the model</li></ul><p name="2364" id="2364" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">10. Codependent Features for Feature Importance</strong></p><ul class="postList"><li name="9ff1" id="9ff1" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Drop-column</strong>: tends to show low or 0 importance scores for codependent features</li><li name="d8a4" id="d8a4" class="graf graf--li graf-after--li graf--trailing"><strong class="markup--strong markup--li-strong">Permutation</strong>: has the effect of pulling down the perceived importance of the original and result in a shared importance</li></ul></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/62cc71b384a"><time class="dt-published" datetime="2021-12-15T18:06:09.150Z">December 15, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/intro-to-machine-learning-5-random-forest-62cc71b384a" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>