<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Intro to Machine Learning 2 | Linear Model Regularization and Various Types of Gradient Descents</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Intro to Machine Learning 2 | Linear Model Regularization and Various Types of Gradient Descents</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Intro to Machine Learning
</section>
<section data-field="body" class="e-content">
<section name="3a5c" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1ffb" id="1ffb" class="graf graf--h3 graf--leading graf--title">Intro to Machine Learning 2 | Linear Model Regularization and Various Types of Gradient Descents</h3><figure name="40dc" id="40dc" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*-fXJmWjQgcGzbmu4.png" data-width="1250" data-height="700" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*-fXJmWjQgcGzbmu4.png"></figure><blockquote name="5a1b" id="5a1b" class="graf graf--blockquote graf-after--figure"><strong class="markup--strong markup--blockquote-strong">Acknowledge</strong>: Thanks to Terence Parr, the professor of the University of San Francisco who made these cool visualizations. You can find these fancy pictures on <a href="https://explained.ai/regularization/index.html" data-href="https://explained.ai/regularization/index.html" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">his blog</a>. Thanks to <a href="https://medium.com/u/3cc31a4b9430" data-href="https://medium.com/u/3cc31a4b9430" data-anchor-type="2" data-user-id="3cc31a4b9430" data-action-value="3cc31a4b9430" data-action="show-user-card" data-action-type="hover" class="markup--user markup--blockquote-user" target="_blank">Lili Jiang</a> and the <a href="https://github.com/lilipads/gradient_descent_viz" data-href="https://github.com/lilipads/gradient_descent_viz" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">gradient visualization tool</a> she built, we can have these cool pictures about some different gradients methods.</blockquote><ol class="postList"><li name="0a73" id="0a73" class="graf graf--li graf-after--blockquote"><strong class="markup--strong markup--li-strong">The Definition of Regularization</strong></li></ol><p name="70d6" id="70d6" class="graf graf--p graf-after--li">Regularization is a form of statistical tool that constrains, regularizes, or shrinks the coefficient estimates towards zero.</p><p name="bdac" id="bdac" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Benefits of Regularization for Linear Models</strong></p><p name="9594" id="9594" class="graf graf--p graf-after--p">Commonly, regularization has three main benefits,</p><ul class="postList"><li name="51fb" id="51fb" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Prevent overfitting</strong></li><li name="210c" id="210c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Increase generality</strong></li><li name="1557" id="1557" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Reduce model variance</strong> of estimates</li></ul><p name="1637" id="1637" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">3. Downsides of Regularization for Linear Models</strong></p><p name="e01f" id="e01f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Increase bias</strong> (the amount that a model’s prediction differs from the target value). This is not serious but the consequence is that we no longer know the effect of <em class="markup--em markup--p-em">x_i </em>on target <em class="markup--em markup--p-em">y</em> via <em class="markup--em markup--p-em">β_i</em>. So the method of regularization seems like we trade bias to get a lower variance.</p><p name="24be" id="24be" class="graf graf--p graf-after--p">Here is a picture showing the relationship between bias and variance.</p><figure name="d744" id="d744" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4EEGdB9Wi6t-724_IDu3ZA.png" data-width="1464" data-height="422" src="https://cdn-images-1.medium.com/max/800/1*4EEGdB9Wi6t-724_IDu3ZA.png"></figure><p name="119c" id="119c" class="graf graf--p graf-after--figure">And here is another figure that shows why we can not achieve good bias and good variance at the same time.</p><figure name="9eda" id="9eda" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*7ZR83aAkD0PWDAougkLc5A.png" data-width="1616" data-height="426" src="https://cdn-images-1.medium.com/max/800/1*7ZR83aAkD0PWDAougkLc5A.png"></figure><p name="fe18" id="fe18" class="graf graf--p graf-after--figure">The expected prediction error decomposition should be as follows,</p><figure name="aa5d" id="aa5d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8W1n1ml-a3_wi7DMTtEoiA.png" data-width="1348" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*8W1n1ml-a3_wi7DMTtEoiA.png"></figure><p name="e187" id="e187" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">4. Regularization Methodology</strong></p><p name="86a5" id="86a5" class="graf graf--p graf-after--p">The idea behind regularization is to restrict the size of coefficients with hard constraint or soft constraint using the penalty term.</p><figure name="4fc6" id="4fc6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yxniGLD6DevIR6XFJrVtkA.png" data-width="1916" data-height="422" src="https://cdn-images-1.medium.com/max/800/1*yxniGLD6DevIR6XFJrVtkA.png"><figcaption class="imageCaption">The soft constraint is actually to find the combined optimization</figcaption></figure><ul class="postList"><li name="ec4f" id="ec4f" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Hard constraint</strong>: this is a conceptual statistical method and we have to construct a safe zone (which is the constraint of the coefficient) for finding the loss minimum. The minimum loss is inside the safe zone or on the zone border.</li></ul><figure name="c6ef" id="c6ef" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*FneDnQiPpPSebtPt0eTq-A.png" data-width="1740" data-height="132" src="https://cdn-images-1.medium.com/max/800/1*FneDnQiPpPSebtPt0eTq-A.png"><figcaption class="imageCaption">The hard constraint with L2 regularization</figcaption></figure><ul class="postList"><li name="2fef" id="2fef" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Soft constraint</strong>: soft constraint is to add a penalty in the loss function by Lagrange multipliers so there’s no hard cutoff.</li></ul><figure name="d85a" id="d85a" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*YZp-IMbQUddb0zoHP4_D8w.png" data-width="1740" data-height="132" src="https://cdn-images-1.medium.com/max/800/1*YZp-IMbQUddb0zoHP4_D8w.png"><figcaption class="imageCaption">The soft constraint with L2 regularization</figcaption></figure><p name="e7d7" id="e7d7" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">5. Why do we need to apply normalization/standardization for regularization?</strong></p><p name="cc93" id="cc93" class="graf graf--p graf-after--p">Note that the concept of normalization or standardization here means the same thing, which is to scale the raw data by centered around the zero mean with a unit standard deviation (i.e. <code class="markup--code markup--p-code">stdev = 1</code>). The concept of normalization in sci-kit learn is quite different and what we are referring to is the <code class="markup--code markup--p-code">StandardScaler</code>.</p><p name="b30a" id="b30a" class="graf graf--p graf-after--p">Basically, we must apply normalization/standardization before regularization for two reasons,</p><ul class="postList"><li name="bcb0" id="bcb0" class="graf graf--li graf-after--p">It leads to <strong class="markup--strong markup--li-strong">faster training</strong>: This is because we can go downhill in a rapid way. Loss contours for normalized variables are more spherical, which leads to faster convergence. See <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" data-href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">this paper</a>.</li></ul><figure name="0e8c" id="0e8c" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*lM9wrcsFf2QAMlX6dqMUAw.png" data-width="1654" data-height="462" src="https://cdn-images-1.medium.com/max/800/1*lM9wrcsFf2QAMlX6dqMUAw.png"><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Note</strong>: the normalized variables have a shape more like a <strong class="markup--strong markup--figure-strong">bowl</strong>, which leads to a rapid convergence</figcaption></figure><ul class="postList"><li name="f4df" id="f4df" class="graf graf--li graf-after--figure">It is <strong class="markup--strong markup--li-strong">required by regularization</strong>: the process of standardization is needed for generalization purposes, or the regularization shrinks coefficients <strong class="markup--strong markup--li-strong">disproportionately</strong></li></ul><p name="0959" id="0959" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">6. Ridge Regression</strong></p><p name="2f30" id="2f30" class="graf graf--p graf-after--p">Ridge regression adds an L2 regularization penalty to the loss function, which equals the square of the magnitude of coefficients.</p><figure name="eb26" id="eb26" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6R1qeWFihZK59FQX1bG97w.png" data-width="1288" data-height="128" src="https://cdn-images-1.medium.com/max/800/1*6R1qeWFihZK59FQX1bG97w.png"></figure><figure name="a5a1" id="a5a1" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*mUFFThwq9qGTCBx8fr_tAA.png" data-width="1017" data-height="234" src="https://cdn-images-1.medium.com/max/800/1*mUFFThwq9qGTCBx8fr_tAA.png"></figure><p name="134f" id="134f" class="graf graf--p graf-after--figure">On mathematics, the OLS estimate is,</p><figure name="7ca4" id="7ca4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WpAhAMlg_eDm1OLBYBRcfw.png" data-width="1338" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*WpAhAMlg_eDm1OLBYBRcfw.png"><figcaption class="imageCaption">Refer to <a href="https://medium.com/adamedelwiess/linear-algebra-7-orthonormal-matrix-orthogonal-transformation-gram-schmidt-orthogonalization-a10cb1c3a218" data-href="https://medium.com/adamedelwiess/linear-algebra-7-orthonormal-matrix-orthogonal-transformation-gram-schmidt-orthogonalization-a10cb1c3a218" class="markup--anchor markup--figure-anchor" target="_blank">this article</a> if you are not familiar with this formula</figcaption></figure><p name="2f1a" id="2f1a" class="graf graf--p graf-after--figure">And the Ridge estimate is,</p><figure name="3e78" id="3e78" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*7N38nd2mZ40MPFXmE9rrxQ.png" data-width="1652" data-height="80" src="https://cdn-images-1.medium.com/max/800/1*7N38nd2mZ40MPFXmE9rrxQ.png"><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Note</strong>: It is called “Ridge” because the identity matrix (I) can be described as a ridge</figcaption></figure><p name="1d0f" id="1d0f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">7. L1 Lasso Regression</strong></p><p name="60b8" id="60b8" class="graf graf--p graf-after--p">Lasso regression adds an L1 regularization penalty to the loss function, which equals the absolute value of the magnitude of coefficients.</p><figure name="2c38" id="2c38" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2R07kJXYQ-HlV35okg1f6A.png" data-width="1422" data-height="130" src="https://cdn-images-1.medium.com/max/800/1*2R07kJXYQ-HlV35okg1f6A.png"></figure><figure name="f0c9" id="f0c9" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*Bcf1VBexLJ7mlSpsN9TsNw.png" data-width="1017" data-height="234" src="https://cdn-images-1.medium.com/max/800/1*Bcf1VBexLJ7mlSpsN9TsNw.png"></figure><p name="5fd6" id="5fd6" class="graf graf--p graf-after--figure">Lasso is called so because the acronym LASSO stands for Least Absolute Shrinkage and Selection Operator.</p><p name="fb2f" id="fb2f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">8. Differences Between L1 and L2 Regularization</strong></p><p name="a750" id="a750" class="graf graf--p graf-after--p">Both L1 and L2 increase model bias.</p><p name="0fce" id="0fce" class="graf graf--p graf-after--p">L1 encourages parameters shrinking to zeros, so it is more useful for variable or feature selection as some parameters go to 0.</p><p name="1ef0" id="1ef0" class="graf graf--p graf-after--p">L2 is more balanced or shrinks evenly (i.e. it discourages any parameter bigger than the other parameters) and is more useful when we have collinear features. This is because L2 reduces the variance of estimate parameters which counteracts the effect of collinearity</p><p name="cb7a" id="cb7a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">9. Turning Parameter (aka. Penalty Parameter) Selection</strong></p><p name="3487" id="3487" class="graf graf--p graf-after--p">The turning parameter λ controls the strength of the L1/L2 penalty term.</p><ul class="postList"><li name="5599" id="5599" class="graf graf--li graf-after--p">When λ increases, we can achieve lower variance but we pay higher bias.</li><li name="a993" id="a993" class="graf graf--li graf-after--li">When λ decreases, we can achieve lower bias but we pay higher variance.</li><li name="a85a" id="a85a" class="graf graf--li graf-after--li">When λ is 0, we remove the penalty term and the regularization turns off.</li></ul><p name="97b7" id="97b7" class="graf graf--p graf-after--li">Particularly, when we have L1 regularization,</p><ul class="postList"><li name="623c" id="623c" class="graf graf--li graf-after--p">When λ increases, we have more parameters shrinking to 0.</li><li name="3e80" id="3e80" class="graf graf--li graf-after--li">When λ decreases, we have fewer parameters shrinking to 0.</li><li name="205f" id="205f" class="graf graf--li graf-after--li">When λ is infinite, all the parameters shrink to 0.</li></ul><p name="fb78" id="fb78" class="graf graf--p graf-after--li">To select a good turning parameter λ, we have to find it by computing the minimum loss for different λ values, then pick λ that gets min loss on the validation set.</p><p name="23c8" id="23c8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">10. Regularized Logistic Regression</strong></p><p name="435c" id="435c" class="graf graf--p graf-after--p">The L1 and L2 regularization for the logistic regression have exactly the same mechanism.</p><figure name="1818" id="1818" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zoJ018naUFT6hl7ApI69cw.png" data-width="1338" data-height="138" src="https://cdn-images-1.medium.com/max/800/1*zoJ018naUFT6hl7ApI69cw.png"><figcaption class="imageCaption">L1 Regularized Logistic Regression</figcaption></figure><figure name="eeba" id="eeba" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*ApD1CEex5IUvhV1y5ecHTw.png" data-width="1338" data-height="138" src="https://cdn-images-1.medium.com/max/800/1*ApD1CEex5IUvhV1y5ecHTw.png"><figcaption class="imageCaption">L2 Regularized Logistic Regression</figcaption></figure><p name="c739" id="c739" class="graf graf--p graf-after--figure">However, according to the <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf" data-href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ESLII book</a> (page 125), we don’t want to shrink β_0 too small because it doesn&#39;t affect the variance as we expected. This means we must find β_0 different than the other parameters.</p><figure name="a3f5" id="a3f5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FS_tucUmD33H5rvJQ47WmA.png" data-width="1338" data-height="164" src="https://cdn-images-1.medium.com/max/800/1*FS_tucUmD33H5rvJQ47WmA.png"></figure><p name="84a4" id="84a4" class="graf graf--p graf-after--figure">Commonly, we will directly assign β_0 as the mean value of <em class="markup--em markup--p-em">y</em>, assuming zero-centered data set.</p><p name="fbc7" id="fbc7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">11. Gradient Descent</strong></p><ul class="postList"><li name="f2e4" id="f2e4" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Slope</strong> (change in loss or β_i) in direction β_i: is the partial derivative</li></ul><figure name="26c3" id="26c3" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*fFZ2tFJwIFyMfSY0PUKf6A.png" data-width="1342" data-height="112" src="https://cdn-images-1.medium.com/max/800/1*fFZ2tFJwIFyMfSY0PUKf6A.png"></figure><ul class="postList"><li name="3ac6" id="3ac6" class="graf graf--li graf-after--figure">The <strong class="markup--strong markup--li-strong">gradient</strong> is a <em class="markup--em markup--li-em">p</em> or <em class="markup--em markup--li-em">p+1</em> dimensional vector of partial derivatives. For example, for a 2D gradient,</li></ul><figure name="57af" id="57af" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*U6CWjTdby5GZwlOLBKzqwg.png" data-width="1342" data-height="134" src="https://cdn-images-1.medium.com/max/800/1*U6CWjTdby5GZwlOLBKzqwg.png"></figure><ul class="postList"><li name="9e88" id="9e88" class="graf graf--li graf-after--figure">The <strong class="markup--strong markup--li-strong">direction</strong> of the gradient is dependent on the sign of the slope, which points towards higher loss. So the gradient should therefore step by the <strong class="markup--strong markup--li-strong">negative</strong> of the gradient in order to find the minimum value.</li></ul><figure name="8d89" id="8d89" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*G2MUxuD_xp5p3jDBpH__TQ.png" data-width="1342" data-height="114" src="https://cdn-images-1.medium.com/max/800/1*G2MUxuD_xp5p3jDBpH__TQ.png"></figure><p name="8f15" id="8f15" class="graf graf--p graf-after--figure">where η is called the learning rate.</p><p name="8a6c" id="8a6c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">12. Learn Rate Selection</strong></p><p name="23c4" id="23c4" class="graf graf--p graf-after--p">Learning rate selection is a trade-off between the rate of convergence and overshooting.</p><ul class="postList"><li name="1cc9" id="1cc9" class="graf graf--li graf-after--p">When η is too large, we can go nowhere and β can even diverge</li><li name="8310" id="8310" class="graf graf--li graf-after--li">When η is large, β oscillate across the valley</li><li name="2251" id="2251" class="graf graf--li graf-after--li">When η is too small, β doesn’t make much progress towards min loss point and it may take too long to converge or stuck in the local</li></ul><p name="c0ba" id="c0ba" class="graf graf--p graf-after--li">To select a good η, we have to find it by computing the minimum loss for different η values, then pick η that gets min loss on the validation set.</p><p name="4aec" id="4aec" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">13. Different Types of Gradient Descent</strong></p><ul class="postList"><li name="8cbf" id="8cbf" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Vanilla Gradient Descent</strong>: the simplest version</li></ul><figure name="7347" id="7347" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*pkj5FOxd2a3OxqOK5i5srA.png" data-width="2696" data-height="636" src="https://cdn-images-1.medium.com/max/800/1*pkj5FOxd2a3OxqOK5i5srA.png"><figcaption class="imageCaption">Source: <a href="https://distill.pub/2017/momentum/" data-href="https://distill.pub/2017/momentum/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://distill.pub/2017/momentum/</a></figcaption></figure><p name="4796" id="4796" class="graf graf--p graf-after--figure">Repeat:</p><figure name="6ffd" id="6ffd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0UN-n7uyQfrofq-9NBH8Pw.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*0UN-n7uyQfrofq-9NBH8Pw.png"></figure><p name="447f" id="447f" class="graf graf--p graf-after--figure">Until:</p><figure name="a3b6" id="a3b6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*P274xDA6xS50yQMzVZXDkA.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*P274xDA6xS50yQMzVZXDkA.png"></figure><p name="6775" id="6775" class="graf graf--p graf-after--figure">Return:</p><figure name="b384" id="b384" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dd36Qz38T-yYQln5grWGpQ.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*dd36Qz38T-yYQln5grWGpQ.png"></figure><ul class="postList"><li name="7297" id="7297" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Gradient Descent with Momentum</strong>: record the historical steps and apply an impact of it on the current step. There are two benefits of applying a momentum gradient descent: (1) it moves<strong class="markup--strong markup--li-strong"> faster</strong> than vanilla with the same learning rate (but the general consumed time is uncertain because it may take a longer distance); (2) it has a shot at <strong class="markup--strong markup--li-strong">escaping local minima</strong> because of the impacts of historical steps were added.</li></ul><figure name="290f" id="290f" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*OcXyoofLgfGmjAGMBbrCBg.png" data-width="2522" data-height="622" src="https://cdn-images-1.medium.com/max/800/1*OcXyoofLgfGmjAGMBbrCBg.png"></figure><p name="f563" id="f563" class="graf graf--p graf-after--figure">Repeat:</p><figure name="d6db" id="d6db" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zAIuhrbnLyjcdiBD-ojIkw.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*zAIuhrbnLyjcdiBD-ojIkw.png"></figure><p name="a623" id="a623" class="graf graf--p graf-after--figure">And then:</p><figure name="a4e9" id="a4e9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ibq6KVu8mrLKDe27XUv7pg.png" data-width="1342" data-height="72" src="https://cdn-images-1.medium.com/max/800/1*ibq6KVu8mrLKDe27XUv7pg.png"></figure><p name="9833" id="9833" class="graf graf--p graf-after--figure">Until:</p><figure name="24e3" id="24e3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*P274xDA6xS50yQMzVZXDkA.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*P274xDA6xS50yQMzVZXDkA.png"></figure><p name="d494" id="d494" class="graf graf--p graf-after--figure">Return:</p><figure name="4838" id="4838" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dd36Qz38T-yYQln5grWGpQ.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*dd36Qz38T-yYQln5grWGpQ.png"></figure><p name="6d67" id="6d67" class="graf graf--p graf-after--figure">where the momentum hyperparameter <em class="markup--em markup--p-em">γ</em> is a new hyperparameter and it is usually set to 0.9 or a similar value.</p><ul class="postList"><li name="16d2" id="16d2" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Adaptive Gradient</strong> (aka. <strong class="markup--strong markup--li-strong">AdaGrad</strong>): sometimes when we have a saddle plane, the momentum approach may not be our best method to choose. This is because it takes a detour towards the minima.</li></ul><figure name="023c" id="023c" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*r44ln629j4FWoCf899icig.png" data-width="1458" data-height="578" src="https://cdn-images-1.medium.com/max/800/1*r44ln629j4FWoCf899icig.png"><figcaption class="imageCaption">Figure: The Momentum (pink) takes a detour to minima than AdaGrad (white) on a saddle</figcaption></figure><p name="1f7b" id="1f7b" class="graf graf--p graf-after--figure">The AdaGrad method is quite simple, which basically allows us to <strong class="markup--strong markup--p-strong">scale</strong> the gradient (i.e. normalize the partial directive on each parameter) so that we can get a balanced value. This is done by dividing each partial directive of the parameters with its square root with history.</p><p name="5d50" id="5d50" class="graf graf--p graf-after--p">One problem of AdaGrad is that it is too slow because we scale the gradient to the standard magnitude. To deal with this problem, we have to choose either a larger learning rate η or we have to leverage more tricks to this approach.</p><p name="6054" id="6054" class="graf graf--p graf-after--p">Repeat:</p><figure name="800f" id="800f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*il1s0W1MaM91Yqo8KpGH6w.png" data-width="1344" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*il1s0W1MaM91Yqo8KpGH6w.png"><figcaption class="imageCaption">Note that history steps are added up because this is an accumulating process (ref: <code class="markup--code markup--figure-code">+=</code>)</figcaption></figure><p name="3fc4" id="3fc4" class="graf graf--p graf-after--figure">And then:</p><figure name="da24" id="da24" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tNrubRxpdzys7HfaO7Ogpg.png" data-width="1344" data-height="158" src="https://cdn-images-1.medium.com/max/800/1*tNrubRxpdzys7HfaO7Ogpg.png"><figcaption class="imageCaption">ϵ is an infinitesimal value, and it is added in the denominator for avoiding divided by zero problem</figcaption></figure><p name="4728" id="4728" class="graf graf--p graf-after--figure">Until:</p><figure name="df85" id="df85" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*P274xDA6xS50yQMzVZXDkA.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*P274xDA6xS50yQMzVZXDkA.png"></figure><p name="ec1f" id="ec1f" class="graf graf--p graf-after--figure">Return:</p><figure name="8871" id="8871" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dd36Qz38T-yYQln5grWGpQ.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*dd36Qz38T-yYQln5grWGpQ.png"></figure><ul class="postList"><li name="1b91" id="1b91" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Root Mean Square Propagation </strong>(aka. <strong class="markup--strong markup--li-strong">RMSProp</strong>): As we have discussed, the problem of AdaGrad is that this is too slow, and it is even slower when we have many steps! This is because the square sum of historical steps only increases and never shrinks.</li></ul><figure name="3795" id="3795" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*9986FOs0krQ2NvCh6sv6Lg.png" data-width="1544" data-height="606" src="https://cdn-images-1.medium.com/max/800/1*9986FOs0krQ2NvCh6sv6Lg.png"><figcaption class="imageCaption">With the same learning rate (1e-3), the RMSProp (green, ρ = 0.99) finds minima quicker than AdaGrad (white)</figcaption></figure><p name="6a2f" id="6a2f" class="graf graf--p graf-after--figure">The idea of RMSProp is that the current step is more important than the historical steps, so we can shrink the sum of historical step squares in each step by adding a hyperparameter decay rate <em class="markup--em markup--p-em">ρ </em>in [0, 1) (e.g. <code class="markup--code markup--p-code u-paddingRight0 u-marginRight0"><em class="markup--em markup--p-em">ρ = </em>0.9</code>). The historical sum of squares will be scaled smaller by <em class="markup--em markup--p-em">ρ</em> and the current square will be scaled larger by (<em class="markup--em markup--p-em">1-ρ</em>).</p><p name="3c77" id="3c77" class="graf graf--p graf-after--p">Repeat:</p><figure name="0e9a" id="0e9a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3x6mOA3tgerRNtpLkt-1qA.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*3x6mOA3tgerRNtpLkt-1qA.png"><figcaption class="imageCaption">ρ is the decay rate used to shrink the weight of historical steps</figcaption></figure><p name="3275" id="3275" class="graf graf--p graf-after--figure">And then:</p><figure name="4f55" id="4f55" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tNrubRxpdzys7HfaO7Ogpg.png" data-width="1344" data-height="158" src="https://cdn-images-1.medium.com/max/800/1*tNrubRxpdzys7HfaO7Ogpg.png"><figcaption class="imageCaption">ϵ is an infinitesimal value, and it is added in the denominator for avoiding divided by zero problem</figcaption></figure><p name="20e0" id="20e0" class="graf graf--p graf-after--figure">Until:</p><figure name="762d" id="762d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*P274xDA6xS50yQMzVZXDkA.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*P274xDA6xS50yQMzVZXDkA.png"></figure><p name="2bd9" id="2bd9" class="graf graf--p graf-after--figure">Return:</p><figure name="534b" id="534b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dd36Qz38T-yYQln5grWGpQ.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*dd36Qz38T-yYQln5grWGpQ.png"></figure><ul class="postList"><li name="c200" id="c200" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Adaptive Moment Estimation</strong> (aka. <strong class="markup--strong markup--li-strong">Adam</strong>): The Adam approach takes the advantage of both the Momentum and the RMSProp, which means it is not only rapid but shrinks directly without detour.</li></ul><figure name="1bbd" id="1bbd" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*EiTPDfKVymhaMjT1L9qfcQ.png" data-width="1420" data-height="614" src="https://cdn-images-1.medium.com/max/800/1*EiTPDfKVymhaMjT1L9qfcQ.png"><figcaption class="imageCaption">Adam (Blue) takes the advantage of Momentum (pink) and RMSProp (green)</figcaption></figure><p name="70a7" id="70a7" class="graf graf--p graf-after--figure">Adam introduces two hyperparameters of the decay rate, β_1, and β_2. β_1 is used to decay the historical values for the sum of the gradient, while β_2 is used to decay the historical values for the sum of the squared gradient.</p><p name="57aa" id="57aa" class="graf graf--p graf-after--p">Repeat:</p><figure name="2a4b" id="2a4b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ckBYzQKWTt4zbecNV_nqfg.png" data-width="1342" data-height="82" src="https://cdn-images-1.medium.com/max/800/1*ckBYzQKWTt4zbecNV_nqfg.png"></figure><p name="61c2" id="61c2" class="graf graf--p graf-after--figure">And:</p><figure name="a8a7" id="a8a7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8-8h8yZIecodhI7KFtNlyQ.png" data-width="1342" data-height="82" src="https://cdn-images-1.medium.com/max/800/1*8-8h8yZIecodhI7KFtNlyQ.png"></figure><p name="aa9e" id="aa9e" class="graf graf--p graf-after--figure">And then:</p><figure name="75f6" id="75f6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ZAzQ-7h93W8vonYUs7Cdow.png" data-width="1342" data-height="124" src="https://cdn-images-1.medium.com/max/800/1*ZAzQ-7h93W8vonYUs7Cdow.png"></figure><p name="7993" id="7993" class="graf graf--p graf-after--figure">Until:</p><figure name="31ff" id="31ff" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*P274xDA6xS50yQMzVZXDkA.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*P274xDA6xS50yQMzVZXDkA.png"></figure><p name="af24" id="af24" class="graf graf--p graf-after--figure">Return:</p><figure name="d1f1" id="d1f1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dd36Qz38T-yYQln5grWGpQ.png" data-width="1342" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*dd36Qz38T-yYQln5grWGpQ.png"></figure><p name="ee83" id="ee83" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">14. Common Gradient Formulas</strong></p><ul class="postList"><li name="e232" id="e232" class="graf graf--li graf-after--p">OLS Linear Regression</li></ul><figure name="2f7f" id="2f7f" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*hNLv6__JIkrcKahoAdNcuw.png" data-width="1342" data-height="142" src="https://cdn-images-1.medium.com/max/800/1*hNLv6__JIkrcKahoAdNcuw.png"></figure><ul class="postList"><li name="9b88" id="9b88" class="graf graf--li graf-after--figure">LASSO Regression (L1)</li></ul><figure name="01de" id="01de" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*gZSfL_fvsLGB8jIgD9a_Hg.png" data-width="1342" data-height="208" src="https://cdn-images-1.medium.com/max/800/1*gZSfL_fvsLGB8jIgD9a_Hg.png"></figure><ul class="postList"><li name="7185" id="7185" class="graf graf--li graf-after--figure">Ridge Regression (L2)</li></ul><figure name="57dd" id="57dd" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*Pwujel0ZpdUBJ-VJq_LcDg.png" data-width="1342" data-height="138" src="https://cdn-images-1.medium.com/max/800/1*Pwujel0ZpdUBJ-VJq_LcDg.png"></figure><ul class="postList"><li name="b420" id="b420" class="graf graf--li graf-after--figure">Logistic Regression</li></ul><figure name="9889" id="9889" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*fOKnxY2fS0v4DyXb84fGxg.png" data-width="1342" data-height="208" src="https://cdn-images-1.medium.com/max/800/1*fOKnxY2fS0v4DyXb84fGxg.png"></figure><ul class="postList"><li name="ccbd" id="ccbd" class="graf graf--li graf-after--figure">L1 Logistic Regression: we have to compute β_0 and β in different ways, which can be a little bit tricky.</li></ul><figure name="4f8a" id="4f8a" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*rvcDZSvmEi2mOUelrc7QTA.png" data-width="1342" data-height="122" src="https://cdn-images-1.medium.com/max/800/1*rvcDZSvmEi2mOUelrc7QTA.png"></figure><figure name="6a8f" id="6a8f" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*f3l9JU6GAE6Cu3j-XiYOBw.png" data-width="1342" data-height="108" src="https://cdn-images-1.medium.com/max/800/1*f3l9JU6GAE6Cu3j-XiYOBw.png"></figure><p name="7f9f" id="7f9f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">15. Other Frequently Asked Questions</strong></p><p name="0ebc" id="0ebc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Why L1 is more likely to shrink the parameter to 0 than L2?</strong></p><p name="a833" id="a833" class="graf graf--p graf-after--p">In mathematics, the parameter in L1 regularization can quickly converge to 0 for a large λ. However, the parameter in L2 regularization struggles to converge to 0 for large λ.</p><figure name="df58" id="df58" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DBlFTNeBBJVugUAT-FtiHA.png" data-width="1864" data-height="520" src="https://cdn-images-1.medium.com/max/800/1*DBlFTNeBBJVugUAT-FtiHA.png"></figure><p name="dc90" id="dc90" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Is L1 always better than L2 or vice versa?</strong></p><p name="ccc0" id="ccc0" class="graf graf--p graf-after--p">No. But if we currently don’t know anything else, we should choose L1 because it makes our model simple.</p><p name="d05d" id="d05d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Is it okay to get rid of L1 parameters that go to zero?</strong></p><p name="d5c7" id="d5c7" class="graf graf--p graf-after--p">Yes, because they don’t contribute to the regression or classification.</p><p name="355b" id="355b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Why the loss function for regression is a bowl?</strong></p><p name="401a" id="401a" class="graf graf--p graf-after--p">This is because of the definition of MSE, where the coefficients have squared values.</p><p name="cde1" id="cde1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Where to start gradient descent from a loss function?</strong></p><p name="2437" id="2437" class="graf graf--p graf-after--p">It doesn’t matter, and we just start from some random point.</p><p name="d694" id="d694" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Where to stop gradient descent?</strong></p><p name="d9a0" id="d9a0" class="graf graf--p graf-after--p">We stop iteration when the loss is smaller than some threshold approximately equals 0 (i.e. flat)(we can not let it become exact zero because we may never reach that point with a given learning rate).</p><p name="b93f" id="b93f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) How can we make sure that we find a global minimum?</strong></p><p name="d70a" id="d70a" class="graf graf--p graf-after--p">By momentum or deep learning.</p><p name="4400" id="4400" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Notice <em class="markup--em markup--p-em">β</em> accelerates and then slows down while iterating in gradient descent. Why?</strong></p><p name="e9a5" id="e9a5" class="graf graf--p graf-after--p graf--trailing">When β is far from the minimum point, the absolute slope (gradient) is large, which means it can move a larger step along the curve in an iteration. When β is close to the minimum point, the absolute slope (gradient) gets smaller and the step becomes smaller.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/2ea9f5aa1294"><time class="dt-published" datetime="2021-11-11T17:09:07.895Z">November 11, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/intro-to-machine-learning-2-linear-model-regularization-and-various-types-of-gradient-descents-2ea9f5aa1294" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>