<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>High-Performance Computer Architecture 33 | Many Core Challenges and Solutions</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">High-Performance Computer Architecture 33 | Many Core Challenges and Solutions</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: High-Performance Computer Architecture
</section>
<section data-field="body" class="e-content">
<section name="f90a" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1193" id="1193" class="graf graf--h3 graf--leading graf--title">High-Performance Computer Architecture 33 | <strong class="markup--strong markup--h3-strong">Many Core Challenges and Solutions</strong></h3><figure name="c5fc" id="c5fc" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*SCftP1-RBm4zFP8y.png" data-width="1446" data-height="864" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*SCftP1-RBm4zFP8y.png"></figure><ol class="postList"><li name="941b" id="941b" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Many Cores</strong></li></ol><p name="5b56" id="5b56" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Many-Core Challenges</strong></p><p name="30d9" id="30d9" class="graf graf--p graf-after--p">In this part, we are going to talk about implementing a multi-core system. When it comes to the situation that we have more than 1 core because we need to maintain coherence between more cores, the coherence traffic increases. Therefore, we will meet the following challenges,</p><ul class="postList"><li name="a1bd" id="a1bd" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Bus Bottleneck</strong>: When we have more coherence traffic on the bus, the bus only allows us to do one request at a time. Then the bus will become a bottleneck.</li><li name="e402" id="e402" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Off-Chip Traffic Bottleneck: </strong>when we have more cores, we also have more off-chip traffic (i.e. memory traffic). Therefore, we need to have more on-chip caches. Commonly if we have 4 cores, we have 4 L1 caches and 4 L2 caches. So that even if we have more cores, each core will have the same misses as the unicore. As a result, there will be higher memory traffic and the off-chip traffic now becomes a bottleneck.</li><li name="177c" id="177c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Over Large Coherence Directory</strong>: Because we have an on-chip network that is no longer a bus, we need directory coherence. But a traditional directory has one entry for each possible memory block, and the memory can be many gigabytes in size, which may result in billions of directory entries that will not fit on the chip.</li><li name="6ce5" id="6ce5" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Power Budget for Cores</strong>: as the number of cores goes up, the power we can spend in each core goes down. This will result in a lower frequency of the core and each core will be much slower.</li><li name="a2fd" id="a2fd" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">OS Confusion</strong>: we will have all of these brings to the OS, which has to deal with multithreading, with one core for several threads, with one chip supporting many cores. So the OS can actually supports many threads. However, if the threads we are executing just use the first core of the system and it can be improved if we splite these threads to other cores, how can the OS figure that out?</li></ul><p name="44fe" id="44fe" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Challenge 1: Bus bottleneck</strong></p><p name="0d9c" id="0d9c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Bus Vs. Network</strong></p><p name="5e8a" id="5e8a" class="graf graf--p graf-after--p">The simple idea is that all the cores will communicate the coherence information through a single <strong class="markup--strong markup--p-strong">shared bus</strong>, but we have discussed that there will be a bottleneck of the performance.</p><figure name="cdb2" id="cdb2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CU_Kacz51dzDtVWRO8oP4A.png" data-width="1650" data-height="282" src="https://cdn-images-1.medium.com/max/800/1*CU_Kacz51dzDtVWRO8oP4A.png"></figure><p name="3595" id="3595" class="graf graf--p graf-after--figure">A better idea is that we can use a network to communicate between different cores. This kind of on-chip network used to connect different cores is called a <strong class="markup--strong markup--p-strong">mesh</strong>. Suppose we have a 4-core system, and what we have is these cores are individually connected as follows,</p><figure name="7794" id="7794" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RetNFv7xKoMNLnmDJ9E8Kg.png" data-width="1270" data-height="282" src="https://cdn-images-1.medium.com/max/800/1*RetNFv7xKoMNLnmDJ9E8Kg.png"></figure><p name="1b9c" id="1b9c" class="graf graf--p graf-after--figure">So when we want to maintain coherence between core #0 and core #1, we can directly communicate with the individual link between them,</p><figure name="b994" id="b994" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8fXSY6zpUN6484Z9GeDjOQ.png" data-width="1270" data-height="282" src="https://cdn-images-1.medium.com/max/800/1*8fXSY6zpUN6484Z9GeDjOQ.png"></figure><p name="38ba" id="38ba" class="graf graf--p graf-after--figure">If we want to maintain coherence between core #0 and core #3, we can not send the message directly. But we can have two paths to send the message, via core #1 or via core #2,</p><figure name="f1d6" id="f1d6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mSIVs0Y2qhskXgOjDJZ3dw.png" data-width="1270" data-height="282" src="https://cdn-images-1.medium.com/max/800/1*mSIVs0Y2qhskXgOjDJZ3dw.png"></figure><p name="8386" id="8386" class="graf graf--p graf-after--figure">We can even have a larger mesh with 16 cores and we will have more individual links between the cores. Therefore, we are going to have a higher throughput compared with only one shared bus. For example,</p><figure name="9881" id="9881" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pZ2lOJ03Ky-EZnp4y4mpDQ.png" data-width="1270" data-height="572" src="https://cdn-images-1.medium.com/max/800/1*pZ2lOJ03Ky-EZnp4y4mpDQ.png"></figure><p name="9e45" id="9e45" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Mesh Network: An Example</strong></p><p name="c31d" id="c31d" class="graf graf--p graf-after--p">Now let’s see an example about the speedup of using a mesh. Suppose we have 4 cores, and each core sends an <strong class="markup--strong markup--p-strong">equal</strong> amount of messages. And the messages sent by each core are <strong class="markup--strong markup--p-strong">round-robin</strong>, which means that the messages from one core are equally send to other cores. Suppose each core sends 10 million messages per second and the bus and each link can support 20 million messages per second.</p><p name="2a51" id="2a51" class="graf graf--p graf-after--p">Then let’s start from core #0 in a mesh. Suppose core #0 is sending all the messages equally to the other three cores. Because core #1 and core #2 are neighborhoods for core #0, it will send all the information directly through the individual link between each pair of them, with the 1/3 amount of messages. However, for core #3, because we have two paths to send to this core, then each of the paths will send 1/6 amount of messages.</p><figure name="a7c1" id="a7c1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*TfLPPByvI2qmGCH5IMfHzQ.png" data-width="1270" data-height="456" src="https://cdn-images-1.medium.com/max/800/1*TfLPPByvI2qmGCH5IMfHzQ.png"></figure><p name="1a41" id="1a41" class="graf graf--p graf-after--figure">Then, the data transfers through the path sent by core #1 should be,</p><figure name="6be6" id="6be6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*w_bCeqZjySZDnU-vMBIHMQ.png" data-width="1270" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*w_bCeqZjySZDnU-vMBIHMQ.png"></figure><p name="9c15" id="9c15" class="graf graf--p graf-after--figure">Then let’s continue this process for the other three cores and finally, we can get the total data transferred through each link is,</p><figure name="7b7e" id="7b7e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-53uqFLRizDmqnNnlnTOrQ.png" data-width="1270" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*-53uqFLRizDmqnNnlnTOrQ.png"></figure><p name="9dda" id="9dda" class="graf graf--p graf-after--figure">We can multiple 4/3 with the amount of data send by each core per second and then we can know that there will be 13.3 million messages transfer on each link per second. This value is smaller than the bandwidth of each link, which is supposed to be 20 million messages per second. So the cores keep sending messages without waiting.</p><p name="6b80" id="6b80" class="graf graf--p graf-after--p">However, when we have a shared bus, we can only have 2 cores working at a time because of the bus bottleneck. Therefore, we can finally know that the speedup in this case of mesh to the bus is 2.</p><p name="2e3b" id="2e3b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Challenge 2: Off-Chip Traffic Bottleneck</strong></p><p name="4586" id="4586" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Shared Last Level Cache Problem</strong></p><p name="642c" id="642c" class="graf graf--p graf-after--p">Because we have a bottleneck of the off-chip traffic, we need to reduce the number of access requests to the memory. The simple way to achieve that is by sharing a big last level cache (LLC), which in most of today’s modern processors is the L3 cache so that all the cores go to that cache finally.</p><p name="fece" id="fece" class="graf graf--p graf-after--p">But there is a problem to have one huge LLC like this. Such a large cache would be <strong class="markup--strong markup--p-strong">slow</strong>, and if it is really one single cache, it will only have <strong class="markup--strong markup--p-strong">one entry point</strong> where we enter the address and expect the data out. Because this one entry point is responsible for all the cache misses from all the cores, it turns out to be a bottleneck.</p><p name="66fa" id="66fa" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Distributed LLC</strong></p><p name="1eb0" id="1eb0" class="graf graf--p graf-after--p">The solution to this problem is that we can have a distributed LLCs that can have more than one entry point. But the main idea is that all of these distributed LLCs will be logically one single cache, which means that there will be no duplicate blocks in them.</p><p name="92fc" id="92fc" class="graf graf--p graf-after--p">However, the distributed LLC is sliced up so that each tile gets part of this cache. So now, on the chip of each core, we will the core, the L1 cache, the L2 cache, and a slice of the L3 cache,</p><figure name="bd47" id="bd47" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Xq6aWsXzrQVngfzZoeJAQA.png" data-width="1424" data-height="550" src="https://cdn-images-1.medium.com/max/800/1*Xq6aWsXzrQVngfzZoeJAQA.png"></figure><p name="e09c" id="e09c" class="graf graf--p graf-after--figure">Now the problem is that how can we find the block we need because the L3 is distributed in each core? There are several solutions,</p><ul class="postList"><li name="a0ff" id="a0ff" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Round-robin by Cache Index</strong>: the simplest idea is that we can simply spread the data to all the cores by the index. This is called a <strong class="markup--strong markup--li-strong">round-robin method by cache index</strong>. However, this method is not good for the <strong class="markup--strong markup--li-strong">locality</strong>. This is because a core is just as likely to access something that is in the opposite corner of the chip as it is to access its own L3 cache slice.</li><li name="cdf4" id="cdf4" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Round-robin by Page Number</strong>: However, we can also spread the data by its page number. Then all the blocks that belong to the same page number will end up in the same slice. This helps because the OS can now map the pages to make accesses more local.</li></ul><p name="fca6" id="fca6" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">4. Challenge 3: Over Large Coherence Directory</strong></p><p name="342c" id="342c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Directory Slicing</strong></p><p name="b7b0" id="b7b0" class="graf graf--p graf-after--p">The directory can be too large because the directory has entries for every memory block. Because the directory is too large, the on-chip directory is also sliced and distributed to each core, and the directory is sliced the <strong class="markup--strong markup--p-strong">same as the LLC</strong>.</p><p name="b497" id="b497" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Partial Directory</strong></p><p name="50c6" id="50c6" class="graf graf--p graf-after--p">In fact, we can also realize that entries that are not on the last level cache or in any caches on chip will not be shared by any caches, so we don’t really have to maintain that information.</p><p name="a885" id="a885" class="graf graf--p graf-after--p">Therefore, we can maintain a directory that is called a <strong class="markup--strong markup--p-strong">partial directory </strong>that will not have every memory block. Instead, the partial directory will have a limited number of entries and then we allocate an entry in this limited directory only for such blocks that have at least one presence bit in one of the private caches.</p><p name="8921" id="8921" class="graf graf--p graf-after--p">When the partial directory runs out of entries, we can simply replace an existing entry possibly by LRU or some other policies.</p><p name="c36f" id="c36f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">5. Challenge 4: Power Budget for Cores</strong></p><p name="7ab6" id="7ab6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Performance of Power Budgets</strong></p><p name="d5c2" id="d5c2" class="graf graf--p graf-after--p">Even if our program supports parallel execution in some parts, it may not benefit from adding more cores because each core slows down. For example, suppose we have a program that,</p><ul class="postList"><li name="48b7" id="48b7" class="graf graf--li graf-after--p">20% time supports 1-core parallelism</li><li name="402b" id="402b" class="graf graf--li graf-after--li">30% time supports 2-core parallelism</li><li name="6a2d" id="6a2d" class="graf graf--li graf-after--li">40% time supports 3-core parallelism</li><li name="a5af" id="a5af" class="graf graf--li graf-after--li">10% time supports 4-core parallelism</li></ul><p name="a6d2" id="a6d2" class="graf graf--p graf-after--li">Let’s suppose we have 100 sec time of execution if we have a unicore system. Then, how long will we spend executing this program if we have 2 or 4 cores?</p><p name="03f8" id="03f8" class="graf graf--p graf-after--p">When we have two cores, the frequency will be 79% (i.e. <code class="markup--code markup--p-code">3√0.5 = 0.79</code>) of the unicore system. And if we have 4 cores, the frequency will be 63% (i.e. <code class="markup--code markup--p-code">3√0.25 = 0.63</code>) of the unicore system.</p><p name="08e5" id="08e5" class="graf graf--p graf-after--p">Regardless of the reduction of the frequency, the execution time we will have for a 2-core system will be 60 s (i.e. <code class="markup--code markup--p-code">[20% + 80%/2]*100 = 60</code>), and the execution time of a 4-core system will be 50.8 s (i.e. <code class="markup--code markup--p-code">[20% + 30%/2 + 40%/3 + 10%/4]*100 = 50.8</code>).</p><p name="c820" id="c820" class="graf graf--p graf-after--p">So finally, take the frequency into consideration, we will have the actual execution time for a 2-core system is 76 s (i.e. <code class="markup--code markup--p-code">60/0.79 = 76</code>), and the actual execution time for a 4-core system is 81 s (i.e. <code class="markup--code markup--p-code">50.8/0.63 = 81</code>).</p><p name="5aa0" id="5aa0" class="graf graf--p graf-after--p">Therefore, we can find out that even we can benefit from the parallelism of cores, the power reduction for the added cores can make the performance worse.</p><p name="8cb9" id="8cb9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Boost Frequency</strong></p><p name="d4ac" id="d4ac" class="graf graf--p graf-after--p">The idea of dealing with this power budget problem is quite simple. When we don’t need core parallelism, it can be a waste if we still support the unused cores. So basically, what are we going to do is that, when there is no parallelism available, we boost the frequency of one or two active cores so that they get the full power budget of the chip.</p><p name="dfcb" id="dfcb" class="graf graf--p graf-after--p">Examples of this solution are Intel’s Core i7–4702 MQ processor (for the laptop) released in Q2, 2013, and Intel’s Core i7–4771 processor released in Q3, 2013.</p><p name="1a16" id="1a16" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">6. Challenge 5: OS Confusion</strong></p><p name="baf7" id="baf7" class="graf graf--p graf-after--p graf--trailing">Actually, most of the modern OS’s today will figure out which core to run the thread to obtain the best performance, but this is beyond not scope of this series. What you have to keep in mind is that the OS implementations should also be aware and responsible for figure out which thread to put on which core in order to achieve the best performance.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/b8b3e94260b0"><time class="dt-published" datetime="2021-04-09T18:15:56.489Z">April 9, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/high-performance-computer-architecture-33-many-core-challenges-and-solutions-b8b3e94260b0" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>