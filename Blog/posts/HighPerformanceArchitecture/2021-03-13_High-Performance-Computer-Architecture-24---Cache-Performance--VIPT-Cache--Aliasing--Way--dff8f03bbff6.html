<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>High-Performance Computer Architecture 24 | Cache Performance, VIPT Cache, Aliasing, Way…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">High-Performance Computer Architecture 24 | Cache Performance, VIPT Cache, Aliasing, Way…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: High-Performance Computer Architecture
</section>
<section data-field="body" class="e-content">
<section name="5708" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="69b5" id="69b5" class="graf graf--h3 graf--leading graf--title">High-Performance Computer Architecture 24 | <strong class="markup--strong markup--h3-strong">Cache Performance, VIPT Cache, Aliasing, Way Prediction, Replacement Policies, Prefetching, Loop Interchange, and Cache Hierarchy</strong></h3><figure name="f28b" id="f28b" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*n_XJWLrV_m1W21LQ.png" data-width="1446" data-height="864" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*n_XJWLrV_m1W21LQ.png"></figure><ol class="postList"><li name="33f3" id="33f3" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Improving Cache Performance</strong></li></ol><p name="3b09" id="3b09" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Recall: Average Memory Access Time (AMAT)</strong></p><p name="e453" id="e453" class="graf graf--p graf-after--p">The average memory access time is defined as the access time to memory as seen by the processor. AMAT can be calculated by,</p><figure name="8ac3" id="8ac3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*kDGoE48R9lQqqfqX.png" data-width="1356" data-height="68" src="https://cdn-images-1.medium.com/max/800/0*kDGoE48R9lQqqfqX.png"></figure><p name="cb8d" id="cb8d" class="graf graf--p graf-after--figure">Based on this formula, we can know that, basically, we have three ways to reduce the AMAT and improve the performance,</p><ul class="postList"><li name="2911" id="2911" class="graf graf--li graf-after--p">reduce the <strong class="markup--strong markup--li-strong">hit time</strong></li><li name="f741" id="f741" class="graf graf--li graf-after--li">reduce the <strong class="markup--strong markup--li-strong">miss rate</strong></li><li name="5345" id="5345" class="graf graf--li graf-after--li">reduce the <strong class="markup--strong markup--li-strong">miss penalty</strong></li></ul><p name="01ee" id="01ee" class="graf graf--p graf-after--li">Now, let’s discuss each of them.</p><p name="94ee" id="94ee" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Reduce Hit Time</strong></p><p name="b743" id="b743" class="graf graf--p graf-after--p">Some of the methods are pretty obvious like,</p><ul class="postList"><li name="9b70" id="9b70" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Reduce cache size</strong>: but this is bad for the miss rate, so the overall AMAT may not be improved.</li><li name="6897" id="6897" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Reduce cache associativity</strong>: this is also bad for the miss rate. We are going to have more conflicts and start kicking each other out even though they will co-exist peacefully in a more associative cache.</li></ul><p name="f867" id="f867" class="graf graf--p graf-after--li">There are some less simple methods that we are going to discuss,</p><ul class="postList"><li name="9960" id="9960" class="graf graf--li graf-after--p">Overlapping cache hit with another hit</li><li name="9f06" id="9f06" class="graf graf--li graf-after--li">Overlapping cache hit with TLB hit</li><li name="a834" id="a834" class="graf graf--li graf-after--li">Optimizing the lookup for the common case</li><li name="73cf" id="73cf" class="graf graf--li graf-after--li">Maintaining replacement state more quickly</li></ul><p name="1a1a" id="1a1a" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Improving Cache Performance with Hit Time</strong></p><p name="5b36" id="5b36" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Methods for Reducing Hit Time #1: Pipelined Caches</strong></p><p name="d48b" id="d48b" class="graf graf--p graf-after--p">One way of speeding up the hit times is to overlap one hit with another hit, and this can be implemented by <strong class="markup--strong markup--p-strong">pipelining caches</strong>.</p><p name="68d2" id="68d2" class="graf graf--p graf-after--p">If the cache takes multiple cycles to be accessed, we can have a situation where,</p><ul class="postList"><li name="5bed" id="5bed" class="graf graf--li graf-after--p">Access 1 comes in cycle N, and it is a cache hit</li><li name="5de1" id="5de1" class="graf graf--li graf-after--li">Access 2 comes in cycle N+1, and it is a cache hit</li></ul><p name="fc57" id="fc57" class="graf graf--p graf-after--li">Because the caches are not pipelined, the second access has to wait until the first access is done using the cache (it takes multiple cycles to finish). Thus, the overall hit time can be expressed by,</p><figure name="7a7a" id="7a7a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9ZxcD1knrZgy5ZA2ilZ5Mg.png" data-width="1356" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*9ZxcD1knrZgy5ZA2ilZ5Mg.png"></figure><p name="b83f" id="b83f" class="graf graf--p graf-after--figure">When we make our cache a pipelined cache, it is obvious that we can reduce the wait time and therefore, reduce the overall hit time. But how can we divide the cache access to different stages of the pipeline? Recall the steps that we use to find the data from the cache. A proportion of the block number will be retrieved and compared with the values of the tags. Then the valid bit will be checked and combined with the result of the tag comparison to decide whether we have a <strong class="markup--strong markup--p-strong">cache hit</strong>. If we have a cache hit, the cache block will be retrieved and calculated with the block offset for getting the data we want.</p><figure name="e8e1" id="e8e1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_aVHMQNVbw1bix7c_ZXQvQ.png" data-width="2056" data-height="604" src="https://cdn-images-1.medium.com/max/800/1*_aVHMQNVbw1bix7c_ZXQvQ.png"></figure><p name="9513" id="9513" class="graf graf--p graf-after--figure">So an example of the pipeline stages are,</p><ul class="postList"><li name="1ad8" id="1ad8" class="graf graf--li graf-after--p">Stage 1. reading out the tags from the cache array</li><li name="60b2" id="60b2" class="graf graf--li graf-after--li">Stage 2. determining the hit and beginning data read from the cache block</li><li name="0d90" id="0d90" class="graf graf--li graf-after--li">Stage 3. finishing data read and getting the data we need</li></ul><p name="ae78" id="ae78" class="graf graf--p graf-after--li">Usually, the actual cache ht time for L1 caches will be 1, 2, or 3 cycles, and 1-cycle L1 cache doesn’t need pipelining, while 2- and 3-cycle L1 caches can be relatively easily pipelined into two or three stages. So L1 caches will always be pipelined.</p><p name="9f20" id="9f20" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) TLB and Cache Hit Implementation</strong></p><p name="0bd7" id="0bd7" class="graf graf--p graf-after--p">Hit time is also affected by having to access the TLB before we access the cache. This is because we always have to do the <strong class="markup--strong markup--p-strong">virtual to memory translation</strong> before we can have an access to the cache. You can refer to the following diagram to see how it works,</p><figure name="d3ab" id="d3ab" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1LZlsdMTms2XLKRFBmzqNg.png" data-width="2090" data-height="1276" src="https://cdn-images-1.medium.com/max/800/1*1LZlsdMTms2XLKRFBmzqNg.png"></figure><p name="7efe" id="7efe" class="graf graf--p graf-after--figure">By this diagram, we can find out that if the TLB takes 1 cycle and the cache takes 1 cycle, we will need 2 cycles from when the processor gives us the virtual address (VA) until we actually get the data.</p><p name="f82e" id="f82e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) The Definition of Physical Cache</strong></p><p name="6a0e" id="6a0e" class="graf graf--p graf-after--p">As we have discussed, a cache that can only be accessed using the physical address is called a <strong class="markup--strong markup--p-strong">physical cache </strong>(aka.<strong class="markup--strong markup--p-strong"> physically accessed cache</strong>, <strong class="markup--strong markup--p-strong">physical-indexed physically-tagged cache</strong>, <strong class="markup--strong markup--p-strong">PIPT cache</strong>). The overall hit latency of a PIPT cache is the TLB hit latency plus the cache hit latency,</p><figure name="269b" id="269b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*7c69tx7fgS2YT1pK03Z7fA.png" data-width="1370" data-height="88" src="https://cdn-images-1.medium.com/max/800/1*7c69tx7fgS2YT1pK03Z7fA.png"></figure><p name="1e02" id="1e02" class="graf graf--p graf-after--figure">In the diagram above, we can find out that both the set and the tags are compared against the bits from the physical memory, then this is why it is called a physical-indexed physical-tagged cache.</p><p name="e5b4" id="e5b4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) The Definition of Virtually Accessed Cache</strong></p><p name="20b7" id="20b7" class="graf graf--p graf-after--p">We can improve the overall hit latency by a <strong class="markup--strong markup--p-strong">virtually accessed </strong>(aka. <strong class="markup--strong markup--p-strong">VIVT</strong> or <strong class="markup--strong markup--p-strong">virtually-indexed virtually-tagged</strong>)<strong class="markup--strong markup--p-strong"> cache</strong>. In that case, the virtual address is what we use to access the cache and get the data.</p><p name="803e" id="803e" class="graf graf--p graf-after--p">If we have a <strong class="markup--strong markup--p-strong">cache miss</strong>, then we would use the virtual address to index the TLB for telling us what the physical address is, so that we can bring the data back to the cache when we get it from the physical memory.</p><figure name="747f" id="747f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*TJTT3CrSrdc8E4_c20bXOQ.png" data-width="2064" data-height="1208" src="https://cdn-images-1.medium.com/max/800/1*TJTT3CrSrdc8E4_c20bXOQ.png"></figure><p name="cd1d" id="cd1d" class="graf graf--p graf-after--figure">But for a <strong class="markup--strong markup--p-strong">cache hit</strong>, we don’t need to go to the TLB and do virtual to physical translation. The advantage of this virtually accessed cache is that the overall hit time will only be the cache hit time without the TLB latency,</p><figure name="af07" id="af07" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MFPYs6whpOKfFhb5A1icBg.png" data-width="1306" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*MFPYs6whpOKfFhb5A1icBg.png"></figure><p name="28c0" id="28c0" class="graf graf--p graf-after--figure">It seems that the virtually accessed cache can surpass the PIPT cache in every aspect. First of all, the latency of the virtually accessed cache is lower because it doesn’t need to wait for the TLB access. Also, the energy will be reduced because it takes fewer operations.</p><p name="6fed" id="6fed" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Problems For Virtually Accessed Cache (VIVT Cache)</strong></p><p name="9c8a" id="9c8a" class="graf graf--p graf-after--p">So far, the virtually accessed cache seems good for us. But there are some problems.</p><p name="1144" id="1144" class="graf graf--p graf-after--p">Firstly, the <strong class="markup--strong markup--p-strong">page permission</strong> information will be stored in the TLB and that means if we don’t access the TLB, we will not know whether or not we have the permission of accessing a page. So it will be a must for us to access the TLB before we access the cache.</p><p name="367c" id="367c" class="graf graf--p graf-after--p">Secondly, a more serious problem is that different applications have different virtual memory addresses for the same data. This means that every time we conduct a context switch between the processes, the content in the cache can not be used anymore. Thus, we have to do a <strong class="markup--strong markup--p-strong">cache flush</strong> for every context switch. Although the cache flush can be quick, it will cause more cache misses and this adds to the overall cost.</p><p name="6388" id="6388" class="graf graf--p graf-after--p">That’s why we design a <strong class="markup--strong markup--p-strong">virtually-indexed physically-tagged (VIPT)</strong> cache.</p><p name="b810" id="b810" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Aliasing For Virtually Accessed Cache</strong></p><p name="55ff" id="55ff" class="graf graf--p graf-after--p">There is another serious problem for the VIVP called <strong class="markup--strong markup--p-strong">aliasing</strong>. Let’s see an example about aliasing. Suppose we have two addresses,</p><pre name="09fe" id="09fe" class="graf graf--pre graf-after--p">A: 0x 1234 5000<br>B: 0x ABCD E000</pre><p name="5215" id="5215" class="graf graf--p graf-after--pre">and we also have a 64 KB directed-mapped cache with 16-byte block size, and this cache is virtually accessed. Because the block size is 16 B, then we have a 4-bit offset. Also, because we have 2¹² entries in the cache (i.e. 64KB/16B = 2¹²), we have 12 bits for the index. The rest of the virtual address will then be the tags. So,</p><pre name="8a1c" id="8a1c" class="graf graf--pre graf-after--p">Addr               Tags        Index     Offset<br>---------------    --------    ------    -------<br>A: 0x 1234 5000    0x 1234     0x500     0x0<br>B: 0x ABCD E000    0x ABCD     0xE00     0x0</pre><p name="d1bb" id="d1bb" class="graf graf--p graf-after--pre">Let’s suppose that we have a write-back cache (means that the cache will not write to the memory immediately) and suppose that both the address A and B will be mapped to the same physical address. This can be legal for most operating systems if we use something like,</p><pre name="11fe" id="11fe" class="graf graf--pre graf-after--p">mmap(A, 4096, fd, 0);<br>mmap(B, 4096, fd, 0);</pre><figure name="f64c" id="f64c" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*4iOwkp8cxk9No-cqL_JHQA.png" data-width="1500" data-height="664" src="https://cdn-images-1.medium.com/max/800/1*4iOwkp8cxk9No-cqL_JHQA.png"></figure><p name="463b" id="463b" class="graf graf--p graf-after--figure">Assume that both A and B are not in the cache, then if we execute the following program,</p><pre name="1197" id="1197" class="graf graf--pre graf-after--p">WR A, 16<br>RD B</pre><p name="31ec" id="31ec" class="graf graf--p graf-after--pre">Because A is not in the cache, we will get a cache miss. Then we will translate A and grab its value from the memory and change the value in the cache from the origin (let’s say, 4) to 16.</p><p name="4508" id="4508" class="graf graf--p graf-after--p">Because B is also not in the cache, we will then translate B and then grab its value. However, since we haven’t written A to the memory, what we read from the memory is the original value 4 instead of the value 16.</p><figure name="c680" id="c680" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PYuZnkTceYBPtT7yvAEwwA.png" data-width="1500" data-height="268" src="https://cdn-images-1.medium.com/max/800/1*PYuZnkTceYBPtT7yvAEwwA.png"></figure><p name="3277" id="3277" class="graf graf--p graf-after--figure">The reason for this problem is that the caches are tagged by the virtual addresses and they can be different even for the same physical address. So VIVT caches need additional support for this problem. Any time we write to the virtual location, we have to do a check in case of an alias in the cache. However, this can be quite expensive to do, and it kind of defeats the purpose of virtually accessed caches.</p><p name="ce28" id="ce28" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Methods for Reducing Hit Time #2: Virtually-Indexed Physically-Tagged (VIPT) Caches</strong></p><p name="650a" id="650a" class="graf graf--p graf-after--p">So the main problem for the PIPT is that we must wait for the TLB hit, and the main problem for the VIVT is that we do not want to keep the cache for all the different applications, so there won’t be any cache flushes.</p><p name="3499" id="3499" class="graf graf--p graf-after--p">The VIPT is actually a combination of the advantages of the PIPT and the VIPT. We use the index bits from the virtual address to find the set we want in the cache, and then we read the valid bits and the tags from it. Meanwhile, we also use the page number to access the TLB and then get the frame number and translate the physical address. Now because we have the physical address, we perform the tag check using the physical address.</p><figure name="d214" id="d214" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pm492fzarbfMCtbYGxVxFA.png" data-width="2090" data-height="1276" src="https://cdn-images-1.medium.com/max/800/1*pm492fzarbfMCtbYGxVxFA.png"></figure><p name="34b0" id="34b0" class="graf graf--p graf-after--figure">Note that the cache array and the TLB access can proceed parallelly, and because the TLB is fast enough because it is very small, the hit time will be equal to the cache hit time.</p><figure name="790a" id="790a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VIT3MwXWk0E90jLfCbD1dA.png" data-width="1306" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*VIT3MwXWk0E90jLfCbD1dA.png"></figure><p name="709d" id="709d" class="graf graf--p graf-after--figure">And because TLB hit latency &lt; Cache hit latency, we have,</p><figure name="ba8a" id="ba8a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6StD5ygBOPBO-rW0zgIqFA.png" data-width="1306" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*6StD5ygBOPBO-rW0zgIqFA.png"></figure><p name="3ca1" id="3ca1" class="graf graf--p graf-after--figure">Also, it turns out that we don’t need to flush when we have a context switch. This is because the caches are checked by the physical address, not the virtual one. So even though the virtual address in another process may map to the same set, its page number may not match the tags.</p><p name="4435" id="4435" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) VIPT Aliasing</strong></p><p name="22ed" id="22ed" class="graf graf--p graf-after--p">The VIPT will not have an aliasing problem because the virtual pages that will have different page numbers that map to the same physical frame number can have the same page offsets. If the index is only determined by the page offsets, then the index will not change during the virtual to physical translation.</p><p name="8610" id="8610" class="graf graf--p graf-after--p">So if all the index bits come from the page offset, we will not have any aliasing problems. To implement this, the <strong class="markup--strong markup--p-strong">caches have to be small</strong>.</p><p name="753e" id="753e" class="graf graf--p graf-after--p">For example, if we have a 4KB page, we will have a 12-bit page offset. If the cache has a 32 B block, then it has a 5-bit block offset. If now we want the index also fit in the page offset, it can only have 7 bits and this means that we can have only up to 128 sets in the cache. You can see that because we want to avoid the aliasing problem in the VIPT cache, we have to limit the size of our cache.</p><figure name="7273" id="7273" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*D8w08iaJ4ZjmQR029vf4ZQ.png" data-width="1608" data-height="618" src="https://cdn-images-1.medium.com/max/800/1*D8w08iaJ4ZjmQR029vf4ZQ.png"></figure><p name="8631" id="8631" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(9) VIPT Cache Size</strong></p><p name="230c" id="230c" class="graf graf--p graf-after--p">Now, we have discussed the maximum index bits we can have to avoid an aliasing problem, then let’s discuss the maximum size of a VIPT cache. Let’s suppose that we have a 4-way set-associative cache with 16 bytes for each block. Assume that the page size is 8KB. Calculate the maximum VIPT cache size if we want to avoid aliasing.</p><p name="30d8" id="30d8" class="graf graf--p graf-after--p">Because we have a page size of 8KB, then the page offset must be 13 bits (i.e. <code class="markup--code markup--p-code">log2(8*1024) = 13</code>). Because we have a block of 16B, the block offset should be 4 bits (i.e. <code class="markup--code markup--p-code">log2(16) = 4</code>). The all the rest of the bits in the page offsets can then be used as the index, so we have 9 bits for the index and thus we can create 512 sets (i.e. <code class="markup--code markup--p-code">2⁹ = 512</code>). Note that we because have a 4-way cache, which means that for each of the sets, we can have 4 blocks. So the overall size of the cache will be 32 KB (i.e. <code class="markup--code markup--p-code">512*16*4 B = 32 KB</code>).</p><p name="9607" id="9607" class="graf graf--p graf-after--p">In a more general case, we can discover that the maximum number of sets we can create times the block will always equal the page size. This means that we can simplify our results. Regardless of the block size, the maximum cache size we can have will be the<strong class="markup--strong markup--p-strong"> page size</strong> times the # of blocks in each set (i.e. the <strong class="markup--strong markup--p-strong">associativity</strong> of the cache). This is to say that,</p><figure name="962f" id="962f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HO__ciIrXN2OHFb3h8NsZg.png" data-width="1416" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*HO__ciIrXN2OHFb3h8NsZg.png"></figure><p name="12b1" id="12b1" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(10) Real VIPT Caches</strong></p><p name="2dd8" id="2dd8" class="graf graf--p graf-after--p">Now, let’s look at the sizes of some actual VIPT caches.</p><p name="cdeb" id="cdeb" class="graf graf--p graf-after--p">For Pentium 4 processor, it has a 4-way set-associative cache with a page size of 4 KB. So the L1 cache is designed to be 16 KB.</p><p name="a163" id="a163" class="graf graf--p graf-after--p">For a more recent Core 2, Nehalem, Sandy Bridge, or Haswell processors, they have an 8-way set-associative cache with a page size of 4 KB. So the L1 caches for these processors are designed to be 32 KB.</p><p name="8d22" id="8d22" class="graf graf--p graf-after--p">The upcoming Skylake designed processor from Intel has a 16-way set-associative cache with a page size of 4 KB. So its L1 cache is designed to be 64 KB.</p><p name="1d8c" id="1d8c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(11) Relationship between Associativity and Hit Time</strong></p><p name="636c" id="636c" class="graf graf--p graf-after--p">If we have a <strong class="markup--strong markup--p-strong">high associativity cache</strong>, we tend to have fewer conflicts, which leads to,</p><ul class="postList"><li name="1f5a" id="1f5a" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">a reduction of the overall miss rate</strong>: this will benefit the performance</li><li name="ab5e" id="ab5e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">a larger VIPT cache size</strong>: this is explained above and it will also benefit the performance and leads to a lower miss rate</li></ul><p name="90a6" id="90a6" class="graf graf--p graf-after--li">Also, because we have to search in the set for the cache hit, it will also lead to a<strong class="markup--strong markup--p-strong"> higher execution time</strong>. This is slow and <strong class="markup--strong markup--p-strong">bad</strong> for the performance! However, because the simple direct-mapped cache has only one place to check for a single block, the execution time will be lower, but the direct-mapped cache sacrifices the miss rate because of the conflicts.</p><p name="bdd0" id="bdd0" class="graf graf--p graf-after--p">For reducing the execution time of a high associativity cache, we have to make some cheating on the associativity.</p><p name="b80a" id="b80a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(12) Cheating #1: Way Prediction</strong></p><p name="c1f0" id="c1f0" class="graf graf--p graf-after--p">One way of cheating on associativity is called the <strong class="markup--strong markup--p-strong">way prediction</strong>. The basic idea for the way prediction is that we can guess which line in the set is the most likely to hit and just check that one.</p><p name="0109" id="0109" class="graf graf--p graf-after--p">If we are right, then the hit time will be reduced to the same as a direct-mapped cache.</p><p name="b8d6" id="b8d6" class="graf graf--p graf-after--p">If we are wrong, then we will get no hits there in the line we guess. Then we will do a normal set-associative check on the other lines, which will have a high hit time.</p><p name="c5da" id="c5da" class="graf graf--p graf-after--p">So basically, for a way prediction method, we will first guess the hit line in the set, so it will be more like a direct-mapped cache. And if we make a wrong guess, we will then do what the normal set-associative caches do.</p><p name="5ec8" id="5ec8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(13) Way Prediction Performance</strong></p><p name="5b4b" id="5b4b" class="graf graf--p graf-after--p">Now, let’s see an example of how does the way prediction improves performance. Suppose we have the following three caches.</p><p name="039c" id="039c" class="graf graf--p graf-after--p">The first one is a 32-KB 8-way set-associative cache with,</p><ul class="postList"><li name="7029" id="7029" class="graf graf--li graf-after--p">a hit rate of 90%</li><li name="4e0f" id="4e0f" class="graf graf--li graf-after--li">a hit latency of 2</li><li name="1999" id="1999" class="graf graf--li graf-after--li">a miss penalty of 20</li></ul><p name="5d76" id="5d76" class="graf graf--p graf-after--li">The second one is a 4KB direct-mapped cache with,</p><ul class="postList"><li name="4cab" id="4cab" class="graf graf--li graf-after--p">a hit rate of 70% (because it has a higher miss rate)</li><li name="15d3" id="15d3" class="graf graf--li graf-after--li">a hit latency of 1 (direct-mapped can be faster)</li><li name="f554" id="f554" class="graf graf--li graf-after--li">a miss penalty of 20</li></ul><p name="4a44" id="4a44" class="graf graf--p graf-after--li">The third one is a 32-KB 8-way set-associative cache with,</p><ul class="postList"><li name="ccf6" id="ccf6" class="graf graf--li graf-after--p">way prediction</li><li name="ea33" id="ea33" class="graf graf--li graf-after--li">a hit rate of 90%</li><li name="8b55" id="8b55" class="graf graf--li graf-after--li">a hit latency of 1 (when guessing right) or 2 (when guessing wrong)</li><li name="78a9" id="78a9" class="graf graf--li graf-after--li">a miss penalty of 20</li></ul><p name="d2c4" id="d2c4" class="graf graf--p graf-after--li">Let’s now calculate the AMAT for these three caches.</p><p name="63ca" id="63ca" class="graf graf--p graf-after--p">The first one can be calculated by <code class="markup--code markup--p-code">2 + (1 — 90%)*20 = 4</code>. The second one can be calculated by <code class="markup--code markup--p-code">1 + (1 — 70%)*20 = 7</code>. Compare these two results, we can find out that even though the direct-mapped cache has a smaller hit time, it can pay higher AMAT because of the reduction of the hit rate.</p><p name="a11e" id="a11e" class="graf graf--p graf-after--p">For the third one, firstly, we will first treat this cache as a direct-mapped cache so that we can have a 70% hit rate with 1 cycle latency each. For those who are missed in the direct-mapped cache, we will then use a normal set-associative cache. In this turn, all the rest 30% pay a 2-cycle latency some with them, 10% pays an extra 20-cycle latency because of the cache miss. Thus, the overall AMAT, in this case, would be,</p><pre name="467f" id="467f" class="graf graf--pre graf-after--p">AMAT = 70%*1 + 30%*2 + 10%*20 = 3.3</pre><p name="ba8d" id="ba8d" class="graf graf--p graf-after--pre">We can see that the performance improves compared with the set-associative cache without way prediction.</p><p name="e8b4" id="e8b4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(14) Relationship between Replacement Policy and Hit Time</strong></p><p name="5355" id="5355" class="graf graf--p graf-after--p">Recall that the replacement policy is the policy for deciding which line to kick out in a set.</p><p name="415f" id="415f" class="graf graf--p graf-after--p">A simple replacement policy (e.g. random policy) has nothing to update on a cache hit. When we have a cache miss, we start to implement our policy by randomly selecting a block in the set to kick out. This policy is <strong class="markup--strong markup--p-strong">good for the hit time</strong> because we don’t have to do extra checks. However, we pay a <strong class="markup--strong markup--p-strong">higher miss rate</strong> because we often kick out blocks that we will frequently use.</p><p name="cc56" id="cc56" class="graf graf--p graf-after--p">An LRU policy can result in a <strong class="markup--strong markup--p-strong">lower miss rate</strong>, but it has to updates lots of counters on every cache hit. So there’s a lot of activities on every cache hit that results from needing the state that will we use later. As a result, we will have to spend lots of power and we will also have <strong class="markup--strong markup--p-strong">slower hits</strong>.</p><p name="8b16" id="8b16" class="graf graf--p graf-after--p">So generally, what we want is to have a replacement policy that has a miss rate very close to LRU, but we want to do less activity on cache hits.</p><p name="58e9" id="58e9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(15) Cheating #2: Not Most Recently Used (NMRU) Policy</strong></p><p name="2250" id="2250" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">NMRU</strong> policy is an approximation of the LRU policy but it will have a better performance. The NMRU is implemented by recording the most recently used (MRU) line by a counter and <strong class="markup--strong markup--p-strong">randomly</strong> kicks out one of the NMRU lines.</p><p name="18e6" id="18e6" class="graf graf--p graf-after--p">If we have a 4-way set-associative cache, there will be 4 blocks in a set. For the LRU policy, we have to have 4 2-bit LRU counters to trace the LRU. However, we only need a single 2-bit counter if we implement the NMRU policy because we only need to record the MRU block. Generally, the NMRU keeps N times less than the LRU counters.</p><p name="71fa" id="71fa" class="graf graf--p graf-after--p">Although this NMRU policy does have a hit rate that is slightly lower than the LRU, the hit time can always be reduced significantly compared with the cost of the hit rate. NMRU is not perfect because it doesn’t track the rest of the blocks except for the MRU and it will conduct a random kick-out.</p><p name="4095" id="4095" class="graf graf--p graf-after--p">What we want is actually a policy that is still simpler than the LRU but keeps more track of what’s less recently than the MRU.</p><p name="7c1a" id="7c1a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(16) Cheating #3: Pseudo LRU (PLRU) Policy</strong></p><p name="d8a1" id="d8a1" class="graf graf--p graf-after--p">The PLRU is a policy that tries to approximate LRU in some way but it doesn’t exactly match the LRU. Instead of keeping the sequence of the usage frequency, the PLRU adds a bit for each block in the set to show that if it is recently used.</p><p name="86a5" id="86a5" class="graf graf--p graf-after--p">In the beginning, all these extra bits will be set to 0. When a block is recently used, this bit of it will be set to 1. After several times, we can find out that some of the blocks will have their bits set to 1 (means that they are recently used) and the other will have their bits remain 0. When now have to kick a block out, we only randomly choose from the lines that have their bit remains 0.</p><p name="c32d" id="c32d" class="graf graf--p graf-after--p">There is a situation when all the bits of the lines are set to 1. At this moment, we can not replace any of the lines in this set. So when we set the last 0 bit to 1, we will <strong class="markup--strong markup--p-strong">zero out the remaining bits</strong>. When we have only 1 bit set to 1 in a set, it is more like an NMRU policy. However, if we have more bits set to 1, this policy will be better than the NMRU policy because we know more than one MRUs.</p><p name="e8ef" id="e8ef" class="graf graf--p graf-after--p">Thus, this policy behaves somewhere in between the LRU policy and the NMRU policy.</p><p name="5f0c" id="5f0c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Improving Cache Performance with Miss Rate</strong></p><p name="9045" id="9045" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Causes of the Misses: 3Cs</strong></p><p name="5bff" id="5bff" class="graf graf--p graf-after--p">Now, we will focus on how to reduce the miss rate. Generally, there are 3 reasons for causing a cache miss,</p><ul class="postList"><li name="6f5c" id="6f5c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Compulsory Miss</strong>: when a block is accessed for the first time, and this will happen even for an infinite cache</li><li name="9c7b" id="9c7b" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Capacity Miss</strong>: blocks evicted when the cache is full. This is because we have a limited cache size.</li><li name="fd28" id="fd28" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Conflict Miss</strong>: blocks are evicted due to associativity. This is because we have limited associativity.</li></ul><p name="7e83" id="7e83" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Methods for Reducing Miss Rate #1: Larger Cache Blocks</strong></p><p name="5bc7" id="5bc7" class="graf graf--p graf-after--p">The first technique to reducing the miss rate is using larger cache blocks. This can reduce the miss rate because more words will are brought in when we have a miss, so the subsequential accesses on the words will not be misses. So this does reduce the miss rate only when the<strong class="markup--strong markup--p-strong"> spatial locality</strong> is good. When the spatial locality is poor, the miss rate will increase because we bring in more words we don’t need.</p><p name="a451" id="a451" class="graf graf--p graf-after--p">So when we increase the block size, the miss rate will first reduce because it benefits from the spatial locality. But as we exhaust the amount of spatial locality, the miss rate will increase.</p><figure name="de15" id="de15" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-Y0ByLXWIPDxF8CQiPi9QA.png" data-width="1954" data-height="572" src="https://cdn-images-1.medium.com/max/800/1*-Y0ByLXWIPDxF8CQiPi9QA.png"></figure><p name="4ee7" id="4ee7" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Methods for Reducing Miss Rate #2: Prefetching</strong></p><p name="2870" id="2870" class="graf graf--p graf-after--p">Another idea that uses a similar idea of the way prediction is called <strong class="markup--strong markup--p-strong">prefetching</strong>. The idea of prefetching is that we guess which block will be accessed soon in the future before they are actually accessed, and then bring those blocks into the cache before they are actually accessed. If our guess is right, when we request something from the memory for the first time, instead of a cache miss, we will have a cache hit. So there will be no need to wait for memory latency.</p><figure name="8d73" id="8d73" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*eNaC1OxnxXd6Kv9zaRY8wQ.png" data-width="1364" data-height="392" src="https://cdn-images-1.medium.com/max/800/1*eNaC1OxnxXd6Kv9zaRY8wQ.png"></figure><p name="c3ee" id="c3ee" class="graf graf--p graf-after--figure">However, if our guess is wrong, we will then have <strong class="markup--strong markup--p-strong">cache pollution</strong> because we are bringing stuff that is useless into the cache. Another problem is that we not only don’t eliminate the cache miss, we may also kick out something that can be useful in the cache.</p><p name="5ec1" id="5ec1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Prefetching Instructions Implementation</strong></p><p name="bb61" id="bb61" class="graf graf--p graf-after--p">The way to implement prefetching is to just add prefetching instructions, and let the compiler and the programmer figure out when to request prefetching. Let’s suppose we have the following program,</p><pre name="da36" id="da36" class="graf graf--pre graf-after--p">for (i=0; i&lt;1000000; i++) {<br>    sum += a[i];<br>}</pre><p name="4be8" id="4be8" class="graf graf--p graf-after--pre">This program will access the elements in the array one at a time and then sum them up. So there will be a lot of spatial localities and we can improve the miss rate by prefetching. With prefetching instructions, the program will be something like this,</p><pre name="50bc" id="50bc" class="graf graf--pre graf-after--p">for (i=0; i&lt;1000000; i++) {<br>    prefetch a[i+pdist];<br>    sum += a[i];<br>}</pre><p name="e5c5" id="e5c5" class="graf graf--p graf-after--pre">Where <code class="markup--code markup--p-code">pdist</code> here is a constant and it should be at least 1. Now our question is how to choose the value of this <code class="markup--code markup--p-code">pdist</code>?</p><ul class="postList"><li name="b82c" id="b82c" class="graf graf--li graf-after--p">If <code class="markup--code markup--li-code">pdist</code> is too small (e.g. 1), we may still have to wait for it because the memory access can not be that fast. This is a problem when the prefetch is too late.</li></ul><figure name="5c9f" id="5c9f" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*hBUlJ42MV8Fwee96snKzMw.png" data-width="1608" data-height="264" src="https://cdn-images-1.medium.com/max/800/1*hBUlJ42MV8Fwee96snKzMw.png"></figure><ul class="postList"><li name="9c19" id="9c19" class="graf graf--li graf-after--figure">If <code class="markup--code markup--li-code">pdist</code> is too large, although we may have this element in the cache ahead of time, it might be kicked up before it is finally accessed. So we will still have a cache miss. This is a problem when the prefetch is too early.</li></ul><figure name="55a4" id="55a4" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*RLVbDBl02vnZm-LCpeOvtg.png" data-width="1608" data-height="230" src="https://cdn-images-1.medium.com/max/800/1*RLVbDBl02vnZm-LCpeOvtg.png"></figure><p name="92f1" id="92f1" class="graf graf--p graf-after--figure">You can find that the coding of the <code class="markup--code markup--p-code">pdist</code> in the program becomes tricky because we can not have it too large nor too small. What’s more, the <code class="markup--code markup--p-code">pdist</code> is also related to the performance of the processor and the memory, which can be verified in every execution.</p><p name="523b" id="523b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Hardware Prefetching</strong></p><p name="c675" id="c675" class="graf graf--p graf-after--p">Because we have some difficulty coding into the program for the prefetch, we can use another method called <strong class="markup--strong markup--p-strong">hardware prefetching</strong>, where the program doesn’t change, and the hardware itself tries what will be accessed soon. There are many hardware prefetchers that work pretty well. For example,</p><ul class="postList"><li name="5d0f" id="5d0f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Stream buffer</strong>: <strong class="markup--strong markup--li-strong">sequential in nature</strong>, so it just trying to fetch several blocks in advance</li><li name="677a" id="677a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Stride prefetcher</strong>: monitors the access and see if the memory we are accessing has a <strong class="markup--strong markup--li-strong">fixed distance</strong> between each other. Then it will prefetch the memory with this times of this fixed distance.</li><li name="2ce0" id="2ce0" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Correlating prefetcher</strong>: predicts the <strong class="markup--strong markup--li-strong">sequence of the accesses</strong>. In a table, it will record the sequence of the memory accesses and try to prefetch the following accesses based on this table.</li></ul><p name="70fa" id="70fa" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(6) Methods for Reducing Miss Rate #3: Loop Interchange</strong></p><p name="c918" id="c918" class="graf graf--p graf-after--p">A final type of optimization for reducing the miss rate is called the <strong class="markup--strong markup--p-strong">loop interchange</strong>. This is one of the <strong class="markup--strong markup--p-strong">compiler optimizations</strong> that can be done to transform the code into a code that has a better locality. Let’s see an example. Suppose we have the following code,</p><pre name="3f35" id="3f35" class="graf graf--pre graf-after--p">for (i=0; i&lt;N; i++) {<br>    for (j=0; j&lt;N; j++) {<br>        a[j][i] = 0; <br>    }<br>}</pre><p name="c1f6" id="c1f6" class="graf graf--p graf-after--pre">Then when we access the memory in this nested loop, we are going to access some way like the following diagram,</p><figure name="3857" id="3857" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Da3A9qeFdfIuWQxX3kRgXQ.png" data-width="1986" data-height="404" src="https://cdn-images-1.medium.com/max/800/1*Da3A9qeFdfIuWQxX3kRgXQ.png"></figure><p name="6df3" id="6df3" class="graf graf--p graf-after--figure">We can see that all the memory we accessed will have a distance from each other, and this is not good for us because we can not benefit from the spatial locality. So a good compiler will find this problem and compile the program above in another way,</p><pre name="fbe6" id="fbe6" class="graf graf--pre graf-after--p">for (j=0; j&lt;N; j++) {<br>    for (i=0; i&lt;N; i++) {<br>        a[j][i] = 0; <br>    }<br>}</pre><p name="5286" id="5286" class="graf graf--p graf-after--pre">We can see that the sequence of the loop will be switched and now we are nicely sequentially accessing the matrix which can make us benefit from the spatial locality. As a result, the hit rate will be dramatically improved.</p><p name="997b" id="997b" class="graf graf--p graf-after--p">However, this method is not always possible, because we can not do this for any loop.</p><p name="0196" id="0196" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4. Improving Cache Performance with Miss Penalty</strong></p><p name="4fe4" id="4fe4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Methods for Reducing Miss Penalty #1: Overlap Misses</strong></p><p name="6250" id="6250" class="graf graf--p graf-after--p">The first method we want to discuss for reducing the miss penalty is called the <strong class="markup--strong markup--p-strong">overlap misses</strong>. Let’s see an example.</p><p name="2add" id="2add" class="graf graf--p graf-after--p">When we have a <strong class="markup--strong markup--p-strong">blocking cache </strong>(means that the cache will be blocked if we have a cache miss), in the beginning, we have some instructions executing. At some point, we perform a load instruction <code class="markup--code markup--p-code">LW A</code>, and A is not in the cache. So we have a cache miss and then we will start to access the memory for A. The program will be an out-of-order execution, so it will continue running.</p><p name="4395" id="4395" class="graf graf--p graf-after--p">However, because more and more instructions will rely on the result of A, then at some point we will run out of the RS or ROB. So the program can not continue to execute. Suppose there’s another load instruction <code class="markup--code markup--p-code">LW B</code> that occurs before the program stops, and B is also not in the cache. Instead of accessing the memory for B, this load instruction has to <strong class="markup--strong markup--p-strong">wait</strong> for the completion of <code class="markup--code markup--p-code">LW A</code> because we can only do 1 memory access at a time.</p><figure name="49b1" id="49b1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Qc37rnVEXH0yP5Xc4tXWig.png" data-width="1756" data-height="336" src="https://cdn-images-1.medium.com/max/800/1*Qc37rnVEXH0yP5Xc4tXWig.png"></figure><p name="8b86" id="8b86" class="graf graf--p graf-after--figure">We can also have a <strong class="markup--strong markup--p-strong">non-blocking cache</strong> and it can support things like,</p><ul class="postList"><li name="57f6" id="57f6" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">hit under miss</strong>: meaning when we are having a cache miss, hit to the other blocks in the cache that are sent by the processor will be serviced and returned to the processor with data</li><li name="2172" id="2172" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">miss under miss</strong>: means that when we have a miss, we can send another miss to the memory. The property that the processor is exploiting here is called <strong class="markup--strong markup--li-strong">memory-level parallelism</strong>, and this means that we can have more than 1 access to the memory at a time.</li></ul><p name="da0e" id="da0e" class="graf graf--p graf-after--li">Thus, the diagram for a non-blocking cache will be,</p><figure name="c47e" id="c47e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dnvG1RE6glr9qkhUYtlhnQ.png" data-width="1756" data-height="336" src="https://cdn-images-1.medium.com/max/800/1*dnvG1RE6glr9qkhUYtlhnQ.png"></figure><p name="5782" id="5782" class="graf graf--p graf-after--figure">We can discover that with overlapped misses, we can have a better performance because we don’t have to pay the cost to wait for the memory access.</p><p name="8c56" id="8c56" class="graf graf--p graf-after--p">So what is most important is that we have to support the miss-under-miss execution or the memory-level parallelism in order to have a better performance.</p><p name="1d5b" id="1d5b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Miss-Under-Miss Support in Caches: Miss Status Handling Registers (MSHRs)</strong></p><p name="6fad" id="6fad" class="graf graf--p graf-after--p">Before, the cache would simply block when it has a miss and not do anything until the miss comes. So it doesn’t need much support for that. However, with miss-under-miss, we have a miss but the cache will not be blocked. So other accesses keep coming to the cache.</p><p name="1479" id="1479" class="graf graf--p graf-after--p">If there are hits, we can handle them normally by just finding those blocks. If there are misses, we have to do more now. We ought to have <strong class="markup--strong markup--p-strong">miss status handling registers (MSHRs)</strong>, and these are used to remember what we request from the memory.</p><p name="465b" id="465b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) MSHR Implementation</strong></p><p name="e0c9" id="e0c9" class="graf graf--p graf-after--p">The MSHRs keeps the information about the cache misses that we currently have in the process. So when we have a cache miss, we have to check the MSHR first to see if there’s any match. When we have a miss, we want to know whether it is a new miss or it is an existing miss waiting for the data back from the memory. So basically, we can have 2 situations,</p><ul class="postList"><li name="c2ac" id="c2ac" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Miss</strong>: if there’s not a match in the MSHRs, we can know that it is a miss to a different block. In that case, we allocate a new MSHR where we remember which instruction in the processor to wake up when the data comes back</li><li name="d68d" id="d68d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Half-Miss</strong>: if there’s a match in the MSHRs, we can know that we try to find data from a block, and that block had at least one previous miss that was already sent to the memory but it didn’t come back yet. So because the request for this has already set to the memory, we should not do that again. Instead, we simply add that instruction to the corresponding MSHR. When the data comes back from the memory, we ought to wake up all the instructions that were added to this MSHR. After waking up the instructions, we release the MSHR that we have allocated for this data.</li></ul><p name="24ca" id="24ca" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) MSHR Amount</strong></p><p name="164b" id="164b" class="graf graf--p graf-after--p">So how many MSHRs we have to use? It turns out that there’s a huge benefit for us even if we only have 2 MSHRs so that we can handle two different block misses at the same time. It would be even better if we have 4 MSHRs. Moreover, even if we have 16 or 32 MSHRs, we can still benefit from the MSHRs.</p><p name="c0e1" id="c0e1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Methods for Reducing Miss Penalty #2: Cache Hierarchy</strong></p><p name="e75f" id="e75f" class="graf graf--p graf-after--p">The method of <strong class="markup--strong markup--p-strong">cache hierarchy</strong> is also called the <strong class="markup--strong markup--p-strong">multi-level caches</strong>. We have already discussed the L1 cache, which is considered to be the first level for a cache hierarchy. When we have a cache miss in the L1 cache, we will just go to another cache.</p><p name="9b93" id="9b93" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) AMAT with Cache Hierarchy</strong></p><p name="5a16" id="5a16" class="graf graf--p graf-after--p">So suppose we have an L2 cache, and when we have a cache miss in L1, we will then go to search in the L2 cache. So, the miss penalty for the L1 cache is not equal to the memory latency. Instead, it will equal to,</p><figure name="ef91" id="ef91" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YGhV1bOcFTww4oI_yYYeFA.png" data-width="1508" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*YGhV1bOcFTww4oI_yYYeFA.png"></figure><p name="ccf2" id="ccf2" class="graf graf--p graf-after--figure">We can even have more levels of cache. Then the miss penalty for the rest of the level should be,</p><figure name="ee57" id="ee57" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mRwIbIu9Qj8eJcLZ7CDEGQ.png" data-width="1508" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*mRwIbIu9Qj8eJcLZ7CDEGQ.png"></figure><p name="c160" id="c160" class="graf graf--p graf-after--figure">Note that if we have a cache miss in the <strong class="markup--strong markup--p-strong">last level cache (LLC)</strong>, we have to go to the memory and find the data we need. So the miss penalty for the last level cache would be,</p><figure name="b9d6" id="b9d6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ofz-RW1ajIwc1BWYB6HVYA.png" data-width="1508" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*ofz-RW1ajIwc1BWYB6HVYA.png"></figure><p name="afc1" id="afc1" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(7) Multi-level Cache Performance</strong></p><p name="fc6c" id="fc6c" class="graf graf--p graf-after--p">Suppose the memory latency is 100 cycles and for the following 4 types of caches, let’s calculate the AMAT for each of them.</p><figure name="1f10" id="1f10" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vsYOoThQHk4kmrsm9egI8Q.png" data-width="1706" data-height="364" src="https://cdn-images-1.medium.com/max/800/1*vsYOoThQHk4kmrsm9egI8Q.png"></figure><p name="3fdc" id="3fdc" class="graf graf--p graf-after--figure">After calculation, the final results would be,</p><figure name="5866" id="5866" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*v55XxBRC8p0A233-0UrCyQ.png" data-width="1706" data-height="400" src="https://cdn-images-1.medium.com/max/800/1*v55XxBRC8p0A233-0UrCyQ.png"></figure><p name="cecb" id="cecb" class="graf graf--p graf-after--figure">We can find out that the cache hierarchies work better than individual caches and why we have them. It is not enough to have only a large cache or a small cache. In that case, you can combine the caches to have a better performance.</p><p name="88ce" id="88ce" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Global Hit Rate Vs. Local Hit rate</strong></p><p name="d8e5" id="d8e5" class="graf graf--p graf-after--p">We can find out that the hit rate of L2 (i.e. 75%) in the cache hierarchy is lower than the result if we only have a 128 kB cache (i.e. 97.5%). This is because, in cache hierarchy, the L2 is only accessed by the data that is missed in L1, and this makes it even harder to predict.</p><p name="7f12" id="7f12" class="graf graf--p graf-after--p">A <strong class="markup--strong markup--p-strong">local hit rate</strong> is a hit rate that the cache actually observes. Because all the accesses to the L2, L3, and etc. caches (except for L1) do not benefit from localities, the local hit rate is lower compared with the <strong class="markup--strong markup--p-strong">global hit rate</strong> (means when such a cache is used alone).</p><ul class="postList"><li name="90cb" id="90cb" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Global Hit Rate</strong></li></ul><figure name="1567" id="1567" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*OzMv4RZg2IwN63DKjV3qfg.png" data-width="1248" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*OzMv4RZg2IwN63DKjV3qfg.png"></figure><ul class="postList"><li name="048f" id="048f" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Global Miss Rate</strong></li></ul><figure name="9e36" id="9e36" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*Q7PHGmvJQ_2RCXOrjQ5tEQ.png" data-width="1248" data-height="108" src="https://cdn-images-1.medium.com/max/800/1*Q7PHGmvJQ_2RCXOrjQ5tEQ.png"></figure><ul class="postList"><li name="73f2" id="73f2" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Local Hit Rate</strong></li></ul><figure name="50b2" id="50b2" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*FIOH-nQT59F2X7UCpAU_Ug.png" data-width="1248" data-height="108" src="https://cdn-images-1.medium.com/max/800/1*FIOH-nQT59F2X7UCpAU_Ug.png"></figure><ul class="postList"><li name="cfec" id="cfec" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Local Miss Rate</strong></li></ul><figure name="3a89" id="3a89" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*f7Wo3pt-33EV-Yx9eI-F4g.png" data-width="1126" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*f7Wo3pt-33EV-Yx9eI-F4g.png"></figure><p name="3bfd" id="3bfd" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(9) Misses Per Thousand Instructions (MPKI)</strong></p><p name="5b71" id="5b71" class="graf graf--p graf-after--p">Another popular metric of how often the cache hits that tries to capture the behavior of caches that are not the L1 cache is <strong class="markup--strong markup--p-strong">misses per thousand instructions (MPKI)</strong>. It is very similar to the global miss rate except that it doesn’t normalize the misses with the number of memory accesses. It normalizes with a number of instructions.</p><p name="a57a" id="a57a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) Inclusion Property for Cache Hierarchy</strong></p><p name="3170" id="3170" class="graf graf--p graf-after--p">Now, let’s discuss the last topic of the cache hierarchy, the inclusion property. When we have a block that exists in the L1 cache, there can be three situations in the L2 cache,</p><ul class="postList"><li name="3f6e" id="3f6e" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Inclusion</strong>: means that this block is also in the L2 cache</li><li name="1d66" id="1d66" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Exclusion</strong>: means that this block is not in the L2 cache</li><li name="5f80" id="5f80" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Random</strong>: means that this block may or may not be in the L2 cache</li></ul><p name="0081" id="0081" class="graf graf--p graf-after--li">Except for the case that we explicitly point out it is an inclusion or an exclusion, most cache hierarchy will result in a random state. So the inclusion and the exclusion don’t necessarily hold for this case because things that accessed in the L1 cache will not access the L2 cache.</p><p name="67e8" id="67e8" class="graf graf--p graf-after--p graf--trailing">To force an inclusion, we need to add a bit for inclusion.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/dff8f03bbff6"><time class="dt-published" datetime="2021-03-13T12:51:06.576Z">March 13, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/high-performance-computer-architecture-24-cache-performance-vipt-cache-aliasing-way-dff8f03bbff6" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>