<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>High-Performance Computer Architecture 30 | Introduction to Cache Coherence, Coherence Protocols…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">High-Performance Computer Architecture 30 | Introduction to Cache Coherence, Coherence Protocols…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: High-Performance Computer Architecture
</section>
<section data-field="body" class="e-content">
<section name="a2be" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="b94e" id="b94e" class="graf graf--h3 graf--leading graf--title">High-Performance Computer Architecture 30 | <strong class="markup--strong markup--h3-strong">Introduction to Cache Coherence, Coherence Protocols, MSI, MOSI, MESI, MOESI, Directory Approach, and Coherence Misses</strong></h3><figure name="449a" id="449a" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*mvDZkfd4fJd9cFQA.png" data-width="1446" data-height="864" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*mvDZkfd4fJd9cFQA.png"></figure><p name="527b" id="527b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">1. Introduction to Cache Coherence</strong></p><p name="1600" id="1600" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Cache Incoherent Problem</strong></p><p name="4cb2" id="4cb2" class="graf graf--p graf-after--p">Before we talk about cache coherence, let’s first see why do we need to have cache coherence. When we use the shared memory for multicores, we assume that we can always retrieve the latest value from the shared memory. However, this is not the case in reality because each core has its own cache. If one core writes to its cache, and the cache has not written to the shared memory, then other cores will grab the wrong data from the memory if they need this value. This is called <strong class="markup--strong markup--p-strong">cache incoherence</strong>.</p><p name="8c63" id="8c63" class="graf graf--p graf-after--p">The following graph is an example of cache incoherence,</p><figure name="3ee9" id="3ee9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*-1G2cn1T8JUOvkqq.gif" data-width="1200" data-height="699" src="https://cdn-images-1.medium.com/max/800/0*-1G2cn1T8JUOvkqq.gif"><figcaption class="imageCaption">from <a href="https://commons.wikimedia.org/w/index.php?title=User:M3tainfo&amp;action=edit&amp;redlink=1" data-href="https://commons.wikimedia.org/w/index.php?title=User:M3tainfo&amp;action=edit&amp;redlink=1" class="markup--anchor markup--figure-anchor" title="User:M3tainfo (page does not exist)" rel="noopener" target="_blank">M3tainfo</a> protected by <a href="https://creativecommons.org/licenses/by-sa/4.0" data-href="https://creativecommons.org/licenses/by-sa/4.0" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">CC BY-SA 4.0</a></figcaption></figure><p name="ec4b" id="ec4b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) The Definition of Cache Coherence</strong></p><p name="af2d" id="af2d" class="graf graf--p graf-after--p">So let’s now define what it means for caches to be coherent. Intuitively, they are coherent because the two things behave like <strong class="markup--strong markup--p-strong">no caches</strong>, and specifically, there are three requirements for the cache coherence,</p><ul class="postList"><li name="e6fa" id="e6fa" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Property #1</strong>: A read <code class="markup--code markup--li-code">R</code> from the address <code class="markup--code markup--li-code">X</code> on the core, C1 returns the value written by the <strong class="markup--strong markup--li-strong">most recent</strong> write <code class="markup--code markup--li-code">W</code> to <code class="markup--code markup--li-code">X</code> on C1</li><li name="1434" id="1434" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Property #2</strong>: If C1 writes to <code class="markup--code markup--li-code">X</code> and C2 reads after a sufficient time, and there are no other writes in between, then C2’s Read must return the value written by C1</li><li name="ca18" id="ca18" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Property #3</strong>: If there are more than 1 writes to the same location, then those writes must be<strong class="markup--strong markup--li-strong"> serialized</strong>. Any two writes to <code class="markup--code markup--li-code">X</code> must be seen to occur in the same order on all the cores</li></ul><figure name="0742" id="0742" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*dS_00oddGH07LlJH.gif" data-width="1200" data-height="699" src="https://cdn-images-1.medium.com/max/800/0*dS_00oddGH07LlJH.gif"><figcaption class="imageCaption">from <a href="https://commons.wikimedia.org/w/index.php?title=User:M3tainfo&amp;action=edit&amp;redlink=1" data-href="https://commons.wikimedia.org/w/index.php?title=User:M3tainfo&amp;action=edit&amp;redlink=1" class="markup--anchor markup--figure-anchor" title="User:M3tainfo (page does not exist)" rel="noopener" target="_blank">M3tainfo</a> protected by <a href="https://creativecommons.org/licenses/by-sa/4.0" data-href="https://creativecommons.org/licenses/by-sa/4.0" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">CC BY-SA 4.0</a></figcaption></figure><p name="4ccb" id="4ccb" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Cache Coherence Improper Approaches</strong></p><p name="341f" id="341f" class="graf graf--p graf-after--p">There are several approaches for achieving cache coherence. However, because some of the approaches have bad performances, we are not going to use them in practice. These approaches include,</p><ul class="postList"><li name="305a" id="305a" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Don’t do caching</strong>: if we don&#39;t do caching, we will not worry about the cache coherence. However, this approach will have a <strong class="markup--strong markup--li-strong">bad performance</strong>, so we normally don’t do so.</li><li name="8aea" id="8aea" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Shared L1 cache</strong>: another option is to make all the cores share the same L1 cache. However, because there will be many conflict cache misses, we will still result in a <strong class="markup--strong markup--li-strong">bad performance</strong>.</li><li name="dcd8" id="dcd8" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Private write-through (or write-back) caches</strong>: we can also have privates caches and make them write-through (directly write to the shared memory after the cache is updated). In that case, the shared memory actually sees all the writes. But the problem is that once the value is read, it doesn’t matter that all the other cores finish their writings. So potentially, we will see the stale value even after many many writes to the shared memory. So even with write-through caches, the caches are <strong class="markup--strong markup--li-strong">not necessarily coherent</strong>.</li></ul><p name="4b82" id="4b82" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Cache Coherence Approaches for Property #2</strong></p><p name="cb85" id="cb85" class="graf graf--p graf-after--p">There are two useful approaches that can be used to achieve property #2 of the cache coherence. One technique is called the write-update coherence, and the other is called the write-invalidate coherence.</p><ul class="postList"><li name="88b9" id="88b9" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Broadcast write to update other caches</strong>: this means that the value is not only written back to the shared memory, but it will also be written to all the other L1 caches as well. After that, all the other caches will now have the new value and they will actually provide the same value. This is also called <strong class="markup--strong markup--li-strong">write-update coherence</strong> because all the other caches are updated after each writes.</li><li name="0c75" id="0c75" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Pretend to have a cache miss after write</strong>: another useful approach is that after a write to the memory, we will also prevent all the other caches from having hits, this means that these caches need to behave as if they have a cache miss or if their blocks are invalid. So this is also called a <strong class="markup--strong markup--li-strong">write-invalidate coherence</strong>.</li></ul><p name="1c14" id="1c14" class="graf graf--p graf-after--li">We need to pick one of these approaches in order to suit property #2.</p><p name="267c" id="267c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Cache Coherence Approaches for Property #3</strong></p><p name="bc4d" id="bc4d" class="graf graf--p graf-after--p">For cache coherence property #3, we have to decide the order of the writes and all the cores need to see the same order. This feature brings us two different approaches,</p><ul class="postList"><li name="77e6" id="77e6" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Snooping</strong>: One way is that we can simply broadcast all the writes on the shared bus, and this means that the order of the writes on the shared bus is the order that should be seen by every core. This technique is called <strong class="markup--strong markup--li-strong">snooping</strong> because when the writes are sent to the bus, every core snoops this write. The downside of this approach is that the shared bus can become a bottom line.</li><li name="5708" id="5708" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Directory</strong>: The other way is that each block is assigned an <strong class="markup--strong markup--li-strong">ordering point </strong>(aka. <strong class="markup--strong markup--li-strong">directory</strong>). Different ordering points can be used for different blocks. For each block, all accesses are ordered by the same entity. But different blocks have different entities so they don’t have a contention. When a write occurs the directory reflects the state change.</li></ul><p name="0561" id="0561" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Coherence Protocols</strong></p><p name="b4d7" id="b4d7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Write-Update Snooping Coherence</strong></p><p name="20ae" id="20ae" class="graf graf--p graf-after--p">Now, let’s first see an example of the write-update snooping coherence. Suppose have two caches and each of these caches has two blocks with</p><ul class="postList"><li name="999f" id="999f" class="graf graf--li graf-after--p">a single</li><li name="daed" id="daed" class="graf graf--li graf-after--li">a tag</li><li name="e9d9" id="e9d9" class="graf graf--li graf-after--li">data</li></ul><p name="f092" id="f092" class="graf graf--p graf-after--li">Both of these two caches are connected to the same bus and we also connect the memory to this bus. Let’s say that the cache is initially empty so that all the valid bits are zero.</p><figure name="2f84" id="2f84" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ht1YG8VmgsND49TcYfh_eA.png" data-width="1552" data-height="718" src="https://cdn-images-1.medium.com/max/800/1*Ht1YG8VmgsND49TcYfh_eA.png"></figure><p name="1a8f" id="1a8f" class="graf graf--p graf-after--figure">Let’s now see how the write-updating and the snooping works. Suppose we have core #1 read from address A in the beginning. Because the cache is empty, we will have a cache miss and the data is read directly from the main memory. Suppose the data we read is 0, then</p><figure name="5c40" id="5c40" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Z6cHkI9eMy2ajzc5yfn0tw.png" data-width="1552" data-height="718" src="https://cdn-images-1.medium.com/max/800/1*Z6cHkI9eMy2ajzc5yfn0tw.png"></figure><p name="32f9" id="32f9" class="graf graf--p graf-after--figure">Now, if core #2 would like to write 1 to address A, then in order to achieve a cache-coherent by writing-update, we should not only update the value in the main memory but also we should update the value in cache #1. So the core #1 will <strong class="markup--strong markup--p-strong">snoop</strong> the bus and check if there is a tag corresponding to address A.</p><figure name="0816" id="0816" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Q0l0UwASAL-mTmOQePWRZw.png" data-width="1552" data-height="718" src="https://cdn-images-1.medium.com/max/800/1*Q0l0UwASAL-mTmOQePWRZw.png"></figure><p name="782b" id="782b" class="graf graf--p graf-after--figure">If both of the core write to the bus simultaneously, these writes will be serialized on the bus and then we will have to write to the main memory and update the other caches. Let’s say core #1 wants to write 2 to A, while core #2 wants to write 3 to A, but core #1 writes to the bus in the first place. So we will have,</p><figure name="debb" id="debb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RrWV3CvB8FTpNaCzIsg3xg.png" data-width="1552" data-height="718" src="https://cdn-images-1.medium.com/max/800/1*RrWV3CvB8FTpNaCzIsg3xg.png"></figure><figure name="72cc" id="72cc" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*6ioeeMBznXTDkdhc0hSTHg.png" data-width="1552" data-height="718" src="https://cdn-images-1.medium.com/max/800/1*6ioeeMBznXTDkdhc0hSTHg.png"></figure><p name="9697" id="9697" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Write-Update Optimization: Memory Write Bottleneck</strong></p><p name="262d" id="262d" class="graf graf--p graf-after--p">Under the condition of a write-update cache, we have a serious problem with memory accesses. As we have discussed for the unicore system, we will achieve bad performance with write-back caches because the amount of the writes can be huge, but the ability for the memory to accesses these writes is limited. Therefore, we can improve the performance by simply remove all the unnecessary writes to the memory. In order to eliminate all the unnecessary writes, we have to add a <strong class="markup--strong markup--p-strong">dirty bit</strong> to each block.</p><figure name="549e" id="549e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mimO0gjPRgFFMCegUcwCgg.png" data-width="1552" data-height="718" src="https://cdn-images-1.medium.com/max/800/1*mimO0gjPRgFFMCegUcwCgg.png"></figure><p name="6a33" id="6a33" class="graf graf--p graf-after--figure">Although this dirty bit (D) has a similar function to the uniprocessor system, it actually has some differences. Commonly, this bit has two functions,</p><ul class="postList"><li name="d179" id="d179" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Avoiding writes to the memory</strong>: one function of the dirty bit is that we don’t have to write back to the main memory immediately after each writes. Instead, the dirty bit will be set to 1, and we will only write to the memory when the block with a dirty bit is kicked out of the cache. For example, if we have the following caches in the beginning,</li></ul><figure name="b2f1" id="b2f1" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*omZlEa4-ZCnQBl9ez_Cw2g.png" data-width="1552" data-height="718" src="https://cdn-images-1.medium.com/max/800/1*omZlEa4-ZCnQBl9ez_Cw2g.png"></figure><p name="acca" id="acca" class="graf graf--p graf-after--figure">Then a write to A with value 17 will update as follows,</p><figure name="10cc" id="10cc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yjOYdNQWPbOAsxP7rh1W-Q.png" data-width="1552" data-height="718" src="https://cdn-images-1.medium.com/max/800/1*yjOYdNQWPbOAsxP7rh1W-Q.png"></figure><ul class="postList"><li name="684d" id="684d" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Directly read from the cache</strong>: there is the other function of the dirty bit. Suppose core #1 would like to read from the B now but the true value of B is not updated in the main memory. Instead, the real value of B stays in the L1 cache of core 2,</li></ul><figure name="eed1" id="eed1" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*9gNcerzmp2d6A_grtJis2A.png" data-width="1552" data-height="718" src="https://cdn-images-1.medium.com/max/800/1*9gNcerzmp2d6A_grtJis2A.png"></figure><p name="6362" id="6362" class="graf graf--p graf-after--figure">In this situation, we will not read from the main memory to get the value of B. However, we will directly access the L1 cache of core #2, and then, the value 3 is read from the cache instead of the main memory.</p><figure name="2cb8" id="2cb8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Qyh0LO4IzQVfygb9Lm2srA.png" data-width="1552" data-height="718" src="https://cdn-images-1.medium.com/max/800/1*Qyh0LO4IzQVfygb9Lm2srA.png"></figure><p name="7b18" id="7b18" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Write-Update Optimization: Shared Bus Bottleneck</strong></p><p name="8392" id="8392" class="graf graf--p graf-after--p">Now that we have dealt with the bottleneck of the main memory, we have to consider the bandwidth of the shared bus. Because the bus still gets all the traffics for every single write, the bus with all the writes now becomes a bottleneck.</p><p name="2f0b" id="2f0b" class="graf graf--p graf-after--p">In order to reduce the writes send to the bus, we can add a shared bit (S). The basic idea of this shared bit is that we can reduce the writes sent to the bus if there is no need to update in another cache. So if we write to an address only exists in the current cache block, we will not send it to the bus.</p><p name="72bc" id="72bc" class="graf graf--p graf-after--p">Let’s see an example. Suppose core #1 read A in the beginning and the value of A from the memory is 6. Then core #2 read B and the value of B from the memory is 17.</p><figure name="c2d5" id="c2d5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4dYeg_1L0nSysBOGcRCRiA.png" data-width="1500" data-height="710" src="https://cdn-images-1.medium.com/max/800/1*4dYeg_1L0nSysBOGcRCRiA.png"></figure><p name="4af6" id="4af6" class="graf graf--p graf-after--figure">Now if core #2 writes 10 to A, then the L1 cache should be updated. The block of A in cache #2 has a dirty bit of 1 (because we write to A but we don’t write back to the memory) and a shared bit of 1 (because now A is shared by core #1 and core #2). This write will be sent to the bus because the shared bit is 1, which means that we have to update some other caches.</p><figure name="3750" id="3750" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*iNNViADXe9HMLPks95zeXg.png" data-width="1500" data-height="710" src="https://cdn-images-1.medium.com/max/800/1*iNNViADXe9HMLPks95zeXg.png"></figure><p name="cb74" id="cb74" class="graf graf--p graf-after--figure">If core #2 writes 5 to B, then the L1 cache of core #1 doesn’t have to be updated. Because the shared bit is 0 for B, which means that B is not shared and we don’t have to update some other caches. Then we don’t need to send this write to the bus and this saves the bandwidth of the bus.</p><figure name="e5d6" id="e5d6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*hQe8UOFge5eyvzK4sAmOVQ.png" data-width="1500" data-height="710" src="https://cdn-images-1.medium.com/max/800/1*hQe8UOFge5eyvzK4sAmOVQ.png"></figure><p name="d4a2" id="d4a2" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Write-Invalidate Snooping Coherence</strong></p><p name="fdc2" id="fdc2" class="graf graf--p graf-after--p">Another approach is by using the write-invalidate one. Instead of updating the other caches after writes, we can simply invalidate them. Suppose we have a read A at the beginning and the value of 0 is read from the main memory.</p><figure name="bc93" id="bc93" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kpf6_TOfkd-xD8ulhu5arw.png" data-width="1500" data-height="710" src="https://cdn-images-1.medium.com/max/800/1*kpf6_TOfkd-xD8ulhu5arw.png"></figure><p name="409a" id="409a" class="graf graf--p graf-after--figure">Then suppose core #2 writes to A with value 17. Because we will invalidate the cache with this address A, the valid bit in the corresponding block of cache #1 will be set to 0. Note the shared bit in cache #2 should be 0 because the value of A only exists in cache #2.</p><figure name="2594" id="2594" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ZAOVVNzVQgBVYEIBhHct9g.png" data-width="1500" data-height="710" src="https://cdn-images-1.medium.com/max/800/1*ZAOVVNzVQgBVYEIBhHct9g.png"></figure><p name="990d" id="990d" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Update Vs. Invalidate Coherence</strong></p><p name="7c92" id="7c92" class="graf graf--p graf-after--p">Although we may consider that the update idea is much simpler, in practice, it is only used when one core writes and another core reads often. If an application has a burst of write to one address ­or if an application writes to different words in the same block, then invalidate is the better method.</p><p name="8d76" id="8d76" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) MSI Coherence</strong></p><p name="87a3" id="87a3" class="graf graf--p graf-after--p">One simpler coherence protocol is called <strong class="markup--strong markup--p-strong">MSI coherence</strong> (aka. modified-shared-invalid) and it s is an invalidation-based protocol. In MSI, a block can be in one of three states in a cache,</p><ul class="postList"><li name="ad92" id="ad92" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Invalid state</strong>: (Valid bit = 0) present in the cache with a 0 valid bit or simply not in the cache</li><li name="cdd9" id="cdd9" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Shared state</strong>: (Valid bit = 1; Dirty bit = 0) a block in the shared state means that we can do simple reads without any further ado. However, we have to do some extra work if we want to write to it</li><li name="5721" id="5721" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Modified state</strong>: (Valid bit = 1; Dirty bit = 1) in this case, we can do local reads and local writes without any further work. This is because we have known that the current block we are operating is not a shared block.</li></ul><figure name="29b7" id="29b7" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*AcICoxKG7I09IXVjuCRT5w.png" data-width="1500" data-height="672" src="https://cdn-images-1.medium.com/max/800/1*AcICoxKG7I09IXVjuCRT5w.png"></figure><p name="845f" id="845f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(7) Cache-to-Cache Transfers</strong></p><p name="fc31" id="fc31" class="graf graf--p graf-after--p">In the previous examples, we only said that if the cache has its dirty bit equals 1 (which is exactly in the modified state), we will not read from the memory. Instead, we can directly read from the cache. This process is called a <strong class="markup--strong markup--p-strong">cache-to-cache transfer</strong>.</p><p name="1b84" id="1b84" class="graf graf--p graf-after--p">When core #1 has block B in the modified state, and core #2 puts a read request for B on the bus, core #1 has to provide the data. In order to provide the data in core #1, basically, we have two methods,</p><ul class="postList"><li name="6568" id="6568" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Solution #1</strong>: Abort &amp; Retry</li></ul><p name="3a23" id="3a23" class="graf graf--p graf-after--li">The first solution is that the core #1 somehow cancels the request from core #2 by an <code class="markup--code markup--p-code">ABORT</code> bus signal. When a request is placed on the bus, another core can assert the abort signal that tells the requesting core to back off and try again later. After the request from core #2 is aborted, core #1 can do a normal write back to the memory. At this point, the memory has to update the data and when core #2 retries its read request, it will get the data from memory.</p><p name="9143" id="9143" class="graf graf--p graf-after--p">The problem of this solution is that we will have two memory latencies before the core #2 can get the data. One for the write-back to memory to happen on C1, and then for us to actually get the data from the memory. Because w have to pay twice the memory latency, this solution is not ideal for us.</p><ul class="postList"><li name="1f1d" id="1f1d" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Solution #2</strong>: Intervention</li></ul><p name="7749" id="7749" class="graf graf--p graf-after--li">The second solution is called intervention. In this case, core #1 tells the memory that it will supply data by a <code class="markup--code markup--p-code">INTERVENTION</code> bus signal through core #1 asserting the intervention&#39;s signal signals to the memory not the responds. After that, core #1 will respond to the request with data. However, after responding to the request, the dirty bit of the cache of core #1 will become 0 and this means that all the caches will be in the shared state. So the memory should also read from the response from core #1 and updates its values, and the memory ensures that it gets the data that will not be written back anymore.</p><p name="20ee" id="20ee" class="graf graf--p graf-after--p">The problem with this solution is that this solution needs more complex hardware. In most modern processors, we are actually using a variant of an intervention approach because the complexity to respond is not too large compared with two memory latencies.</p><p name="274b" id="274b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Cache-to-Cache Transfer Writing Overheads</strong></p><p name="69a9" id="69a9" class="graf graf--p graf-after--p">When we do cache-to-cache transfer, there are actually two aspects that we can improve.</p><ul class="postList"><li name="fe5b" id="fe5b" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Unnecessary memory writes</strong>: suppose in the beginning, c1 (core #1) has the block in the M state and c2 (core #2) wants to read. By intervention, c2 will read from the c1 instead of the memory. After c1 responds to the request of c2, both the blocks in c1 and c2 become S state and the memory is also updated. Now if c2 writes to the block, c1 gets an I state and c2 gets an M state. If we continue this loop by c2 starting with M state and c1 reads from it, then we can finally achieve a result that the data is moving around between the caches and we don’t actually need to write to the main memory so many times in this case.</li><li name="3fdd" id="3fdd" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Unnecessary memory reads</strong>: there are also some unnecessary memory reads in the MSI protocol. When c1 sends a read request and c2 provides the data, both of them will become the S state and the data will be written to the main memory. Now when we have another core c3 who would like to request the data, we can only send the request to the main memory although both c1 and c2 can provide the data. Therefore, in fact, there are some unnecessary memory reads.</li></ul><p name="1838" id="1838" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(9) The Definition of the Owner State (O)</strong></p><p name="f9f1" id="f9f1" class="graf graf--p graf-after--p">In order to deal with the unnecessary memory reads and writes, we need to make a non-modified version of the block in one of the caches that are responsible for,</p><ul class="postList"><li name="23db" id="23db" class="graf graf--li graf-after--p">giving data to other caches (so that we don’t need unnecessary memory reads)</li><li name="047c" id="047c" class="graf graf--li graf-after--li">eventually writing block to the memory (so that we don’t need unnecessary writes)</li></ul><p name="590a" id="590a" class="graf graf--p graf-after--li">So what we really need to know is which of the shared copies is the one responsible, and we will know this by introducing another state called <strong class="markup--strong markup--p-strong">owned</strong> (O).</p><p name="dff7" id="dff7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) MOSI Coherence</strong></p><p name="f450" id="f450" class="graf graf--p graf-after--p">MOSI is designed to deal with the problems we have talked about for the MSI coherence. The MSI and MOSI are similar, but the main difference is that when we are in the modified state, we provide the data as before. But after providing the data, we will move into the O state instead of the S state. By moving the block into the O state, we will not update the memory immediately.</p><p name="2897" id="2897" class="graf graf--p graf-after--p">When a block is in the O state, it will basically have two functions,</p><ul class="postList"><li name="59a7" id="59a7" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">providing data</strong>: when it snoops a read (only necessary reads)</li><li name="603c" id="603c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">write back</strong> to memory if replaced (only necessary writes)</li></ul><p name="7de4" id="7de4" class="graf graf--p graf-after--li">So eventually, we have used the MOSI state to avoid the inefficiency of the MSI protocol that had to do with the memory, getting unnecessary accesses that could be satisfied by caches that already have the data. However, there is another inefficiency that both MSI and MOSI have, which is related to the <strong class="markup--strong markup--p-strong">thread-private data</strong> that is only ever accessed by a single thread.</p><p name="a99f" id="a99f" class="graf graf--p graf-after--p">Now, let’s to talk more about this inefficiency problem.</p><p name="f08e" id="f08e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(11) The Definition of Thread-Private Data</strong></p><p name="ff75" id="ff75" class="graf graf--p graf-after--p">The data that is only accessed by a single thread is called the <strong class="markup--strong markup--p-strong">thread-private data</strong>. For example, all data in a single thread will be thread-private. So if we have a 4-core processor around four single threaded programs, they will never share any data.</p><p name="8195" id="8195" class="graf graf--p graf-after--p">Even if we have a parallel program, we still have some data (e.g. data in the <strong class="markup--strong markup--p-strong">stack</strong>) that are still can only be accessed by a single thread.</p><p name="a518" id="a518" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(12) Inefficiency Problem for MSI and MOSI</strong></p><p name="1285" id="1285" class="graf graf--p graf-after--p">With both MSI and MOSI, we have the inefficiency when we <strong class="markup--strong markup--p-strong">read the data</strong> in a thread and then we <strong class="markup--strong markup--p-strong">write to it</strong>, and the current thread is the <strong class="markup--strong markup--p-strong">only one</strong> that is accessing the data. When we read the data with the I state, we will have a cache miss and then we will access the memory to get the data. After that, the block will be at the S state. If we then write a new value to this block, we will first send <code class="markup--code markup--p-code">INTERVENTION</code> to the bus so as to invalidate all the other caches with the same block.</p><figure name="9d5d" id="9d5d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*E_qI_QZFgYShArRsqaTlDQ.png" data-width="1594" data-height="410" src="https://cdn-images-1.medium.com/max/800/1*E_qI_QZFgYShArRsqaTlDQ.png"><figcaption class="imageCaption">MSI read-then-write procedure</figcaption></figure><figure name="7f9f" id="7f9f" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*ls9Y_Ke1FpfBY2wo2lrMDA.png" data-width="1594" data-height="410" src="https://cdn-images-1.medium.com/max/800/1*ls9Y_Ke1FpfBY2wo2lrMDA.png"><figcaption class="imageCaption">MOSI read-then-write procedure</figcaption></figure><p name="6abc" id="6abc" class="graf graf--p graf-after--figure">But this can be inefficiency when we only have one thread that has this data. Actually, we don&#39;t have to send <code class="markup--code markup--p-code">INTERVENTION</code> to the bus because the current data is private for the current thread and it can not be accessed by the other cores and caches.</p><p name="c74d" id="c74d" class="graf graf--p graf-after--p">We can notice that the send <code class="markup--code markup--p-code">INTERVENTION</code> penalty to the bus is actually what we have paid in this case for cache coherence, because we have assumed that no data are private, and we can always share the data. So what we want to do is to reduce the cost of sending this <code class="markup--code markup--p-code">INTERVENTION</code> to invalidate other cache blocks when the current block is thread-private. To avoid this invalidation, we will include a new state called <strong class="markup--strong markup--p-strong">exclusive</strong> (E).</p><p name="f388" id="f388" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(13) The Definition of the Exclusive State (E)</strong></p><p name="5ad0" id="5ad0" class="graf graf--p graf-after--p">Before we talk about the exclusive state, let’s review some other states,</p><ul class="postList"><li name="2bb7" id="2bb7" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">M state</strong>: the modified state gives us <strong class="markup--strong markup--li-strong">exclusive access</strong> so that we can do both read and write. And it makes us the owner of the <strong class="markup--strong markup--li-strong">dirty</strong> block so that we are going to update the memory.</li><li name="6b78" id="6b78" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">S state</strong>: the shared state gives us the <strong class="markup--strong markup--li-strong">shared access</strong> so that we can only read. And we are not responsible for giving data to the other caches or the memory, so we are <strong class="markup--strong markup--li-strong">clean</strong> in this state.</li><li name="8b5d" id="8b5d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">O state</strong>: The owned state also gives us a <strong class="markup--strong markup--li-strong">shared access</strong> so that we can do reads. However, it makes us the owner of the dirty block so that we are responsible for giving data to the other caches or the memory.</li></ul><p name="735e" id="735e" class="graf graf--p graf-after--li">What the exclusive state does is that,</p><ul class="postList"><li name="6772" id="6772" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">E state</strong>: it gives us the exclusive access so that we can do reads and writes. However, the block is still clean and the memory don’t have to be updated.</li></ul><p name="a003" id="a003" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(14) MESI/MOESI Coherence</strong></p><p name="f23d" id="f23d" class="graf graf--p graf-after--p">So finally, let’s talk about how the exclusive bit works to deal with all the thread-private data inefficiency. Let’s again, see an example of the read then write case for four different protocols.</p><p name="6b59" id="6b59" class="graf graf--p graf-after--p">If we first read A and then we write to A, as we have said, both the MSI and MOSI coherence will write to the bus twice. One for the cache miss after read, the other for sending the <code class="markup--code markup--p-code">INTERVENTION</code> signal.</p><figure name="63f3" id="63f3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*E_qI_QZFgYShArRsqaTlDQ.png" data-width="1594" data-height="410" src="https://cdn-images-1.medium.com/max/800/1*E_qI_QZFgYShArRsqaTlDQ.png"></figure><p name="10bb" id="10bb" class="graf graf--p graf-after--figure">However, for MESI and the MOESI, we don’t go directly to the shared state if we detect we are the only one that having this block. So instead, we will go to the exclusive state (E). So now when we write data to it, we know that this is an exclusive access. So that we can write without telling anybody. After that, we will transite to the M state, because the block is dirty and we have to write to the memory in the future.</p><figure name="3381" id="3381" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*s7ntUcuf4wqQ9_umIDvBow.png" data-width="1594" data-height="410" src="https://cdn-images-1.medium.com/max/800/1*s7ntUcuf4wqQ9_umIDvBow.png"></figure><p name="9d27" id="9d27" class="graf graf--p graf-after--figure">Finally, a general MOESI coherence protocol should be as follows,</p><figure name="87cc" id="87cc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8vtXNYFQD4FOg3rAMZW5Aw.png" data-width="1594" data-height="592" src="https://cdn-images-1.medium.com/max/800/1*8vtXNYFQD4FOg3rAMZW5Aw.png"></figure><p name="1420" id="1420" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">3. The Directory Approach</strong></p><p name="98e1" id="98e1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Snooping Downsides</strong></p><p name="9ada" id="9ada" class="graf graf--p graf-after--p">In the previous discussions, we have discussed that the other way to achieve the cache coherence is by a <strong class="markup--strong markup--p-strong">directory-based method</strong>. Before we talk about the directory, let’s first see what’s wrong with snooping. With the snooping approach, we can find out thart all the requests will be broadcasted to the bus so that the others can see them and establish ordering. However, the bandwidth can always be limited and we will have a <strong class="markup--strong markup--p-strong">bottleneck on the bus</strong>.</p><p name="661d" id="661d" class="graf graf--p graf-after--p">Therefore, the snooping approach doesn’t work well with more than 8 to 16 cores.</p><p name="a72f" id="a72f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Defintion of Directory</strong></p><p name="da5e" id="da5e" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">directory</strong> structure is a distributed structure across all the cores. It is not centralized in one place like the bus. Therefore, not all requests have to go to the same part of the directory.</p><ul class="postList"><li name="5a3d" id="5a3d" class="graf graf--li graf-after--p">Each “slice” of the directory will serve a set of blocks</li><li name="efe7" id="efe7" class="graf graf--li graf-after--li">Each core has its own “slice”</li></ul><p name="1e76" id="1e76" class="graf graf--p graf-after--li">Therefore, we can achieve a higher bandwidth because each slices will operate independently. In fact, a slice has,</p><ul class="postList"><li name="80a9" id="80a9" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Entries</strong> for blocks it serves, which tracks the information about which caches in the system have a non-invalid state.</li><li name="8e40" id="8e40" class="graf graf--li graf-after--li">The order of accesses for a particular block is determined by the <strong class="markup--strong markup--li-strong">home slice</strong> of that block. So that accesses goes to the same block are serialized</li></ul><p name="d262" id="d262" class="graf graf--p graf-after--li">Even if we have the directory protocol instead of the snooping protocol, the caches still have the same state (M/O/E/S/I). What is different is that when we send a request, the request no longer goes to the bus. Instead, the request will be sent via the network to the directory. Therefore, we can have many requests travelling to the individual slices, and then the directory figures out what to do next.</p><p name="a963" id="a963" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) The Directory Entry</strong></p><p name="c5bf" id="c5bf" class="graf graf--p graf-after--p">Now, let’s see the organization of the directory entry. Each directory entry has,</p><ul class="postList"><li name="8e1e" id="8e1e" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">1 dirty bit</strong>: indicates that the block is dirty in some cache in the system</li><li name="6825" id="6825" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">cache bits</strong> for each caches: indicates whether or not the data present in that cache. There should be 1 cache bit per cache in the entry.</li></ul><p name="81b3" id="81b3" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) The Directory Entry: Example</strong></p><p name="32f0" id="32f0" class="graf graf--p graf-after--p">Now, let’s see an example of the directory entry. Let’s suppose we have 8 caches in the system (cache #0 ~ cache #7, but only cache #1 and cache 2 are shown). At the beginning, all the caches and the directory structures are empty.</p><figure name="0ae1" id="0ae1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ZoNtesEStBkGwT1TEscGrw.png" data-width="1594" data-height="814" src="https://cdn-images-1.medium.com/max/800/1*ZoNtesEStBkGwT1TEscGrw.png"></figure><p name="0e14" id="0e14" class="graf graf--p graf-after--figure">Then if c1 send a read B request, we will first go to the directory and check if B exists in the system.</p><figure name="6301" id="6301" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*laXlM8wtSEU-c3iy-uV77g.png" data-width="1594" data-height="814" src="https://cdn-images-1.medium.com/max/800/1*laXlM8wtSEU-c3iy-uV77g.png"></figure><p name="f7bc" id="f7bc" class="graf graf--p graf-after--figure">Becasue all the caches are initially empty, the directory will figure out a cache miss and then it will get B from the memory,</p><figure name="d185" id="d185" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YZGRThRiX2ekdkYlXNHQfA.png" data-width="1594" data-height="814" src="https://cdn-images-1.medium.com/max/800/1*YZGRThRiX2ekdkYlXNHQfA.png"></figure><p name="881b" id="881b" class="graf graf--p graf-after--figure">Then the data of B will be sent to c1 and the corresponding block in c1 will be in the E state (suppose we have MOESI protocol), and the directory will give exclusive access to c1. Because now the cache #1 has the B block, we will have to set the corresponding cache bit in the directory entry to 1. What’s more, because now c1 has exclusive accesses for read and writes, we will also set the dirty bit in the directory entry to 1 so that it will always check the c1.</p><figure name="3225" id="3225" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Hd-_NN-HjJuOzD0V_UKhqg.png" data-width="1594" data-height="814" src="https://cdn-images-1.medium.com/max/800/1*Hd-_NN-HjJuOzD0V_UKhqg.png"></figure><p name="2c7b" id="2c7b" class="graf graf--p graf-after--figure">Now, let’s suppose c2 sends a write B request,</p><figure name="a762" id="a762" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*wqg5nJkVF6t9DhBLUm8q4A.png" data-width="1594" data-height="814" src="https://cdn-images-1.medium.com/max/800/1*wqg5nJkVF6t9DhBLUm8q4A.png"></figure><p name="4802" id="4802" class="graf graf--p graf-after--figure">Because the dirty bit of the directory entry is 1, we have to invalidate block B in c1. Therefore, before conduct the write request, it will be firstly send to the c1 and the corresponding block B in c1 will be invalidate.</p><figure name="d13f" id="d13f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Q_bImEL1NBjQlcx3NjG2Rg.png" data-width="1594" data-height="782" src="https://cdn-images-1.medium.com/max/800/1*Q_bImEL1NBjQlcx3NjG2Rg.png"></figure><p name="c047" id="c047" class="graf graf--p graf-after--figure">Before c1 is invalidate, the block B in c1 is in the exclusive state. So it can choose to respond with the data or it can just keep quite and confirm the validation. So that after the cache gets the write request, it will send at least the acknowledgment for invalidation if not with the data back to the directory, which means that the invalidation is done. When the acknowledgment arrives to the directory, the corresponding entry with the cache bit will be reset to 0, and if the data didn’t arrive, then we can simply replace the dirty bit.</p><figure name="9282" id="9282" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kgYw1D8zSwN5nf3G1bU2_Q.png" data-width="1594" data-height="782" src="https://cdn-images-1.medium.com/max/800/1*kgYw1D8zSwN5nf3G1bU2_Q.png"></figure><p name="00b2" id="00b2" class="graf graf--p graf-after--figure">Because now the caches seems all empty again and we have to access the memory to get the block B. Again, we will get cache B from the memory and then send it to c2. The corresponding bits in the entry will be changed. After c2 gets the block, it will write to the block with its new value and then the block state becomes M.</p><figure name="4115" id="4115" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ZGqIf3jyMu8-qb4kDzex4A.png" data-width="1594" data-height="782" src="https://cdn-images-1.medium.com/max/800/1*ZGqIf3jyMu8-qb4kDzex4A.png"></figure><p name="45a1" id="45a1" class="graf graf--p graf-after--figure">The reason why we can have higher bandwidth is because when c1 send a request to for B, c2 can send requests for maybe D to another directory without worrying about the bandwidth. Therefore, the bandwidth for requests will be enlarged.</p><p name="5ab3" id="5ab3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Coherence Misses</strong></p><p name="be7d" id="be7d" class="graf graf--p graf-after--p">We have talked about the cache misses and we have known that there will be 3Cs of cache misses. These include,</p><ul class="postList"><li name="739e" id="739e" class="graf graf--li graf-after--p">compulsory misses</li><li name="90f7" id="90f7" class="graf graf--li graf-after--li">conflict misses</li><li name="df83" id="df83" class="graf graf--li graf-after--li">capacity misses</li></ul><p name="731f" id="731f" class="graf graf--p graf-after--li">However, now we can have the another kind of cache miss, which is called the <strong class="markup--strong markup--p-strong">coherence misses</strong>. The coherence miss happens because we invalidate some blocks in the cache to maintain the cache coherence. There are actually two kinds of coherence misses,</p><ul class="postList"><li name="5df8" id="5df8" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">True sharing coherence misses</strong>: the cache misses we have seen so far, which happens when different cores accessing the same data. For example, let’s suppose we have the following two blocks in a 4-core system,</li></ul><figure name="1542" id="1542" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*csVIFZ-VlwrBq8CtmrSwLg.png" data-width="1338" data-height="150" src="https://cdn-images-1.medium.com/max/800/1*csVIFZ-VlwrBq8CtmrSwLg.png"></figure><p name="5fcf" id="5fcf" class="graf graf--p graf-after--figure">Then if we do,</p><pre name="981f" id="981f" class="graf graf--pre graf-after--p">c0: RD X<br>c1: WR X<br>c0: RD X     &lt;- True sharing coherence misses</pre><p name="c327" id="c327" class="graf graf--p graf-after--pre">Then we will have a true sharing coherence misses in the third operation.</p><ul class="postList"><li name="3567" id="3567" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">False sharing coherence misses</strong>: this occurs when different cores access <strong class="markup--strong markup--li-strong">different data</strong> so that there should not be any coherence messages because of that, except for these data items are in the same block. For example, if we still have the following two blocks in a 4-core system,</li></ul><figure name="19d2" id="19d2" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*csVIFZ-VlwrBq8CtmrSwLg.png" data-width="1338" data-height="150" src="https://cdn-images-1.medium.com/max/800/1*csVIFZ-VlwrBq8CtmrSwLg.png"></figure><p name="f291" id="f291" class="graf graf--p graf-after--figure">Then if we do,</p><pre name="2341" id="2341" class="graf graf--pre graf-after--p">c0: RD X<br>c1: WR W<br>c0: RD X     &lt;- False sharing coherence misses</pre><p name="ce0b" id="ce0b" class="graf graf--p graf-after--pre graf--trailing">Then we will have false sharing coherence misses in the third operation.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/259a225572a3"><time class="dt-published" datetime="2021-04-07T11:15:02.734Z">April 7, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/high-performance-computer-architecture-30-introduction-to-cache-coherence-coherence-protocols-259a225572a3" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>