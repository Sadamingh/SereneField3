<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>High-Performance Computer Architecture 28 | Introduction to Fault Tolerance, Memory and Storage …</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">High-Performance Computer Architecture 28 | Introduction to Fault Tolerance, Memory and Storage …</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: High-Performance Computer Architecture
</section>
<section data-field="body" class="e-content">
<section name="4e91" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1a65" id="1a65" class="graf graf--h3 graf--leading graf--title">High-Performance Computer Architecture 28 |<strong class="markup--strong markup--h3-strong"> Introduction to Fault Tolerance, Memory and Storage Fault Tolerance, RAID</strong></h3><figure name="c02c" id="c02c" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*Muf8-pVLIKV8js73.png" data-width="1446" data-height="864" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*Muf8-pVLIKV8js73.png"></figure><p name="6a94" id="6a94" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">1. Introduction to Fault Tolerance</strong></p><p name="3c3d" id="3c3d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of the Dependability</strong></p><p name="f9d4" id="f9d4" class="graf graf--p graf-after--p">Dependability means the quality of a delivered service that justifies relying on the system to provide the service. So a dependable system is the one that provides the service in a way that makes us expect it to do provide the correct service.</p><p name="c6aa" id="c6aa" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Two Kinds of Servies</strong></p><p name="77fc" id="77fc" class="graf graf--p graf-after--p">The service we have talked about actually has two definitions.</p><ul class="postList"><li name="e7be" id="e7be" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Specified service</strong>: what the behavior of the system should look like (the <strong class="markup--strong markup--li-strong">expected behavior</strong>)</li><li name="8d9d" id="8d9d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Delivered service</strong>: the <strong class="markup--strong markup--li-strong">actual behavior </strong>that the system provides</li></ul><p name="5d17" id="5d17" class="graf graf--p graf-after--li">We can also conclude that dependability is about could we say that the specified service matches the delivered service.</p><p name="6fff" id="6fff" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) System Components</strong></p><p name="2edb" id="2edb" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">system components</strong> (aka. <strong class="markup--strong markup--p-strong">modules</strong>) are some largish components in our computer like the processor, the memory, and so on. For each of these modules, we can specify some behavior that it should ideally get. So when we talk about things that make things not be so dependable, we are really talking about modules deviating from the specified behavior and thus, the delivered service no longer matches the specified services.</p><p name="f999" id="f999" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) The Definition of Fault</strong></p><p name="7939" id="7939" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">faults</strong> (also called the <strong class="markup--strong markup--p-strong">latent error</strong>) happen when the <strong class="markup--strong markup--p-strong">modules</strong> deviate from the specified behaviors. The most common fault is a programming mistake. For example, suppose we have an <code class="markup--code markup--p-code">add</code> function that works really well except for the case <code class="markup--code markup--p-code">3+5=7</code>. This fault may not occur at the beginning, but it is just a matter of time.</p><p name="3712" id="3712" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) The Definition of Error</strong></p><p name="1c81" id="1c81" class="graf graf--p graf-after--p">The error (also called <strong class="markup--strong markup--p-strong">activated fault</strong>, <strong class="markup--strong markup--p-strong">effective error</strong>) happens when the actual <strong class="markup--strong markup--p-strong">behavior within the system</strong> differs from the specified behavior. For example, we call the <code class="markup--code markup--p-code">add</code> function that we have talked about and add 3 to 5. Result 7 is put in some other variable.</p><p name="322b" id="322b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) The Definition of Failure</strong></p><p name="63ed" id="63ed" class="graf graf--p graf-after--p">The failure occurs when the <strong class="markup--strong markup--p-strong">system</strong> deviated from the specified behavior. For example, when we want to schedule a meeting time at 8 but we finally scheduled the time to 7 because of the error in the function we have talked about.</p><p name="e507" id="e507" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Difference between Fault, Error, and Failure</strong></p><p name="1119" id="1119" class="graf graf--p graf-after--p">We should keep in mind that all not all faults will become errors, and only those activated faults will become errors. Similarly, we can have an error sometimes, but not all the errors will become failures.</p><p name="d3fd" id="d3fd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) The Definition of Reliability</strong></p><p name="fc21" id="fc21" class="graf graf--p graf-after--p">There are some other properties in fault tolerance, and one of them is called <strong class="markup--strong markup--p-strong">reliability</strong>, which is something we can measure. In order to measure the reliability, we consider the system in one of the following two states,</p><ul class="postList"><li name="f4c9" id="f4c9" class="graf graf--li graf-after--p">Service accomplishment state (the normal state)</li><li name="ccec" id="ccec" class="graf graf--li graf-after--li">Service interruption (the service is not providing)</li></ul><p name="96b4" id="96b4" class="graf graf--p graf-after--li">The reliability can be defined by measuring the continuous service accomplishment states, and a typical measure is called <strong class="markup--strong markup--p-strong">mean time to failure</strong> (or <strong class="markup--strong markup--p-strong">MTTF</strong>). This is basically how long do we have service accomplishment before we get into the next service interruption.</p><p name="062d" id="062d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) The Definition of Availability</strong></p><p name="6730" id="6730" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">availability</strong> measures service accomplishment as a fraction of the overall time. So reliability measures how long do we get from the service accomplishment until the next failure, the availability measures what percentage of time we are in the service accomplishment state.</p><p name="5f86" id="5f86" class="graf graf--p graf-after--p">For the availability, we have to know the <strong class="markup--strong markup--p-strong">main time to repair</strong> (or <strong class="markup--strong markup--p-strong">MTTR</strong>). This is once the service has interrupted, how long does it last until it goes back to the service accomplishment state. So the availability can be calculated by the MTTF divided by the sum of MTTF plus MTTR,</p><figure name="a09f" id="a09f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PdJjuaZzbTE7Qb2VBska5Q.png" data-width="1256" data-height="92" src="https://cdn-images-1.medium.com/max/800/1*PdJjuaZzbTE7Qb2VBska5Q.png"></figure><p name="872f" id="872f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(10) Different Types of Faults</strong></p><p name="a361" id="a361" class="graf graf--p graf-after--p">We can classify the faults by different causes, these contain</p><ul class="postList"><li name="1d87" id="1d87" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Hardware (HW) faults</strong>: hardware fails to perform as designed</li><li name="c72f" id="c72f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Design faults</strong>: like the software bugs, or the hardware design mistakes (e.g. <a href="https://en.wikipedia.org/wiki/Pentium_FDIV_bug" data-href="https://en.wikipedia.org/wiki/Pentium_FDIV_bug" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">FDIV bugs</a>)</li><li name="40fe" id="40fe" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Operation faults</strong>: operator or the user’s mistakes. For example, an operator mistakenly shut down the service</li><li name="5787" id="5787" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Environmental faults</strong>: like the fire and the power failure</li></ul><p name="15dd" id="15dd" class="graf graf--p graf-after--li">Or we can also classify the faults by duration, which means how long do we have the fault condition,</p><ul class="postList"><li name="25c5" id="25c5" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Permanent faults</strong>: once we have it, it doesn’t get corrected</li><li name="a96f" id="a96f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Intermittent faults</strong>: last for a while but recurring (e.g. if we overclock the system)</li><li name="9744" id="9744" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Transient faults</strong>: last for a while and then go away</li></ul><p name="f16d" id="f16d" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(11) How to Improve Reliability and Availability?</strong></p><p name="a0b1" id="a0b1" class="graf graf--p graf-after--p">There are actually several things we can do to improve the reliability and the availability of the system. We can try to,</p><ul class="postList"><li name="6a66" id="6a66" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Avoid faults</strong> as much as we can: preventing faults from occurring</li><li name="9b3c" id="9b3c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Tolerance of some faults</strong>: prevent faults from being failures (e.g. Error-correcting code)</li><li name="4f53" id="4f53" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Speed up repair</strong>: which only affects availability.</li></ul><p name="1c0f" id="1c0f" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(12) Fault Tolerance Techniques</strong></p><p name="8bfe" id="8bfe" class="graf graf--p graf-after--p">In order to improve the reliability and the availability, we can use some fault tolerance techniques like,</p><ul class="postList"><li name="4e84" id="4e84" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Checkpoints</strong>: the checkpointing saves the state periodically, then if we detect the errors, we will stop the system and restore the state. This technique works well for many transient and intermittent faults. However, if the checkpoint takes too long to recover, this will be treated as a service interruption.</li><li name="0d45" id="0d45" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">2-way Redundancy</strong>: (also called <strong class="markup--strong markup--li-strong">dual-module redundancy</strong>, or <strong class="markup--strong markup--li-strong">DMR</strong>) we can also use the technique called the 2-way redundancy where two modules do the exact same work. After each service, we will compare the results of them and we will roll back if the results are different. This is an error detection technique and it should be associated with the recovery techniques like checkpointing.</li><li name="86c8" id="86c8" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">3-way Redundancy</strong>: (also called <strong class="markup--strong markup--li-strong">triple-module redundancy</strong>, or <strong class="markup--strong markup--li-strong">TMR</strong>) but with a 3-way redundancy, we can both detect and recover some faults. In this case, three modules are going to do the same work and they will vote for what the correct result should be. If one module is malfunctioning, then the other two will still produce the same result and that result will be elected as the correct result. Although the implementation of this design is expensive, it can actually tolerant any fault in 1 module.</li><li name="209f" id="209f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">N-way Redundancy</strong>: a more general redundancy approach is called an N-way redundancy. If N = 5, there will be 5 votes. If we have one wrong result in a vote, we can tolerant this fault and we can continue the normal operation. If we have 2 wrong results in a vote, although we will have no failure, we will <strong class="markup--strong markup--li-strong">abort the current mission</strong>. This is because that we are confirmed that 2 out of 5 modules must have some faults and if we continue, we may have one more module with faults, and then we can not safely land the space shuttle.</li></ul><p name="c6a6" id="c6a6" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Memory and Storage Fault Tolerance</strong></p><p name="5187" id="5187" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Overkilling Problem for Memory and Storage</strong></p><p name="5b00" id="5b00" class="graf graf--p graf-after--p">Now, let’s see some fault tolerance techniques for memory and storage. We could use DMR or TMR for memory and storage. But these techniques are considered to <strong class="markup--strong markup--p-strong">overkill</strong> for these devices because DMR and TMR are typically used for the hardware that does the computation. Thus, we have to figure out some better techniques for this problem.</p><p name="d12c" id="d12c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Definition of Codes</strong></p><p name="326e" id="326e" class="graf graf--p graf-after--p">The error detection technique developed only for the memory and the storage is called the error-correction code. The idea is that we can store some extra bits (we call it <strong class="markup--strong markup--p-strong">code</strong>) that allow us to perform fault detection and correction depending on the code.</p><p name="28c6" id="28c6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) The Definition of Parity Bit</strong></p><p name="87b2" id="87b2" class="graf graf--p graf-after--p">The parity bit is the simplest code that has only 1 bit, and it can be calculated by simply XOR all the data bits. When we do have parity and the fault flips only 1 bit, then the parity code will no longer match the data.</p><p name="cdcd" id="cdcd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) The Definition of Error-Correction Code (ECC)</strong></p><p name="7e64" id="7e64" class="graf graf--p graf-after--p">The error-correction codes are the more complicated ones with more than 1 bit. A typical ECC is called the <strong class="markup--strong markup--p-strong">single error correction, double error detection</strong> (aka. <strong class="markup--strong markup--p-strong">SECDED</strong>) code. This code will correct it if there’s only a 1-bit flip, and this code will also figure out that there is a fault if we have a 2-bit flip.</p><p name="ca72" id="ca72" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Reed-Solomon Error-Correction Code for Hardware</strong></p><p name="49b5" id="49b5" class="graf graf--p graf-after--p">This code is even fancier and it is used mainly by the hardware (e.g. disks). This code can detect and correct multiple bit errors and they are especially powerful when we have a streak of flipped bits, which happens when the magnetic head isolate the platter for a little bit while it is spinning and misses some of the bits. We are not going to take about this in detail.</p><p name="d561" id="d561" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. RAID</strong></p><p name="3b4d" id="3b4d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of Redundant Array of Independent Disks (RAID)</strong></p><p name="40ba" id="40ba" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">RAID</strong> is another technique family that we commonly used for error correction, which stands for <strong class="markup--strong markup--p-strong">redundant array of independent disks</strong>. The fact of the RAID is that we have several disks that play the role of only one disk in different ways. They can either pretend to be a larger disk, or they can pretend to be a more reliable disk, or they can pretend to be a disk that is both larger and more reliable than we have only one disk.</p><p name="5696" id="5696" class="graf graf--p graf-after--p">Each of the disks in a RAID model will still detect the error using codes so that we can know which and whether a disk has an error in the RAID scheme. So what we want from RAID is a <strong class="markup--strong markup--p-strong">better performance </strong>and we want normal read-write accomplishment even when there are <strong class="markup--strong markup--p-strong">bad sectors</strong> on disks or an <strong class="markup--strong markup--p-strong">entire disk fails</strong>.</p><p name="0db9" id="0db9" class="graf graf--p graf-after--p">Not all the goals we have mentioned will be implemented by only one RAID, so we have several RAIDs and we name them as RAID 0, RAID 1, RAID 2, etc.</p><p name="96b6" id="96b6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) RAID 0: Striping</strong></p><p name="c92c" id="c92c" class="graf graf--p graf-after--p">The RAID 0 uses a technique called striping to improve performance. If we have one disk, it will turn out to have track 0, track 1, track 2, … on that disk. The magnetic head will read through the whole track and then the data can be retrieved from the track. However, this can be <strong class="markup--strong markup--p-strong">inefficient</strong> because we can not read from a track and moving the head to another at the same time.</p><figure name="d464" id="d464" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tFu1bcaLiUvh5msbuA-new.png" data-width="1480" data-height="470" src="https://cdn-images-1.medium.com/max/800/1*tFu1bcaLiUvh5msbuA-new.png"></figure><p name="13f2" id="13f2" class="graf graf--p graf-after--figure">The RAID 0 deals with this problem by using a technique called striping. What it does is to write tracks on two different disks and treat them as a whole, larger disk. Now, these “track”s on different disks are called <strong class="markup--strong markup--p-strong">strips</strong> instead of the track to avoid confusion with the track on the physical disk. On average, the performance (<strong class="markup--strong markup--p-strong">throughput</strong>) will be <strong class="markup--strong markup--p-strong">twice</strong> because we can read from one disk and move the head of another disk, and we can also read simultaneously from both of the disks at the same time. When the first read finishes, we can immediately read from the second disk without any latency for moving the heads.</p><figure name="698a" id="698a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*QId4P8KtQcJSJQzP2VTWKw.png" data-width="1480" data-height="470" src="https://cdn-images-1.medium.com/max/800/1*QId4P8KtQcJSJQzP2VTWKw.png"></figure><p name="07ac" id="07ac" class="graf graf--p graf-after--figure">However, the reliability of the RAID 0 is getting worse. Let’s assume <em class="markup--em markup--p-em">f</em> is the <strong class="markup--strong markup--p-strong">failure rate</strong> for a single disk, which is defined by the number of failures per disk per second. Obviously, this number will be extremely small. Usually, we can assume that the failure rate is constant over time. So the MTTF of a single disk can be calculated by,</p><figure name="4b0d" id="4b0d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*t8ADChuFv6DakRlH_1oUfw.png" data-width="1142" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*t8ADChuFv6DakRlH_1oUfw.png"></figure><p name="f990" id="f990" class="graf graf--p graf-after--figure">When we have N disks in RAID 0, the failure rate for each disk will be f and in general, we will have a failure rate of N<em class="markup--em markup--p-em">f</em>, so the MTTF, in this case, would be,</p><figure name="49f6" id="49f6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*R7GJGUyce2AMCVlhWeqQSg.png" data-width="1142" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*R7GJGUyce2AMCVlhWeqQSg.png"></figure><p name="95dd" id="95dd" class="graf graf--p graf-after--figure">As a result, we can find out that the MTTF with N disks in RAID 0 is N times smaller than the MTTF with a single disk. When N is getting larger and larger, we will have a really small MTTF limit to 0, and the machine is not reliable anymore.</p><p name="c651" id="c651" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) RAID 1: Mirroring</strong></p><p name="289b" id="289b" class="graf graf--p graf-after--p">The next RAID technique is RAID 1 and it uses a technique called mirroring to improve reliability. We will have a copy of the data on another disk, so this means that each disk will have the same write. So each disk does exactly the <strong class="markup--strong markup--p-strong">same writing</strong> that they will be doing.</p><p name="765e" id="765e" class="graf graf--p graf-after--p">However, when we read from the disk, we will read from only one disk because both of the two disks contain the same data. Now, if there is a bad sector on one disk, we can just read the same data from the other disk. Or if we have an entire disk fail, we are also left with a disk that still has all the data we can read. We don&#39;t compare these to disks to detect errors because the ECC is already assigned to each disk. So that we can just read from the correct disk.</p><p name="d68c" id="d68c" class="graf graf--p graf-after--p">Now let’s talk about the reliability problem related to RAID 1. Suppose the RAID 1 has 2 disks and the failure rate is <em class="markup--em markup--p-em">f</em>. Then the MTTF for the entire 2 disks (one for the copy) as we have discussed with the same content should be,</p><figure name="db5f" id="db5f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8F4vni-CsE6-41ge34ccHg.png" data-width="1142" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*8F4vni-CsE6-41ge34ccHg.png"></figure><p name="f94d" id="f94d" class="graf graf--p graf-after--figure">However, after the first disk fails, we are going to have an MTTF of a single disk, which is exactly,</p><figure name="c661" id="c661" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NqZwhiAG8XbBaNE1DzFYkQ.png" data-width="1142" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*NqZwhiAG8XbBaNE1DzFYkQ.png"></figure><p name="31ea" id="31ea" class="graf graf--p graf-after--figure">Therefore, the overall MTTF of our system should be,</p><figure name="7ad6" id="7ad6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RIURkHYdpY-JL5zDpWSBUQ.png" data-width="1142" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*RIURkHYdpY-JL5zDpWSBUQ.png"></figure><p name="0dfe" id="0dfe" class="graf graf--p graf-after--figure">In the example above, we have assumed that no disk is replaced after we detect a fault, which is actually not the case in reality. Well, in reality, we will definitely replace the fault disk immediately and then there will not be a problem.</p><p name="2cf4" id="2cf4" class="graf graf--p graf-after--p">So how can we calculate the MTTF when considering the fact that all the fault disks are replaced properly? The answer is that we have to consider the repairing time (MTTR). Let’s reconsider the 2-disk RAID 1 system. In the beginning, the MTTF of the entire system would be,</p><figure name="941e" id="941e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ejt0NlMNylzjvcJkn48IHg.png" data-width="1142" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*Ejt0NlMNylzjvcJkn48IHg.png"></figure><p name="c019" id="c019" class="graf graf--p graf-after--figure">After the first disk fails, we have to replace it with a new disk so that it will be working later. During the repairing time, we will only have one disk, but because the MTTR (for days) is relatively small compared with the MTTF (for years), we can ignore the process of a single disk and just consider it as two disks working all the time.</p><p name="cc92" id="cc92" class="graf graf--p graf-after--p">Then the overall MTTF will be MTTf_2 times the times of repairings until we can not finish one in the MTTF time. So in general, the result will be,</p><figure name="511a" id="511a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ny0lPsxCi_x90So0BS4YfQ.png" data-width="1142" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*ny0lPsxCi_x90So0BS4YfQ.png"></figure><p name="4368" id="4368" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) RAID 2&amp;3: Not going to talk about</strong></p><p name="6278" id="6278" class="graf graf--p graf-after--p">We are not going to talk about RAID 2 and RAID 3 because they are rarely used today.</p><p name="8f7a" id="8f7a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) RAID 4: Block Interleaved Parity</strong></p><p name="b18b" id="b18b" class="graf graf--p graf-after--p">The RAID 4 uses a technique called <strong class="markup--strong markup--p-strong">block interleaved parity</strong> (aka. <strong class="markup--strong markup--p-strong">BIP</strong>). The content of this technique is similar to RAID 0 which separates different tracks as strips on different disks. The only thing that is different is that it adds an additional disk which is used for keeping the XOR results of strips as <strong class="markup--strong markup--p-strong">parities</strong>.</p><figure name="a35f" id="a35f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*g_LHjFg1_ArD1b7D9zj54w.png" data-width="1418" data-height="228" src="https://cdn-images-1.medium.com/max/800/1*g_LHjFg1_ArD1b7D9zj54w.png"></figure><p name="50a4" id="50a4" class="graf graf--p graf-after--figure">So when we write to the disk, we are actually writing to both the stripe and the parity. When we read from the disk, we are actually reading from only the corresponding disk.</p><p name="346f" id="346f" class="graf graf--p graf-after--p">Now, let’s see the overall performance of RAID 4. The throughput of reading from RAID 4 is N-1 times larger than the throughput of a single disk because we can read from N-1 disks simultaneously. However, the writing throughput is much worse than that because we have to write to both the corresponding disk and the parity disk. As a result, the writing throughput will be halved. This is also the reason why we need RAID 5.</p><p name="55dc" id="55dc" class="graf graf--p graf-after--p">In fact, the RAID 4 write is not as simple as that, we also have to consider how we can update the parities. Let’s see if we have a new stripe value for a 5-disk RAID 4 system (including 1 as the parity disk), one idea for generating the parity is that we simply read from the other 3 disks and then conduct XOR calculations. However, in this process, we have to conduct 3 reads (from the other 3 disks) and 2 writes (write to both the corresponding data disk and the parity disk). You can see that this is such a bad idea.</p><p name="62d9" id="62d9" class="graf graf--p graf-after--p">What can we do to reduce the reads and writes for RAID 4 is that we will not read from the other disks. What we can do is that we first read from the old data we would like to write, then we conduct an XOR between the old data and the new data. This XOR helps us finding the flipped bits between these two data disks. Then we will conduct an XOR between the flipped bits and the parity. Before we do so, we have to read from the parity first. After we calculate the result, we will write it back to the parity.</p><figure name="e6ec" id="e6ec" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YNjdUF-c_VrzsPrkcoUvsg.png" data-width="1466" data-height="398" src="https://cdn-images-1.medium.com/max/800/1*YNjdUF-c_VrzsPrkcoUvsg.png"></figure><p name="efbc" id="efbc" class="graf graf--p graf-after--figure">The reliability of the RAID 4. Again, all the disks are okay at the beginning until the first fault.</p><figure name="5b54" id="5b54" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Mb6Vq8cceG-PLPX0wezqPA.png" data-width="1044" data-height="86" src="https://cdn-images-1.medium.com/max/800/1*Mb6Vq8cceG-PLPX0wezqPA.png"></figure><p name="5b4e" id="5b4e" class="graf graf--p graf-after--figure">Then the fault appears in some disk and then we are willing to replace the disk with that fault. Similar to the last example, we can have an overall MTTF of</p><figure name="0e14" id="0e14" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9lP9uQ9hgrraGSXvNFmAvw.png" data-width="1044" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*9lP9uQ9hgrraGSXvNFmAvw.png"></figure><p name="6f30" id="6f30" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(6) RAID 5: Distributed Block Interleaved Parity</strong></p><p name="28d2" id="28d2" class="graf graf--p graf-after--p">The RAID 5 is similar to RAID 4, but the parties are distributed among all the disks.</p><figure name="2522" id="2522" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6AE5BxWsOGRO2kKRkfPX6w.png" data-width="1396" data-height="216" src="https://cdn-images-1.medium.com/max/800/1*6AE5BxWsOGRO2kKRkfPX6w.png"></figure><p name="7cfe" id="7cfe" class="graf graf--p graf-after--figure">Because we can now read data from all of these N disks, we can confirm that the reading throughput will be N times the throughput of a single disk. A single write is also going to result in 4 accesses (2 reads and 2 writes) per write. But because the write is also distributed among 4 disks, we can have N/4 times the throughput of a single disk.</p><p name="b8c4" id="b8c4" class="graf graf--p graf-after--p">The reliability of RAID 5 is exactly the same as RAID 4.</p><p name="be29" id="be29" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) RAID 6</strong></p><p name="a82d" id="a82d" class="graf graf--p graf-after--p">There is actually a RAID 6 in addition to the RAID 5. And what is different is that the RAID 6 adds another parity for each group of strips and this newly added check block is used for recovering when we have 2 disks failed. When there is only 1 disk failed, we can use the original parity for recovery. And when there are 2 disks failed, we can solve some equations of the original parity and the check block for recovery.</p><p name="299b" id="299b" class="graf graf--p graf-after--p">RAID 6 has twice the overall overheads to RAID 5 and we have to pay this for improving the reliability. There are more overheads per write because we have to write to both the parities and the check blocks. So now there are 6 accesses per write including 3 reads and 3 writes.</p><p name="c902" id="c902" class="graf graf--p graf-after--p">The RAID 6 is only used for the situation that we have a disk fails, and then we expect another fails before we replace the first disk. However, if all the faults are independent, it is unlikely that we have another disk fails when we are replacing the fault disk. So the RAID 6 is considered to be an overkill.</p><p name="e105" id="e105" class="graf graf--p graf-after--p graf--trailing">In fact, there are some cases when the failures are dependent. For example, we mistakenly replaced a correct disk and the fault disk still remains. In this situation, we will probability have 2 fault disks and if we have a RAID 6 system, we are able to recover from it.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/f1cf47e6dcf9"><time class="dt-published" datetime="2021-04-04T02:37:59.156Z">April 4, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/high-performance-computer-architecture-28-introduction-to-fault-tolerance-memory-and-storage-f1cf47e6dcf9" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>