<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>High-Performance Computer Architecture 22 | Locality Principle, Cache, Types of Caches, LRU, and…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">High-Performance Computer Architecture 22 | Locality Principle, Cache, Types of Caches, LRU, and…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: High-Performance Computer Architecture
</section>
<section data-field="body" class="e-content">
<section name="22a8" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="434c" id="434c" class="graf graf--h3 graf--leading graf--title">High-Performance Computer Architecture 22 | <strong class="markup--strong markup--h3-strong">Locality Principle, Cache, Types of Caches, LRU, and Write Policies</strong></h3><figure name="a633" id="a633" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*Ij6dzQV67aLjo2lS.png" data-width="1446" data-height="864" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*Ij6dzQV67aLjo2lS.png"></figure><ol class="postList"><li name="bf72" id="bf72" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Locality Principle</strong></li></ol><p name="e95f" id="e95f" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) The Definition of Locality Principle</strong></p><p name="b535" id="b535" class="graf graf--p graf-after--p">Things that will happen soon are likely to be close to things that just happened. Based on this principle, we have already used this principle for branch prediction, and now, it is time to be used for caches</p><p name="012a" id="012a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Locality Principle: Memory References</strong></p><p name="55d0" id="55d0" class="graf graf--p graf-after--p">If we know that the processor has accessed an address recently, the locality principle says that,</p><ul class="postList"><li name="e75b" id="e75b" class="graf graf--li graf-after--p">the processor is likely to access <strong class="markup--strong markup--li-strong">the same address</strong> again in the near future</li><li name="76b6" id="76b6" class="graf graf--li graf-after--li">the processor is likely to access <strong class="markup--strong markup--li-strong">addresses close to this address</strong></li></ul><p name="c9a3" id="c9a3" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) The Definition of Temporal Locality</strong></p><p name="a30c" id="a30c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Temporal locality</strong> means that once we access an address, we are likely to access the same address again. In the example above, the processor is likely to access the same address again in the near future is actually a temporal locality.</p><p name="fdba" id="fdba" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) The Definition of Spatial Locality</strong></p><p name="3830" id="3830" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Spatial locality</strong> means that if we access an address, we are likely to access nearby addresses so on. In the example above, the processor is likely to access addresses close to this address is actually a spatial locality.</p><p name="2e67" id="2e67" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Locality Example: Finding Information</strong></p><p name="1de6" id="1de6" class="graf graf--p graf-after--p">Suppose we would like to find the information from the library, and you can imagine that the library is quite large, so it will be quite slow for us to find the book we need if we do a one-by-one search.</p><p name="9fe7" id="9fe7" class="graf graf--p graf-after--p">If every time we go to the library, we would like to find a book about the locality principle, we will end up searching at the same place. This happens to be a <strong class="markup--strong markup--p-strong">temporal locality</strong>.</p><p name="d663" id="d663" class="graf graf--p graf-after--p">However, we may also want to search for some books about computer architecture. So what we can do is to find the books close to the books about the locality principle. And this is what we called a <strong class="markup--strong markup--p-strong">spatial locality</strong>.</p><p name="e3e6" id="e3e6" class="graf graf--p graf-after--p">So now, a student will have three options on that,</p><ul class="postList"><li name="3ec1" id="3ec1" class="graf graf--li graf-after--p">Go to the library, find the info, go back home</li><li name="75a7" id="75a7" class="graf graf--li graf-after--li">Go to the library, borrow the book, go back home</li><li name="0cce" id="0cce" class="graf graf--li graf-after--li">Go to the library, borrow all the books we may need in the future, and build a smaller library at home</li></ul><p name="dc0d" id="dc0d" class="graf graf--p graf-after--li">The first option is quite time-consuming and we do not benefit from the locality. The second option is reasonable because it does exploit temporal locality and spatial locality, and we eliminate the time we need for searching the same thing many times. This is why most people choose this option. Although the third option saves our future time on the trip, it is very expensive for us. What’s more, we also have to search among many books, finding them on the shelves and so on.</p><p name="d23b" id="d23b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) From Locality to Cache</strong></p><p name="4f16" id="4f16" class="graf graf--p graf-after--p">Like in the library example, we would like to borrow some books we need and close to us, but we won’t borrow too many books because this can also be slow and expensive.</p><p name="9057" id="9057" class="graf graf--p graf-after--p">So for the computer architecture, we would like to have a small amount of data compared to the memory, but it should keep the information that we are really interested in. So that small repository of information is called a <strong class="markup--strong markup--p-strong">cache</strong> when we are talking about memory accesses.</p><p name="de3b" id="de3b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Cache</strong></p><p name="fc80" id="fc80" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of Cache</strong></p><p name="4f62" id="4f62" class="graf graf--p graf-after--p">The cache is a <strong class="markup--strong markup--p-strong">small memory inside a processor</strong> where we are trying to find the data before we go to the main memory.</p><p name="579c" id="579c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Properties of Cache</strong></p><p name="7713" id="7713" class="graf graf--p graf-after--p">Let’s now take some time to think about what we need for a cache,</p><ul class="postList"><li name="0a34" id="0a34" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Fast</strong>: means that the cache has to be small</li><li name="4730" id="4730" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Not everything will fit</strong>: this is because the cache is small</li><li name="bae0" id="bae0" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Access states</strong>: it is possible that the data we want is in/not in the cache, so this brings us to two different access states</li></ul><p name="837b" id="837b" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Two States for Cache</strong></p><p name="e3ef" id="e3ef" class="graf graf--p graf-after--p">Basically, we have two states for each cache,</p><ul class="postList"><li name="e8d9" id="e8d9" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Cache Hit (Hot Cache)</strong>: This means that the data we want to access is already in the cache (quick)</li><li name="2741" id="2741" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Cache Miss (Cold Cache)</strong>: This means that the data we want to access is not in the cache (slow)</li></ul><p name="bb66" id="bb66" class="graf graf--p graf-after--li">When we have a cache miss, we will copy this location to the cache so that the next time we access it, we will hopefully benefit from the locality and have a cache hit. Based on the locality principle, we can know that the cache miss happens rarely and it is reasonable for us to design caches.</p><p name="ac67" id="ac67" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Cache Performance</strong></p><p name="54c4" id="54c4" class="graf graf--p graf-after--p">There are several metrics that can be used to measure the performance of a cache,</p><ul class="postList"><li name="dcc1" id="dcc1" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Average Memory Access Time (AMAT)</strong>: the access time to memory as seen by the processor. AMAT can be calculated by,</li></ul><figure name="3286" id="3286" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*pS3noiIM6RZ9BzFd0yvR7Q.png" data-width="1356" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*pS3noiIM6RZ9BzFd0yvR7Q.png"></figure><p name="e77b" id="e77b" class="graf graf--p graf-after--figure">The lower the AMAT, the better the cache. To lower the AMAT, we have to reduce the hit time (by using a small &amp; fast cache), reduce the miss rate (by using large and/or smart cache).</p><p name="627f" id="627f" class="graf graf--p graf-after--p">The miss penalty is also called the <strong class="markup--strong markup--p-strong">main memory access time (MMAT)</strong>, which can be very large compared with the hit time.</p><p name="454a" id="454a" class="graf graf--p graf-after--p">Thus, if we want to improve the performance of our cache, we have to make a balance between the <strong class="markup--strong markup--p-strong">hit time</strong> and the <strong class="markup--strong markup--p-strong">miss rate</strong>. Although some caches can be extremely small and fast, we have to pay the cost of a high miss rate because of cache misses.</p><ul class="postList"><li name="229e" id="229e" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Miss Time</strong>: the miss time means the overall time it takes if we have a cache miss and it is defined by,</li></ul><figure name="54e3" id="54e3" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*Ry5_WSZhx4p6uoVrn7Eixw.png" data-width="1262" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*Ry5_WSZhx4p6uoVrn7Eixw.png"></figure><p name="1de4" id="1de4" class="graf graf--p graf-after--figure">Miss time is also the time we have to pay for a cache miss.</p><p name="0bab" id="0bab" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) General AMAT</strong></p><p name="4fdf" id="4fdf" class="graf graf--p graf-after--p">Based on the previous average memory access time (AMAT) definition and the miss time we have already talked about, we can have the following expression as a <strong class="markup--strong markup--p-strong">general AMAT expression</strong>,</p><figure name="c0f9" id="c0f9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5wXoMs6a3HbRMaB-QHBf1g.png" data-width="1262" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*5wXoMs6a3HbRMaB-QHBf1g.png"></figure><p name="2096" id="2096" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(6) Cache Size</strong></p><p name="7755" id="7755" class="graf graf--p graf-after--p">We have seen that we want to keep our cache small and fast, but we also have to keep in mind that the cache can not be too small because then it keeps very little stuff. However, in the real processors, we do not have only one cache. The cache that directly services the read/write requests from the processor is always called an <strong class="markup--strong markup--p-strong">L1 cache</strong> or a <strong class="markup--strong markup--p-strong">level-1 cache</strong>. This is what we are going to discuss for its size.</p><p name="dbc0" id="dbc0" class="graf graf--p graf-after--p">By the way, it can be quite complicated if we have a cache miss in the L1 cache, and then instead of going directly to the main memory, we will go to some other caches. We are going to talk about this later.</p><p name="a722" id="a722" class="graf graf--p graf-after--p">So how big is the L1 cache? In recent processors, the typical size of the L1 cache has been <strong class="markup--strong markup--p-strong">16 KB to 64 KB</strong>, and this is large enough to get about a 90% hit rate. Moreover, it is also smaller enough to have a hit time of only 1~3 cycles.</p><p name="be8f" id="be8f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Cache Organization</strong></p><p name="c639" id="c639" class="graf graf--p graf-after--p">Let’s discuss how the cache is organized so as to determine whether we have a cache hit or a cache miss. Conceptually, the cache can be treated as a table with some bits that we take from the others of the data. In each line/entry of a cache, some bits are going to tell us whether we have a hit or not, and there are also some other bits that are really the data we want if we do have this data in the cache. The cache will be indexed with some bits that we have taken from the address of the data.</p><figure name="a8be" id="a8be" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6aVCB4tYCY1M70vE8MiW5A.png" data-width="1440" data-height="474" src="https://cdn-images-1.medium.com/max/800/1*6aVCB4tYCY1M70vE8MiW5A.png"></figure><p name="786a" id="786a" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(8) Cache Block Size</strong></p><p name="cdf7" id="cdf7" class="graf graf--p graf-after--p">How much data we have in each entry is called the <strong class="markup--strong markup--p-strong">block size</strong> (aka. <strong class="markup--strong markup--p-strong">line size</strong>) because each entry in the cache is also called a <strong class="markup--strong markup--p-strong">block</strong> (aka. <strong class="markup--strong markup--p-strong">line</strong>). The block size is defined as the number of <strong class="markup--strong markup--p-strong">bytes</strong> are in each of these entries.</p><p name="7e1f" id="7e1f" class="graf graf--p graf-after--p">Imagine a situation that we have a block size of <strong class="markup--strong markup--p-strong">1 byte</strong>. This means that the data that can be stored in each of the entries is 1 byte. Even though this works for most of the immediate values, we have to notice that for some load/store operations, what we want to look up in the cache are addresses that will take up 4 bytes each. Thus, one entry won’t be enough to store this data and we should look up 4 different lines in the cache. So the block size won’t be that small.</p><p name="cdd9" id="cdd9" class="graf graf--p graf-after--p">Imagine another situation that we have <strong class="markup--strong markup--p-strong">1 kb</strong> as the block size. In this case, we will fetch a lot of data from the memory every time when we have a miss. As a result, if there is not enough locality, a lot of data will not be used. So we are occupying spaces in our cache. Remember that the modern caches have only 16~64 kb, so this block will also consuming a significant part of the cache and much of the data that we are passing is not going to be used in the future.</p><p name="52df" id="52df" class="graf graf--p graf-after--p">Therefore, for the L1 cache, we don’t want the block size to become too large (like 1 kb), and we also don’t want the block size to become over small (like 1 byte). So block size of <strong class="markup--strong markup--p-strong">32 bytes to 128 bytes</strong> are made for the modern caches because we balance between the use of spatial locality, and not depend too much on the spatial locality since most programs don’t have that much.</p><p name="a19f" id="a19f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) Aligned Block Starting Address</strong></p><p name="db85" id="db85" class="graf graf--p graf-after--p">For all reasonable caches, we will only consider <strong class="markup--strong markup--p-strong">aligned blocks</strong> which means that we can not fetch any 64-byte blocks from the memory to the cache. Instead, we need to fetch the 64-byte block that contains the data and has its block aligned. Let’s see why.</p><p name="9641" id="9641" class="graf graf--p graf-after--p">Suppose we have a block starting address that starts from anywhere in the memory. Thus, in the cache, we may probably have the first entry ranges from address 0~63, and the second one ranges from 1~64. Because the blocks are not aligned, you may find these two entries have an overlapped part from the address 1~63, and this can be useless.</p><p name="0175" id="0175" class="graf graf--p graf-after--p">The term <strong class="markup--strong markup--p-strong">aligned</strong> means that the blocks should not have data that overlap with each other. For example, if the first entry ranges from 0 to 63, then the second one should be 64～127.</p><figure name="c996" id="c996" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jm2ze6yCQNfDaFvPQ4_ETA.png" data-width="1724" data-height="292" src="https://cdn-images-1.medium.com/max/800/1*jm2ze6yCQNfDaFvPQ4_ETA.png"></figure><p name="3fb1" id="3fb1" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(10) From Memory to Cache</strong></p><p name="8cae" id="8cae" class="graf graf--p graf-after--p">Now, let’s see how can the data in the memory be fetched into the cache. Suppose we have a computer that each address has 1 byte, and we also have a cache whose cache block size is 16 bytes. Then, 16 addresses in the memory will be combined as a block (e.g. block 1 and block 2 in the following diagram), and then the data will be fetched in a line of the cache.</p><figure name="7ad6" id="7ad6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*sIAS05q8vXOtrT0D7Z5iTQ.png" data-width="1724" data-height="450" src="https://cdn-images-1.medium.com/max/800/1*sIAS05q8vXOtrT0D7Z5iTQ.png"></figure><p name="9927" id="9927" class="graf graf--p graf-after--figure">Note that we can always confirm that the block size is always equal to the line size.</p><p name="1c6e" id="1c6e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(11) Block Offset</strong></p><p name="5253" id="5253" class="graf graf--p graf-after--p">Suppose we have a cache with a block size of <strong class="markup--strong markup--p-strong">16 bytes </strong>(thus we need at least 4 bit to index these 16 bytes), then as we have discussed above, 16 addresses in the memory will be mapped to the same block. When the processor wants to get the data, it will produce a 32-bit address, which represents the location that the processor wants to find in the cache. The <strong class="markup--strong markup--p-strong">0 to 3 bits</strong> of this address tell us which part of this block should we read and the remaining bits (<strong class="markup--strong markup--p-strong">4 to 31</strong>) tells us which block we are trying to find.</p><p name="d07e" id="d07e" class="graf graf--p graf-after--p">The bits that tell us where in the block are we going to find the data are called the <strong class="markup--strong markup--p-strong">block offset</strong> and the bits that tell us which block are we interested in are called the <strong class="markup--strong markup--p-strong">block number</strong>.</p><figure name="a43c" id="a43c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*e2bwAaD_iID9BPefSx2B7Q.png" data-width="1724" data-height="450" src="https://cdn-images-1.medium.com/max/800/1*e2bwAaD_iID9BPefSx2B7Q.png"></figure><p name="aebf" id="aebf" class="graf graf--p graf-after--figure">So what we do is that,</p><ul class="postList"><li name="d10c" id="d10c" class="graf graf--li graf-after--p">access the cache</li><li name="e437" id="e437" class="graf graf--li graf-after--li">find the block we need according to the <strong class="markup--strong markup--li-strong">block number</strong></li><li name="c2a8" id="c2a8" class="graf graf--li graf-after--li">get the right data from the block by the <strong class="markup--strong markup--li-strong">block offset</strong></li></ul><p name="3832" id="3832" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(12) Block Offset: Example</strong></p><p name="ddf4" id="ddf4" class="graf graf--p graf-after--p">Suppose we have a cache that has a block size of 32 bytes, and we have a 16-bit address used to lookup data from this cache,</p><pre name="853c" id="853c" class="graf graf--pre graf-after--p">1111 0000 1010 0101</pre><p name="390f" id="390f" class="graf graf--p graf-after--pre">Then the block number should be,</p><pre name="abce" id="abce" class="graf graf--pre graf-after--p">1111 0000 101</pre><p name="4f1e" id="4f1e" class="graf graf--p graf-after--pre">The block offset should be,</p><pre name="10a3" id="10a3" class="graf graf--pre graf-after--p">0 0101</pre><p name="dcd0" id="dcd0" class="graf graf--p graf-after--pre">This is because we need at least 5 bits for indexing a 32-byte cache block.</p><p name="5922" id="5922" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(13) Cache Tags</strong></p><p name="6992" id="6992" class="graf graf--p graf-after--p">We have known that the processor can use the block number to find which block to access. So how does this implement? The answer is by using <strong class="markup--strong markup--p-strong">tags</strong>. The tags can be treated as the unique ID of the blocks in the cache. If the block number equal to the tag for a block, then we can confirm that we have a cache hit, and then the block offset can be used to find which data to use.</p><figure name="79ec" id="79ec" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*cH17xe6pQPfMuYH9j0dDbg.png" data-width="1852" data-height="532" src="https://cdn-images-1.medium.com/max/800/1*cH17xe6pQPfMuYH9j0dDbg.png"></figure><p name="82e5" id="82e5" class="graf graf--p graf-after--figure">Note that the tags can be part of the block number, and this means that the block number doesn’t have to be equivalent to the tag. So in a more general case, we might have the following diagram,</p><figure name="ad70" id="ad70" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WJ5YhEZWwcpwia5Eoqj1iQ.png" data-width="1852" data-height="608" src="https://cdn-images-1.medium.com/max/800/1*WJ5YhEZWwcpwia5Eoqj1iQ.png"></figure><p name="c617" id="c617" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(14) Valid Bit</strong></p><p name="c17a" id="c17a" class="graf graf--p graf-after--p">Let’s think about the following situation. Suppose we have initialized a program, and after that, all the tags and cache blocks contain some garbage values. If we have a tag with its initial value as,</p><pre name="f112" id="f112" class="graf graf--pre graf-after--p">00 0000</pre><p name="e968" id="e968" class="graf graf--p graf-after--pre">And there is a block number that contains this tag, and the part of it used for indexing is also this. Then, the processor will mistakenly think that we have a cache hit and starts to read some <strong class="markup--strong markup--p-strong">garbage values</strong> from the corresponding cache block.</p><p name="ccd6" id="ccd6" class="graf graf--p graf-after--p">In order to deal with this problem, what we do is to add an additional bit of state to the cache. So before we check the equality of the block number and the tag, we have to check for the valid bit. Initially, all the valid bits will be set to 0, and this means that even though the block number is equal to the tag, it will not be a cache hit.</p><figure name="ac1c" id="ac1c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*x4zBBc0gEuhEQBn7p7CTtw.png" data-width="1852" data-height="608" src="https://cdn-images-1.medium.com/max/800/1*x4zBBc0gEuhEQBn7p7CTtw.png"></figure><p name="c17d" id="c17d" class="graf graf--p graf-after--figure">This is to say that, generally, the <strong class="markup--strong markup--p-strong">hit condition</strong> should be,</p><figure name="8b52" id="8b52" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*d4qAln9g57mxd-TdFlxETA.png" data-width="1202" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*d4qAln9g57mxd-TdFlxETA.png"></figure><p name="dad2" id="dad2" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">3. Types of Caches</strong></p><p name="638c" id="638c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Three Different Types of Caches</strong></p><p name="d866" id="d866" class="graf graf--p graf-after--p">Basically, we have three types of caches,</p><ul class="postList"><li name="8ea6" id="8ea6" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Fully Associative Cache</strong>: any block from the memory can be put into any line of the cache. Thus, if we have 16 lines in the cache, we have to search up to 16 lines in order to find the block we are looking for.</li><li name="0191" id="0191" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Set Associative Cache</strong>: a block from the memory can only be put into N lines (where N can be 2~8). So if we want to find a block, we only need to search for these N lines.</li><li name="040d" id="040d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Directed Mapped Cache</strong>: a block can only go into 1 line of the cache. Thus, we only need to check one line in the cache.</li></ul><p name="57e5" id="57e5" class="graf graf--p graf-after--li">Now, let’s talk more about these different types of caches.</p><p name="1877" id="1877" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Directed Mapped Cache</strong></p><p name="5ca9" id="5ca9" class="graf graf--p graf-after--p">Suppose we have the following memory and a 4-line cache,</p><figure name="76d3" id="76d3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*trMNGJPuaxxmIP60HC10Mw.png" data-width="1510" data-height="482" src="https://cdn-images-1.medium.com/max/800/1*trMNGJPuaxxmIP60HC10Mw.png"></figure><p name="eca3" id="eca3" class="graf graf--p graf-after--figure">Suppose we have the following mapping rules,</p><pre name="2157" id="2157" class="graf graf--pre graf-after--p">Block 0 -&gt; Line 0<br>Block 1 -&gt; Line 1<br>Block 2 -&gt; Line 2<br>Block 3 -&gt; Line 3<br>Block 4 -&gt; Line 0<br>Block 5 -&gt; Line 1<br>Block 6 -&gt; Line 2</pre><p name="770f" id="770f" class="graf graf--p graf-after--pre">Thus, we have,</p><figure name="1672" id="1672" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*i8lTeKU1SPxMzOqSDHF13Q.png" data-width="1510" data-height="482" src="https://cdn-images-1.medium.com/max/800/1*i8lTeKU1SPxMzOqSDHF13Q.png"></figure><p name="c722" id="c722" class="graf graf--p graf-after--figure">We can conclude that each of the blocks in the memory can only go to only 1 specific line in the cache. So this cache is a direct-mapped cache. Then, let’s take a careful look at the block memory address.</p><figure name="9915" id="9915" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qzttktSvhob3DzQY43ix6A.png" data-width="1328" data-height="232" src="https://cdn-images-1.medium.com/max/800/1*qzttktSvhob3DzQY43ix6A.png"></figure><ul class="postList"><li name="8f6a" id="8f6a" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Block offset</strong>: decides which part of the data we will use in the cache block</li><li name="b5b9" id="b5b9" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Index</strong>: decides which line of the cache should we go to. Because we have 4 lines in the cache, a 2-bit index is enough for indexing.</li></ul><pre name="e81c" id="e81c" class="graf graf--pre graf-after--li">Index<br>00      -&gt;     Line 0<br>01      -&gt;     Line 1<br>10      -&gt;     Line 2<br>11      -&gt;     Line 3</pre><ul class="postList"><li name="abc9" id="abc9" class="graf graf--li graf-after--pre"><strong class="markup--strong markup--li-strong">Tag</strong>: Even though each of the memory blocks can only be mapped to 1 line of the cache, a line can have many blocks mapping to it. For example, both <code class="markup--code markup--li-code">Block 0</code> and <code class="markup--code markup--li-code">Block 4</code> will be mapped to <code class="markup--code markup--li-code">Line 0</code>. Thus, we have to decide what is really in the cache, <code class="markup--code markup--li-code">Block 0</code> or <code class="markup--code markup--li-code">Block 4</code>. Because both of <code class="markup--code markup--li-code">Block 0</code> and <code class="markup--code markup--li-code">Block 4</code> have the same index of <code class="markup--code markup--li-code">00</code>, the index bits are not useful for identifying these blocks. However, the rest of the block numbers can be treated as an ID to trace these different blocks. Therefore, these bits can be treated as tags and used to decide if we have a cache hit.</li></ul><p name="c8ec" id="c8ec" class="graf graf--p graf-after--li">So in general, the final diagram could be,</p><figure name="ce5c" id="ce5c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qT3T540p_bJDj9-Gp_Zusg.png" data-width="1664" data-height="528" src="https://cdn-images-1.medium.com/max/800/1*qT3T540p_bJDj9-Gp_Zusg.png"></figure><p name="542b" id="542b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Evaluation of the Direct Mapped Cache</strong></p><p name="9321" id="9321" class="graf graf--p graf-after--p">let’s now look at the upsides and the downsides of the direct-mapped cache. The upsides are,</p><ul class="postList"><li name="6e37" id="6e37" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Fast</strong>: because we only have to check for <strong class="markup--strong markup--li-strong">1 line</strong> in the cache for each block</li><li name="03ee" id="03ee" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Cheaper</strong>: because we only have to do <strong class="markup--strong markup--li-strong">1 comparison</strong> of the tag and the block number</li><li name="8422" id="8422" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Energy Efficient</strong></li></ul><p name="70fd" id="70fd" class="graf graf--p graf-after--li">The downsides are,</p><ul class="postList"><li name="17b4" id="17b4" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Conflict</strong>: when we have a block B that has the same index of block A in the cache, block A will be kicked out even if it can be useful in the cache. This brings us to the result that different blocks will fight to the 1 line in the cache.</li><li name="20fe" id="20fe" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Higher Miss Rate</strong>: the more conflicts, the higher the miss rate.</li></ul><p name="8e3f" id="8e3f" class="graf graf--p graf-after--li">In conclusion, although the direct-mapped cache can benefit from fast access, it should pay a higher miss rate because of the conflicts.</p><p name="ceaf" id="ceaf" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Direct Mapped Cache: An Example</strong></p><p name="5122" id="5122" class="graf graf--p graf-after--p">Suppose we have a 16 kB directed-mapped cache, 256-byte blocks which of these conflict with <code class="markup--code markup--p-code">0x12345678</code>, then,</p><ul class="postList"><li name="5385" id="5385" class="graf graf--li graf-after--p">the <strong class="markup--strong markup--li-strong">block offset</strong> of this address should be the 8 (because <code class="markup--code markup--li-code">log2(256) = 8</code>) least significant bits <code class="markup--code markup--li-code">0111 1000</code></li><li name="5c78" id="5c78" class="graf graf--li graf-after--li">the <strong class="markup--strong markup--li-strong">index</strong> of this address should be 6 bits long (because <code class="markup--code markup--li-code">log2(16*1024/256) = log2(64) = 6</code>). And the index should be <code class="markup--code markup--li-code">01 0110</code></li><li name="3a23" id="3a23" class="graf graf--li graf-after--li">The rest of the address will be the tag, which should be 18 bits.</li><li name="44d2" id="44d2" class="graf graf--li graf-after--li">the address <code class="markup--code markup--li-code">0x12345677</code> will not cause a conflict because it is in the same block of the original address</li><li name="a5ab" id="a5ab" class="graf graf--li graf-after--li">the address <code class="markup--code markup--li-code">0x12341666</code> will cause a conflict because they will be mapped into the same line but the tag of them are not the same</li></ul><p name="2996" id="2996" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(5) Set Associative Cache</strong></p><p name="202e" id="202e" class="graf graf--p graf-after--p">In a direct-mapped cache, the index is used for getting the right line. While in a set-associative cache, the index is used for directing to the <strong class="markup--strong markup--p-strong">set</strong>, and the set is defined as a combination of several lines. An <strong class="markup--strong markup--p-strong">N-way set-associative</strong> cache means that each of the sets in the cache has N lines. So for a given block in the memory, it can be mapped to N possible lines in a set.</p><p name="501d" id="501d" class="graf graf--p graf-after--p">Suppose we have a two-way set-associative cache and each of the sets in this cache has 2 lines, so a given block can be in either of these two lines. In the following diagram, we have a 4-line 2-set two-way set-associative cache,</p><figure name="f6e1" id="f6e1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_GzoLbGDsbgl561m8sqqrg.png" data-width="1664" data-height="528" src="https://cdn-images-1.medium.com/max/800/1*_GzoLbGDsbgl561m8sqqrg.png"></figure><p name="3ccc" id="3ccc" class="graf graf--p graf-after--figure">The upsides of the set-associative cache are,</p><ul class="postList"><li name="8f68" id="8f68" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Reduce the conflicts</strong>: the conflicts can be reduced because now we can have several lines for storing the same block</li><li name="e38e" id="e38e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Reduce the miss rate</strong>: because we have reduced the conflicts in the cache, there is a trend that we can have a higher miss rate</li></ul><p name="0765" id="0765" class="graf graf--p graf-after--li">The downsides of it should be,</p><ul class="postList"><li name="4eeb" id="4eeb" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Slow</strong>: because we have to search up to N times in the set to decide whether there is a cache hit</li></ul><p name="bce6" id="bce6" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(6) Fully Associative Cache</strong></p><p name="c3f1" id="c3f1" class="graf graf--p graf-after--p">A fully associative cache is a cache that any memory blocks can be mapped to any lines in the cache. This means that we do not have to use the index anymore. So all the remaining bits in the address will be considered as the tag.</p><figure name="8517" id="8517" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jn2PoEcDYWNyNucHgz5Uqw.png" data-width="1664" data-height="528" src="https://cdn-images-1.medium.com/max/800/1*jn2PoEcDYWNyNucHgz5Uqw.png"></figure><p name="15f6" id="15f6" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(7) Relationship of Three Types of Caches</strong></p><p name="28bb" id="28bb" class="graf graf--p graf-after--p">The direct-mapped cache and the fully associative cache are actually two specific cases for the set-associative cache. If we have a 1-way set-associative cache, then this will be equivalent to a direct-mapped cache. Meanwhile, if we have a fully-associative cache, then this means that have a #-of-lines-way set-associative cache. Thus, for all these three types,</p><ul class="postList"><li name="33b0" id="33b0" class="graf graf--li graf-after--p">Offset Size = log2(block size)</li><li name="1694" id="1694" class="graf graf--li graf-after--li">Index Size = log2(# of sets)</li><li name="d885" id="d885" class="graf graf--li graf-after--li">Tag Size = the size of the remaining bits</li></ul><p name="2a8a" id="2a8a" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">4. Cache Replacement</strong></p><p name="2625" id="2625" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of Cache Replacement</strong></p><p name="addf" id="addf" class="graf graf--p graf-after--p">We have discussed how cache finds the data when we are looking for it. Now let’s talk about what happens when we need to remove something from the cache to make room for the other data. When the set for a memory block is <strong class="markup--strong markup--p-strong">full</strong> but we also have a <strong class="markup--strong markup--p-strong">cache miss</strong> problem, we have to do <strong class="markup--strong markup--p-strong">cache replacement</strong>.</p><p name="6bae" id="6bae" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Common Cache Replacement Algorithms</strong></p><p name="aae3" id="aae3" class="graf graf--p graf-after--p">To decide which block should we kick out, normally we would have the following policies,</p><ul class="postList"><li name="1523" id="1523" class="graf graf--li graf-after--p">Random</li><li name="23a6" id="23a6" class="graf graf--li graf-after--li">FIFO</li><li name="416a" id="416a" class="graf graf--li graf-after--li">Least Recently Used (LRU): LRU is a good policy but it is not easy to do</li><li name="7687" id="7687" class="graf graf--li graf-after--li">Not Most Recently Used (NMRU): a policy to mimic the LRU</li></ul><p name="1a12" id="1a12" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Implementing LRU</strong></p><p name="7466" id="7466" class="graf graf--p graf-after--p">Now let’s see how we can implement the LRU. Suppose we have a 4-way associative cache and let’s track what’s happening in a single set. Continue to what we have discussed, this set should be as follows,</p><figure name="d58e" id="d58e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UuuuZw_EOhbbEp-oHVJcqA.png" data-width="1272" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*UuuuZw_EOhbbEp-oHVJcqA.png"></figure><p name="59b9" id="59b9" class="graf graf--p graf-after--figure">Now because we want to know which block is the least recently used one, we will add the LRU counters to each of these blocks.</p><figure name="310e" id="310e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*G7n8q2f7-LrbunZzYdGoWw.png" data-width="1272" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*G7n8q2f7-LrbunZzYdGoWw.png"></figure><p name="f996" id="f996" class="graf graf--p graf-after--figure">The values in the LRU counter should all be different all the time. So initially, the value will be set to 0 (means least recently used) to 3 (means most recently used) according to the size of the set (i.e. in this case, 4). Thus, we have,</p><figure name="4f31" id="4f31" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_yHZcmJU7WMXNSVexWPQug.png" data-width="1272" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*_yHZcmJU7WMXNSVexWPQug.png"></figure><p name="263f" id="263f" class="graf graf--p graf-after--figure">Now assume that we have a cache replacement and the LRU block according to the set is line 0. So we will kick the data in line 0 out and then update the LRU counter. Because now the line 0 is just used recently, it becomes the most recently used and the LRU counter should be set to 3. Because we have discussed that the LRU counters should all be different in a set at any time, we should also update the values in other counters by decreasing 1.</p><figure name="1c63" id="1c63" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*J4LuJmsutJbFOSi4xQyFcQ.png" data-width="1272" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*J4LuJmsutJbFOSi4xQyFcQ.png"></figure><p name="aaab" id="aaab" class="graf graf--p graf-after--figure">If line 0 is accessed again, then line 0 will still be the most recently used block and the LRU counter should not change.</p><figure name="a746" id="a746" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xKLnFqtzFfzKFt1kaEnU6w.png" data-width="1272" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*xKLnFqtzFfzKFt1kaEnU6w.png"></figure><p name="e54d" id="e54d" class="graf graf--p graf-after--figure">If now the block in line 2 is accessed, then we can conclude that the block in line 2 should be the most recently used block. So we have to update the value of the LRU counter to 3 for line 2. Instead of decreasing all the other LRU counters by 1, in this case, we will only reduce 1 to the LRU counter that is larger than the original value of the LRU counter in line 2 (i.e. 1) and these counters are the LRU counter for line 0 and line 3. For those whose values are below the original value of the LRU counter in line 2 (i.e. the value of line 1), we will keep its value. By doing so, we will maintain that all the LRU counters are different all the time.</p><figure name="67be" id="67be" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AWniRyoGPXWv8BGkmRPJUQ.png" data-width="1272" data-height="264" src="https://cdn-images-1.medium.com/max/800/1*AWniRyoGPXWv8BGkmRPJUQ.png"></figure><p name="accd" id="accd" class="graf graf--p graf-after--figure">Note that after each step, we can check that all the LRU counters in a set have different values. If some of the counters have the same value, we might probably make some mistakes somewhere.</p><p name="20e1" id="20e1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) LRU Cost</strong></p><p name="4604" id="4604" class="graf graf--p graf-after--p">We have to take care of the cost of the LRU. In the example above, we have a 4-way set-associative cache, and each of the LRU counters needs to have 2 bits if we want to express value 0~3. So in reality, the final state that we have discussed should be,</p><figure name="a45c" id="a45c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*d6FAYdzfIDtWSPQUSTu35g.png" data-width="1272" data-height="264" src="https://cdn-images-1.medium.com/max/800/1*d6FAYdzfIDtWSPQUSTu35g.png"></figure><p name="22f8" id="22f8" class="graf graf--p graf-after--figure">Therefore, generally, if we have an N-way set-associative cache, we will need log2(N) bits for each counter.</p><p name="7993" id="7993" class="graf graf--p graf-after--p">If the value of N is higher, like 32, then we need to have 5 bits in each counter and the <strong class="markup--strong markup--p-strong">cost of maintaining these LRU counters can be really high</strong>. Also, because we have to update 32 5-bit counters on each access even for the cache hits, the LRU can be very <strong class="markup--strong markup--p-strong">energy-consuming</strong>.</p><p name="7b23" id="7b23" class="graf graf--p graf-after--p">To deal with the LRU cost, the LRU approximations like NMRU try to keep fewer counters and fewer updates on cache hits.</p><p name="994b" id="994b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">5. Cache Write</strong></p><p name="1866" id="1866" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Cache Writing Policy</strong></p><p name="9291" id="9291" class="graf graf--p graf-after--p">We have already discussed the read policy for a cache, and now, let’s discuss what if we write to the cache. Normally, we are concerned about two different aspects when we write to the cache,</p><ul class="postList"><li name="6984" id="6984" class="graf graf--li graf-after--p">Do we insert the block we write to the cache?</li><li name="ff21" id="ff21" class="graf graf--li graf-after--li">Do we also update the memory?</li></ul><p name="36a6" id="36a6" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Types of Caches: Part 1</strong></p><p name="a4d3" id="a4d3" class="graf graf--p graf-after--p">When we write to the cache if we have a write miss, should the block brought to the cache or not? There are two types of caches,</p><ul class="postList"><li name="1d98" id="1d98" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Write-allocate Cache</strong>: bring the write miss block to the cache</li><li name="0916" id="0916" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Non-write-allocate Cache</strong>: not bring the write miss block to the cache</li></ul><p name="f24e" id="f24e" class="graf graf--p graf-after--li">Most caches today are <strong class="markup--strong markup--p-strong">write-allocated caches</strong> because there is some locality between reads and writes. This means that if we write to something, we are also likely to read from it.</p><p name="4feb" id="4feb" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Types of Caches: Part 2</strong></p><p name="3772" id="3772" class="graf graf--p graf-after--p">The other aspect of the writing policy is that when we have a write, should we write only to the cache, or should we also write to the memory immediately? There are also two types of caches based on this policy,</p><ul class="postList"><li name="6c36" id="6c36" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Write-through Cache</strong>: This means that we will update the memory immediately</li><li name="41b1" id="41b1" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Write-back Cache</strong>: under this policy, we first write to the cache, and then we write to the memory when the block is replaced from the cache</li></ul><p name="47f0" id="47f0" class="graf graf--p graf-after--li">Write-through caches are quite unpopular because they send stuff to memory directly. We usually want to have a <strong class="markup--strong markup--p-strong">write-back cache</strong> because it prevents the memory from being overwhelmed and reduces the number of writes that we might have.</p><p name="7c6e" id="7c6e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Relationship between Write-allocate Cache and Write-back Cache</strong></p><p name="7b94" id="7b94" class="graf graf--p graf-after--p">If we want to have a write-back cache, we also need to have it as the write-allocate cache. This is because the write-back cache must have the write block in the cache so that we don’t need to update it in the memory.</p><p name="d57b" id="d57b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Write-Back Cache Problem: Distinguish Between Read/Write</strong></p><p name="f88d" id="f88d" class="graf graf--p graf-after--p">However, if we want to implement a write-back cache, there can be a tricky problem. Some blocks in the cache will be the blocks that we have <strong class="markup--strong markup--p-strong">read</strong> from the memory, and the other blocks will be the <strong class="markup--strong markup--p-strong">written</strong> ones. For the read blocks, we don’t have to update the memory if we kick them out of the cache, while for the written block, we have to write them to the memory if they expire from the cache. So the problem is that how can the processor know whether a block is read or written?</p><p name="ac44" id="ac44" class="graf graf--p graf-after--p">Probably, we have the following options,</p><ul class="postList"><li name="c455" id="c455" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Write All</strong>: Regardless of whether it is a read block or a written block, we will write to the memory every time we replace a block. The problem is that we can have many read stuff and this will end up writing to memory over and over again.</li><li name="66fc" id="66fc" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Dirty Bit</strong>: As another option, we can add a dirty bit to every block in the cache. The dirty bit shows whether we have written the block since it was placed in the cache or not. If the dirty bit equals 0, it means the block is clean and this block is not written since it was last brought from the memory. If the dirty equals 1, it means the block is dirty and we should write this block back to the memory when a replacement happens.</li></ul><p name="bc61" id="bc61" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(6) Write-Back Cache: An Example</strong></p><p name="2bcf" id="2bcf" class="graf graf--p graf-after--p">Again, we will use an empty 4-way set-associative write-back cache with dirty bits in this case. We will only talk about set 0 of this cache. Thus, initially, we can have,</p><figure name="0ece" id="0ece" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ea9Qt4cgQH0uMRaCyuhAAQ.png" data-width="1442" data-height="276" src="https://cdn-images-1.medium.com/max/800/1*Ea9Qt4cgQH0uMRaCyuhAAQ.png"></figure><p name="c072" id="c072" class="graf graf--p graf-after--figure">Suppose we have the following operations,</p><pre name="34fa" id="34fa" class="graf graf--pre graf-after--p">WR A<br>RD A<br>RD B<br>RD C<br>WR C</pre><p name="491a" id="491a" class="graf graf--p graf-after--pre">Let’s see what will happen when we conduct these operations.</p><p name="1467" id="1467" class="graf graf--p graf-after--p">When we have the first write to A, because this is a written block, the dirty will be set to 1.</p><figure name="423b" id="423b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pGLtitoc3LZkWEnqV7emVQ.png" data-width="1442" data-height="276" src="https://cdn-images-1.medium.com/max/800/1*pGLtitoc3LZkWEnqV7emVQ.png"></figure><p name="eb0d" id="eb0d" class="graf graf--p graf-after--figure">Then when we read from A, the processor checks the tag and the valid and determines that we have a cache hit. So A will be directly read from the cache.</p><figure name="e2f5" id="e2f5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PDYmn7g0oajhzrGDdIJmUw.png" data-width="1442" data-height="276" src="https://cdn-images-1.medium.com/max/800/1*PDYmn7g0oajhzrGDdIJmUw.png"></figure><p name="30b7" id="30b7" class="graf graf--p graf-after--figure">Then we read B and we can find that B is not in the cache (cache miss). Thus, B will be read into the cache, and the dirty bit of B will be set to 0.</p><figure name="c9fc" id="c9fc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ngkMs5qX1KFruuj74YRN_w.png" data-width="1442" data-height="260" src="https://cdn-images-1.medium.com/max/800/1*ngkMs5qX1KFruuj74YRN_w.png"></figure><p name="f9e1" id="f9e1" class="graf graf--p graf-after--figure">Also, when we read C, we have a cache miss. C will be read into the cache, and the dirty bit of C will be set to 0.</p><figure name="d4c7" id="d4c7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4ORCoGnrd6IfN0jX-3h9Uw.png" data-width="1442" data-height="268" src="https://cdn-images-1.medium.com/max/800/1*4ORCoGnrd6IfN0jX-3h9Uw.png"></figure><p name="447e" id="447e" class="graf graf--p graf-after--figure">Finally, because we write to the C, the processor checks the cache and finds a cache hit. So we update the cache block and set the dirty bit to 1. Because our cache is a write-back cache, we don’t have to update the memory now.</p><figure name="ecde" id="ecde" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*dKUWWl45QU41xsYnKxBpDg.png" data-width="1442" data-height="268" src="https://cdn-images-1.medium.com/max/800/1*dKUWWl45QU41xsYnKxBpDg.png"></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/a264e7c73e8f"><time class="dt-published" datetime="2021-03-10T16:39:37.681Z">March 10, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/high-performance-computer-architecture-22-locality-principle-cache-types-of-caches-lru-and-a264e7c73e8f" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>