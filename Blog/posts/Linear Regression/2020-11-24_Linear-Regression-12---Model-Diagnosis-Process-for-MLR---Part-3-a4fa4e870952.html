<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Linear Regression 12 | Model Diagnosis Process for MLR — Part 3</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Linear Regression 12 | Model Diagnosis Process for MLR — Part 3</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Linear Regression
</section>
<section data-field="body" class="e-content">
<section name="7c9c" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="a5ac" id="a5ac" class="graf graf--h3 graf--leading graf--title">Linear Regression 12 | Model Diagnosis Process for MLR — Part 3</h3><figure name="c60b" id="c60b" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*JRXYaS8AmzV2HWueCj2mRA.png" data-width="1514" data-height="716" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*JRXYaS8AmzV2HWueCj2mRA.png"></figure><ol class="postList"><li name="1469" id="1469" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Model Diagnosis Process for MLR</strong></li></ol><ul class="postList"><li name="aaef" id="aaef" class="graf graf--li graf-after--li">(0) Goal of Modeling</li><li name="6c82" id="6c82" class="graf graf--li graf-after--li">(1) Step 1. Check Multicollinearity</li><li name="1743" id="1743" class="graf graf--li graf-after--li">(2) Step 2. Fit the Initial Model</li><li name="df4e" id="df4e" class="graf graf--li graf-after--li">(3) Step 3. Check Influential Points</li><li name="dbd6" id="dbd6" class="graf graf--li graf-after--li">(4) Step 4. Check Heteroscedasticity</li><li name="e6fb" id="e6fb" class="graf graf--li graf-after--li">(5) Step 5. Check Normality</li><li name="a447" id="a447" class="graf graf--li graf-after--li">(6) Step 6. Check Linearity</li></ul><p name="7368" id="7368" class="graf graf--p graf-after--li">You can find the topics above in part 1 and part 2.</p><div name="df36" id="df36" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/adamedelwiess/linear-regression-9-model-diagnosis-process-for-mlr-part-1-205c1f71d581" data-href="https://medium.com/adamedelwiess/linear-regression-9-model-diagnosis-process-for-mlr-part-1-205c1f71d581" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/linear-regression-9-model-diagnosis-process-for-mlr-part-1-205c1f71d581"><strong class="markup--strong markup--mixtapeEmbed-strong">Linear Regression 9 | Model Diagnosis Process for MLR - Part 1</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Linear Regression</em>medium.com</a><a href="https://medium.com/adamedelwiess/linear-regression-9-model-diagnosis-process-for-mlr-part-1-205c1f71d581" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="2d17825b7b0e09bb7d23da6be7685041" data-thumbnail-img-id="1*JRXYaS8AmzV2HWueCj2mRA.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*JRXYaS8AmzV2HWueCj2mRA.png);"></a></div><div name="fc33" id="fc33" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/adamedelwiess/linear-regression-10-model-diagnosis-process-for-mlr-part-2-733ca5e99437" data-href="https://medium.com/adamedelwiess/linear-regression-10-model-diagnosis-process-for-mlr-part-2-733ca5e99437" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/linear-regression-10-model-diagnosis-process-for-mlr-part-2-733ca5e99437"><strong class="markup--strong markup--mixtapeEmbed-strong">Linear Regression 10 | Model Diagnosis Process for MLR — Part 2</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Linear Regression</em>medium.com</a><a href="https://medium.com/adamedelwiess/linear-regression-10-model-diagnosis-process-for-mlr-part-2-733ca5e99437" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="7adad13baa1e9dc4f0436799d62fd6b4" data-thumbnail-img-id="1*JRXYaS8AmzV2HWueCj2mRA.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*JRXYaS8AmzV2HWueCj2mRA.png);"></a></div><p name="a4f3" id="a4f3" class="graf graf--p graf-after--mixtapeEmbed">Let’s continue our discussion.</p><p name="edfe" id="edfe" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Step 7. Modify the initial model and fit the data again</strong></p><p name="63e2" id="63e2" class="graf graf--p graf-after--p">Because we have modified the data so that our data can be able to suit the assumptions of our linear model. Because we can put different variables as our predictors and this results in several linear models. We have to choose the best model so that our final model is the best model. This process is called the model selection process and we have quite a lot of measures to tell whether or not a model has a good performance.</p><p name="51ac" id="51ac" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Step 8. Best Subset Model Selection: based on adjusted R² or Mallow’s Cp</strong></p><ul class="postList"><li name="7e96" id="7e96" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Criteria #1. R-Square</strong></li></ul><figure name="7966" id="7966" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*8aCS5zFkHdWqOItOWWQT3A.png" data-width="952" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*8aCS5zFkHdWqOItOWWQT3A.png"></figure><p name="3166" id="3166" class="graf graf--p graf-after--figure">This is always not a good idea in MLR because it will say yes to the overfitted model. We have talked about this</p><ul class="postList"><li name="9f9b" id="9f9b" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Criteria #2. Adjusted R-Square</strong></li></ul><figure name="d6b1" id="d6b1" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*pFnubRYdevuERrnFnHly8g.png" data-width="1162" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*pFnubRYdevuERrnFnHly8g.png"></figure><p name="09ac" id="09ac" class="graf graf--p graf-after--figure">The adjusted R-square takes a balance of the decrease in SSE and the decrease in the degree of freedom when adding more predictors, so it will have a better performance in telling the overfitted model.</p><ul class="postList"><li name="f908" id="f908" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Criteria #3. Mallows’s Cp</strong></li></ul><p name="3cab" id="3cab" class="graf graf--p graf-after--li">Given a linear model with <em class="markup--em markup--p-em">k</em> parameters such as,</p><figure name="f8bc" id="f8bc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ThxrrfBcJpmV9Onhgfl1kQ.png" data-width="1136" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*ThxrrfBcJpmV9Onhgfl1kQ.png"></figure><p name="fd58" id="fd58" class="graf graf--p graf-after--figure">Suppose we select <em class="markup--em markup--p-em">p</em> parameters (<em class="markup--em markup--p-em">p-</em>1 predictors) from the full model, then the Mellows’s Cp statistic for this subset model is defined as,</p><figure name="712c" id="712c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2YFH6OUvWEL4vnnlhM9fgQ.png" data-width="982" data-height="114" src="https://cdn-images-1.medium.com/max/800/1*2YFH6OUvWEL4vnnlhM9fgQ.png"></figure><p name="bd61" id="bd61" class="graf graf--p graf-after--figure">We don’t have to know how to construct this statistic.</p><p name="46b5" id="46b5" class="graf graf--p graf-after--p">The most common interpretation of the Mallows’s Cp is that <strong class="markup--strong markup--p-strong">smaller Cp values are better</strong> as they indicate smaller amounts of unexplained error. If the model with p-1 predictors is a good model that can be useful, then its Cp should be small and the Cp statistic should be close to the value p. This is because the MSE for the full model is then close to the MSE of the subset model.</p><p name="4b18" id="4b18" class="graf graf--p graf-after--p">In practice, since Cp is a sample estimate, then it could be less than the value p. You may want to choose the smallest model for which Cp ≤ p is true, but there can be an overfitting problem if Cp is less than 1.</p><ul class="postList"><li name="9050" id="9050" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Criteria #4. Unbiased Mallows’s Cp</strong></li></ul><p name="f877" id="f877" class="graf graf--p graf-after--li">The unbiased Mallows’s Cp (unbiased for the prediction risk R, and we don’t have to understand this) is defined as,</p><figure name="491e" id="491e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*55TJPIFsvgAY4DfVa4hJwg.png" data-width="982" data-height="114" src="https://cdn-images-1.medium.com/max/800/1*55TJPIFsvgAY4DfVa4hJwg.png"></figure><p name="326e" id="326e" class="graf graf--p graf-after--figure">Then, replace the Cp with the definition of the Mallows’s Cp,</p><figure name="c63b" id="c63b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*sAvWnV-Y6f_ZXli-QciPag.png" data-width="1068" data-height="108" src="https://cdn-images-1.medium.com/max/800/1*sAvWnV-Y6f_ZXli-QciPag.png"></figure><p name="9860" id="9860" class="graf graf--p graf-after--figure">This measure is equivalent to the Mallows’s Cp with a different value. The similarity between the unbiased Mallows’s Cp and the Mallows’s Cp is that the smaller values are better in the model selection process. For example, here is a possible result we can have from a certain model.</p><figure name="d562" id="d562" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5jcRVcMu1Iz6XpGHnRWnRQ.png" data-width="1148" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*5jcRVcMu1Iz6XpGHnRWnRQ.png"></figure><p name="1e8f" id="1e8f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(9) Step 9. AIC/BIC Method</strong></p><ul class="postList"><li name="b360" id="b360" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Why we have those two methods?</strong></li></ul><p name="fa67" id="fa67" class="graf graf--p graf-after--li">Both the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) measures the <strong class="markup--strong markup--p-strong">amount of information lost</strong> by a given model. With the common sense that the less information a model loses, the higher quality the model has, we can select the model with those two criteria.</p><p name="5a7e" id="5a7e" class="graf graf--p graf-after--p">In summary, the smaller values of these two criteria indicate a better model.</p><ul class="postList"><li name="75fd" id="75fd" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Recall: Likelihood Function</strong></li></ul><p name="df98" id="df98" class="graf graf--p graf-after--li">The likelihood function is the joint probability density of all the observations in the sample. It tells us the general probability we have to obtain such a certain sample. So that we want to maximize the likelihood of obtaining our sample in order to get the best-fitted parameters.</p><p name="afd2" id="afd2" class="graf graf--p graf-after--p">Because the density function of the Gaussian distribution is less than 1, then we are able to draw a conclusion that the maximized likelihood of the sample set is no more than 1 and thus, the maximized log-likelihood is negative.</p><ul class="postList"><li name="031f" id="031f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Akaike Information Criterion (AIC)</strong></li></ul><p name="c059" id="c059" class="graf graf--p graf-after--li">Suppose <em class="markup--em markup--p-em">k</em> is the number of parameters of a linear regression model, then let L-hat be the maximum value of the likelihood function for the model. The AIC is defined by,</p><figure name="0279" id="0279" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FkfCGGOwWsx8NlHe7YjyVA.png" data-width="880" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*FkfCGGOwWsx8NlHe7YjyVA.png"></figure><p name="491d" id="491d" class="graf graf--p graf-after--figure">We are having this result because of the information theory and we don’t have to understand it now. Note that the multiplier of the <em class="markup--em markup--p-em">k</em> is 2, which is defined as the penalty coefficient on the number of the parameters. In fact, AIC and Mallows’s Cp is equivalent mathematically.</p><ul class="postList"><li name="e560" id="e560" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Bayesian Information Criterion (BIC)</strong></li></ul><p name="c173" id="c173" class="graf graf--p graf-after--li">AIC and BIC are theoretically different methods. AIC is a based on the likelihood function whereas BIC is based on the estimated function of a posterior probability of a model being true.</p><p name="030d" id="030d" class="graf graf--p graf-after--p">Although these two criteria are based on different assumptions, in practice, the only difference is in the size of the penalty. BIC penalizes the model complexity more heavily.</p><figure name="4a47" id="4a47" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*s58p92crhi93QGWqwDqjkQ.png" data-width="880" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*s58p92crhi93QGWqwDqjkQ.png"></figure><ul class="postList"><li name="a73e" id="a73e" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Interpretation of the AIC and the BIC</strong></li></ul><p name="bb4f" id="bb4f" class="graf graf--p graf-after--li">A predictor has the lowest AIC and BIC means that the amount of information lost by removing this predictor from the model is minimum among all the predictors.</p><p name="947b" id="947b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) Step 10. Stepwise Model Selection: based on t-test</strong></p><p name="4255" id="4255" class="graf graf--p graf-after--p">There are two kinds of stepwise model selection. If we start will the null model with none of the predictors and we add 1 predictor in each step, then it is called a <strong class="markup--strong markup--p-strong">forward stepwise model selection (forward selection)</strong>. If we start with the full model and we remove 1 predictor in each step, then it is called a <strong class="markup--strong markup--p-strong">backward stepwise model selection (backward selection)</strong>.</p><figure name="f71a" id="f71a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*anohjTesdqfciEV07LuARQ.png" data-width="1650" data-height="710" src="https://cdn-images-1.medium.com/max/800/1*anohjTesdqfciEV07LuARQ.png"><figcaption class="imageCaption"><a href="https://quantifyinghealth.com/stepwise-selection/" data-href="https://quantifyinghealth.com/stepwise-selection/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://quantifyinghealth.com/stepwise-selection/</a></figcaption></figure><ul class="postList"><li name="44b3" id="44b3" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">When to stop selection: by AIC and BIC</strong></li></ul><p name="d99e" id="d99e" class="graf graf--p graf-after--li">When we add more predictors to the model or we remove the predictors from the model, we have to set up a standard for our model so that we can know when to stop our selection. The first standard we can use is by AIC and BIC.</p><p name="2840" id="2840" class="graf graf--p graf-after--p">When the AIC and BIC for adding a new predictor is relatively low and the tendency is flat, then we are able to make the conclusion that the amount of information lost by this predictor is so small that we can stop our selection. For example, in the following picture, we are able to say that we can stop at the 4th predictor (the critical turning point) because it has the minimum AIC and BIC.</p><figure name="0a17" id="0a17" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kbV1gIfhbRn1RtER6tmliQ.png" data-width="1140" data-height="364" src="https://cdn-images-1.medium.com/max/800/1*kbV1gIfhbRn1RtER6tmliQ.png"></figure><ul class="postList"><li name="b3b6" id="b3b6" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">When to stop selection: by Mallows’s Cp</strong></li></ul><p name="1f91" id="1f91" class="graf graf--p graf-after--li">Similar to AIC and BIC, the Mallows’s Cp can be used to find where to stop. Because a smaller Cp tells a better performance of the model, then we have to find the lowest Cp for our model.</p><figure name="db96" id="db96" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CgT3nISOq6WP0z8QLlY1qA.png" data-width="1092" data-height="384" src="https://cdn-images-1.medium.com/max/800/1*CgT3nISOq6WP0z8QLlY1qA.png"></figure><ul class="postList"><li name="3335" id="3335" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">When to stop selection: by Adjusted R-Square</strong></li></ul><p name="998b" id="998b" class="graf graf--p graf-after--li">It is quite obvious that a higher adjusted R-Square tells a better performance of the model. Note that based on different criteria, the destination of the selection process can be different. For example,</p><figure name="658f" id="658f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*f5NrDpXJF9DmNEcHPt22sA.png" data-width="1366" data-height="440" src="https://cdn-images-1.medium.com/max/800/1*f5NrDpXJF9DmNEcHPt22sA.png"></figure><ul class="postList"><li name="53f8" id="53f8" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">When to stop selection: by t-testing and F-testing</strong></li></ul><p name="d8ed" id="d8ed" class="graf graf--p graf-after--li graf--trailing">The t-testing and the F-testing process can also be used in the stepwise selection. But they are old-school methods and we don’t use them commonly nowadays.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/a4fa4e870952"><time class="dt-published" datetime="2020-11-24T00:46:47.988Z">November 24, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/linear-regression-9-model-diagnosis-process-for-mlr-part-3-a4fa4e870952" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>