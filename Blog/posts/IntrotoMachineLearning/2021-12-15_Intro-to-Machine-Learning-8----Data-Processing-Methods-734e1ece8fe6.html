<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Intro to Machine Learning 8 |  Data Processing Methods</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Intro to Machine Learning 8 |  Data Processing Methods</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Intro to Machine Learning
</section>
<section data-field="body" class="e-content">
<section name="0c1b" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="523a" id="523a" class="graf graf--h3 graf--leading graf--title">Intro to Machine Learning 8 | Data Processing Methods</h3><figure name="b824" id="b824" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*oyjoxzCe_hGrS4DZ.png" data-width="1250" data-height="700" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*oyjoxzCe_hGrS4DZ.png"></figure><ol class="postList"><li name="e297" id="e297" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Handling Missing Values</strong></li></ol><p name="31c2" id="31c2" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Types of Missing Values</strong></p><ul class="postList"><li name="552f" id="552f" class="graf graf--li graf-after--p">Missing completely at random (MCAR): Data is missing completely at random if all observations have the same likelihood of being missing.</li><li name="dbd8" id="dbd8" class="graf graf--li graf-after--li">Missing at random (MAR): When data is missing at random (MAR) the likelihood that a data point is missing is not related to the missing data but may be related to other observed data.</li><li name="1f77" id="1f77" class="graf graf--li graf-after--li">Missing not at random (MNAR): When data is missing not at random (MNAR) the likelihood of a missing observation is related to its values.</li></ul><p name="8dde" id="8dde" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Drop Row or Columns</strong></p><p name="ea48" id="ea48" class="graf graf--p graf-after--p">Unless most values in the dropped columns are missing, the model loses access to a lot of information with this approach. So this approach should only be appropriate when we have too many missing values in that column. This can be used for MCAR and MAR but not MNAR.</p><figure name="a420" id="a420" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*42KkhTlb6yqma1QYI6rABA.png" data-width="1366" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*42KkhTlb6yqma1QYI6rABA.png"></figure><p name="c345" id="c345" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Imputation</strong></p><p name="574d" id="574d" class="graf graf--p graf-after--p">Imputation fills in the missing values with some numbers. For instance, we can fill in the mean value along each column. The imputed value won’t be exactly right in most cases, but it usually leads to more accurate models than you would get from dropping the column entirely.</p><figure name="8fe6" id="8fe6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pLY5QlRndrYX6UbDfXaJZg.png" data-width="1366" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*pLY5QlRndrYX6UbDfXaJZg.png"></figure><p name="e6e1" id="e6e1" class="graf graf--p graf-after--figure">Typically, we have the following methods to impute that missing value,</p><ul class="postList"><li name="f7e7" id="f7e7" class="graf graf--li graf-after--p">mean or median for numerical data</li><li name="75ef" id="75ef" class="graf graf--li graf-after--li">mode for categorical data</li><li name="e358" id="e358" class="graf graf--li graf-after--li">average of the k nearest neighbor for <em class="markup--em markup--li-em">k</em>NN</li><li name="d7c1" id="d7c1" class="graf graf--li graf-after--li">prediction by using the column of the variable as target feature</li></ul><p name="bb1a" id="bb1a" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Imputation + Adding Column</strong></p><p name="a03b" id="a03b" class="graf graf--p graf-after--p">Sometimes when the missing values are not missed randomly. Then we may consider adding one column implying if we have a missing value in that row after imputation. This column shows the likelihood that a data point is missing.</p><figure name="e8f1" id="e8f1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*D5t_bZzr2g2yKZU-NoM7Pg.png" data-width="1366" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*D5t_bZzr2g2yKZU-NoM7Pg.png"></figure><p name="1ad7" id="1ad7" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">2. Handling Outliers</strong></p><p name="a443" id="a443" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition Of InterQuartile Range (IQR)</strong></p><p name="6bd0" id="6bd0" class="graf graf--p graf-after--p">The interquartile range (IQR) is a measure of statistical dispersion (e.g. one of the other examples is the standard deviation), which is the spread of the data. It is defined by,</p><figure name="0964" id="0964" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*01W5HYWk5K1yQxDGyYKycw.png" data-width="1324" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*01W5HYWk5K1yQxDGyYKycw.png"></figure><p name="89a6" id="89a6" class="graf graf--p graf-after--figure">Where,</p><ul class="postList"><li name="6a74" id="6a74" class="graf graf--li graf-after--p">Q3 is the 75th percentile</li><li name="de05" id="de05" class="graf graf--li graf-after--li">Q1 is the 25th percentile</li></ul><p name="16be" id="16be" class="graf graf--p graf-after--li">Note that Q2 (50th percentile) is also called the median.</p><p name="e125" id="e125" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Definition of Outliers</strong></p><p name="3a08" id="3a08" class="graf graf--p graf-after--p">An outlier is an observation that lies outside the overall pattern of distribution. There are two types of outliers.</p><ul class="postList"><li name="10ff" id="10ff" class="graf graf--li graf-after--p">Normal Outlier: Outliers that can be detected within a column. It is defined mathematically as,</li></ul><figure name="815f" id="815f" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*-RihvDU-904VUPqOK_UibA.png" data-width="1348" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*-RihvDU-904VUPqOK_UibA.png"></figure><p name="1e79" id="1e79" class="graf graf--p graf-after--figure">Note that there are many other definitions of outliers and you can find them in this <a href="https://www.lexjansen.com/nesug/nesug07/sa/sa16.pdf" data-href="https://www.lexjansen.com/nesug/nesug07/sa/sa16.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">paper</a>.</p><ul class="postList"><li name="b7a5" id="b7a5" class="graf graf--li graf-after--p">Joint Outliers: Outliers that occur when comparing relationships between two sets of data.</li></ul><p name="4003" id="4003" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Outlier Detection</strong></p><ul class="postList"><li name="5613" id="5613" class="graf graf--li graf-after--p">Histogram: The data lies outside the distribution of a feature is an outlier</li></ul><figure name="9d77" id="9d77" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*czsxtLP3xFsk4dolatg0bQ.png" data-width="1802" data-height="486" src="https://cdn-images-1.medium.com/max/800/1*czsxtLP3xFsk4dolatg0bQ.png"></figure><ul class="postList"><li name="44f6" id="44f6" class="graf graf--li graf-after--figure">Box Plot: The data that lies outside the minimum-maximum range is an outlier. This is the simplest way for outlier detection.</li></ul><figure name="f6de" id="f6de" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*xTL7AoOLtdGPmZCPAihRGA.png" data-width="2138" data-height="486" src="https://cdn-images-1.medium.com/max/800/1*xTL7AoOLtdGPmZCPAihRGA.png"></figure><ul class="postList"><li name="807a" id="807a" class="graf graf--li graf-after--figure">Scatter Plot: However, the joint outliers can not be detected by the box plot because the data is still in the distribution of each column. In this case, we may think about using a scatter plot instead of a box plot for finding the outlier.</li></ul><figure name="12d6" id="12d6" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*-OgCIAl2w0mSwOXPViTnag.png" data-width="1852" data-height="528" src="https://cdn-images-1.medium.com/max/800/1*-OgCIAl2w0mSwOXPViTnag.png"><figcaption class="imageCaption">point (1, 29) is an outlier but it is not identified in the box plot</figcaption></figure><p name="ab7f" id="ab7f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Outlier Solutions</strong></p><ul class="postList"><li name="8921" id="8921" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Drop Row</strong>: A human mistake or error will definitely not be helpful for prediction. But be careful about this because we may lose some critical information.</li><li name="35f7" id="35f7" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Imputation</strong>: Even if the outlier is confirmed to be a mistake, we may still want to take the advantage of other information in this record. So instead of dropping the row, we can impute the outlier. Common imputation methods include using the mean of a variable or utilizing a regression model to predict the missing value.</li><li name="be92" id="be92" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Capping/Flooring</strong>: This is the simplest way to deal with the errors by capping the outliers with the 1st percentile or 99th percentile of the data.</li><li name="a498" id="a498" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Transformation</strong>: When the outlier is not caused by mistake. For example, California has a way higher drug cost compared with other states. We can think about using the log-transformed data rather than using the data itself.</li><li name="ee5e" id="ee5e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Consider a Tree-Based Model</strong>: Trees are extremely good at handling outliers because they will shunt them to small leaves.</li></ul><p name="b5a5" id="b5a5" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">3. Handling Categorical Data</strong></p><p name="e94b" id="e94b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Label Encoding</strong></p><p name="abc5" id="abc5" class="graf graf--p graf-after--p">Typically, with label encoding, categorical values are replaced with a numeric value between 0 or 1 and the number of classes minus 1 or the number of classes.</p><figure name="b413" id="b413" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*e3sQWvfXtZMQO1ujkvgV-g.png" data-width="1322" data-height="344" src="https://cdn-images-1.medium.com/max/800/1*e3sQWvfXtZMQO1ujkvgV-g.png"><figcaption class="imageCaption">The 2nd column is a label encoded result</figcaption></figure><p name="0235" id="0235" class="graf graf--p graf-after--figure">The advantages of label encoding are,</p><ul class="postList"><li name="0e86" id="0e86" class="graf graf--li graf-after--p">Less memory consumption because only 1 column is added</li><li name="c9e0" id="c9e0" class="graf graf--li graf-after--li">Works well with tree-based models because tree-based models don’t take ordinal information into account while splitting</li><li name="672b" id="672b" class="graf graf--li graf-after--li">Works well with ordinal category data</li></ul><p name="cc82" id="cc82" class="graf graf--p graf-after--li">The drawbacks of label encoding are,</p><ul class="postList"><li name="4383" id="4383" class="graf graf--li graf-after--p">Implies the order of categories (e.g. Broccoli &gt; Chicken &gt; Apple)</li><li name="1e29" id="1e29" class="graf graf--li graf-after--li">Implies the non-existed relationship (e.g. mean(Broccoli, Apple) = Chicken)</li><li name="1ebd" id="1ebd" class="graf graf--li graf-after--li">Can not be used with linear models</li><li name="b133" id="b133" class="graf graf--li graf-after--li">Can not be used with neural networks</li></ul><p name="a4c8" id="a4c8" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) One-Hot Encoding (OHE)</strong></p><p name="3281" id="3281" class="graf graf--p graf-after--p">OHE converts each categorical value into a new categorical column and assigns a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector.</p><figure name="f7d9" id="f7d9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Wq_VKVqmf8xgpq0TjFIQVw.png" data-width="1316" data-height="298" src="https://cdn-images-1.medium.com/max/800/1*Wq_VKVqmf8xgpq0TjFIQVw.png"></figure><p name="eec4" id="eec4" class="graf graf--p graf-after--figure">The advantages of OHE are,</p><ul class="postList"><li name="a8e6" id="a8e6" class="graf graf--li graf-after--p">Works well with tree-based models but we commonly don’t choose it because it is very expensive</li><li name="1c47" id="1c47" class="graf graf--li graf-after--li">Works well with linear models and neural networks</li></ul><p name="7031" id="7031" class="graf graf--p graf-after--li">The drawbacks of OHE are,</p><ul class="postList"><li name="2dcc" id="2dcc" class="graf graf--li graf-after--p">Expensive memory consumption because only # of classes of columns are added. This is also called the curse of dimensionality. We may need to apply dimensional reduction methods like PCA.</li><li name="ac23" id="ac23" class="graf graf--li graf-after--li">Can not be applied to ordinal category data because we lose information</li></ul><p name="dc8e" id="dc8e" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Trade-off: Hash Encoding</strong></p><p name="a4c7" id="a4c7" class="graf graf--p graf-after--p">We have discussed that label encoding is extremely good at the cost but it can not be used for linear models or neural networks. Besides, OHE is extremely good for working with linear models or neural networks, but it is not appropriate to use because we will have too many dimensions. A trade-off approach to these two methods is hash encoding. It maps each category into a vector of <em class="markup--em markup--p-em">d</em> dimensions, and we have,</p><ul class="postList"><li name="159d" id="159d" class="graf graf--li graf-after--p">when <code class="markup--code markup--li-code">d = 1</code>: it is equivalent to label encoding</li><li name="7513" id="7513" class="graf graf--li graf-after--li">when <code class="markup--code markup--li-code">d = # of classes</code> : it is equivalent to one-hot encoding</li></ul><p name="a62d" id="a62d" class="graf graf--p graf-after--li">Commonly, we can assign dimension <em class="markup--em markup--p-em">d</em> to a value between 3 to 5 for avoiding a high cost of encoding.</p><p name="7df8" id="7df8" class="graf graf--p graf-after--p">The advantages of hash encoding are,</p><ul class="postList"><li name="46d5" id="46d5" class="graf graf--li graf-after--p">It is faster and memory-efficient than OHE</li><li name="baeb" id="baeb" class="graf graf--li graf-after--li">It works well with linear models and neural networks</li><li name="ae15" id="ae15" class="graf graf--li graf-after--li">It can work with tree-based models but again, label encoding is enough for these models</li></ul><p name="e66b" id="e66b" class="graf graf--p graf-after--li">The drawbacks of hash encoding are,</p><ul class="postList"><li name="4cd3" id="4cd3" class="graf graf--li graf-after--p">There can be a low probability of collisions because of the hash function. But the probability is very low so that it can be ignored.</li><li name="5955" id="5955" class="graf graf--li graf-after--li">We can not interpret our features after encoding</li><li name="d7c2" id="d7c2" class="graf graf--li graf-after--li">Feature Importance no longer works</li><li name="a920" id="a920" class="graf graf--li graf-after--li">The dimension of the hash vector is a hyperparameter and it is hard to decide.</li></ul><p name="4199" id="4199" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Categorical Embeddings</strong></p><p name="a77a" id="a77a" class="graf graf--p graf-after--p">An embedding is a vector representation of a categorical variable. We will take more about it in the future. This can be used for neural networks and linear models.</p><p name="17ef" id="17ef" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4. Feature Engineering</strong></p><p name="96e9" id="96e9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Categorical Feature Engineering</strong></p><ul class="postList"><li name="1c32" id="1c32" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Target/Mean Encoding</strong></li></ul><p name="5377" id="5377" class="graf graf--p graf-after--li">Compute the mean per category for the categorical feature. For overfitting, we have to generalize this encoding by cross-validation. For avoiding zero means, we can consider applying Laplace smoothing.</p><figure name="b03d" id="b03d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CtAcwwZmaxmyhd3fM45L7g.png" data-width="1600" data-height="276" src="https://cdn-images-1.medium.com/max/800/1*CtAcwwZmaxmyhd3fM45L7g.png"></figure><ul class="postList"><li name="d905" id="d905" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Frequency Encoding</strong></li></ul><p name="e80b" id="e80b" class="graf graf--p graf-after--li">Make a column with the frequency of each category.</p><figure name="44b9" id="44b9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Uymv9HFmvU9yi3fQy_8ScA.png" data-width="1312" data-height="228" src="https://cdn-images-1.medium.com/max/800/1*Uymv9HFmvU9yi3fQy_8ScA.png"></figure><ul class="postList"><li name="e4ab" id="e4ab" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Expansion Encoding</strong></li></ul><p name="4528" id="4528" class="graf graf--p graf-after--li">We can create multiple variables from a high cardinality variable like strings or JSON results.</p><p name="2fb5" id="2fb5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Numerical Feature Engineering</strong></p><ul class="postList"><li name="c752" id="c752" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Binning Encoding</strong></li></ul><p name="aab6" id="aab6" class="graf graf--p graf-after--li">In this method, the data is first sorted and then the sorted values are distributed into a number of buckets or bins. It requires some heuristic or domain knowledge. For example, ages to generations.</p><ul class="postList"><li name="a05a" id="a05a" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Rank/Quantile Transformation</strong></li></ul><p name="1cfb" id="1cfb" class="graf graf--p graf-after--li">This is a method for map values to rank or quantiles. It can smooth out the distribution and wrap out the outliers.</p><ul class="postList"><li name="379d" id="379d" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Power Transformation</strong></li></ul><p name="fdc3" id="fdc3" class="graf graf--p graf-after--li">This is a common way of transforming data that is not normally distributed. For example, if the data is positive and right-skewed, we can use <code class="markup--code markup--p-code">log</code> or <code class="markup--code markup--p-code">sqrt</code> for transformation.</p><ul class="postList"><li name="9cd3" id="9cd3" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Interaction Transformation</strong></li></ul><p name="53c3" id="53c3" class="graf graf--p graf-after--li">We can also encode interactions between variables like subtractions and addition.</p><p name="a33b" id="a33b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">5. Imbalanced Data</strong></p><p name="3fbf" id="3fbf" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Examples of Imbalanced Data</strong></p><ul class="postList"><li name="1f5a" id="1f5a" class="graf graf--li graf-after--p">Cancer Diagnosis</li><li name="8b05" id="8b05" class="graf graf--li graf-after--li">Tumor Classification</li><li name="795f" id="795f" class="graf graf--li graf-after--li">Credit Card Fraud</li><li name="dfda" id="dfda" class="graf graf--li graf-after--li">Computer Version</li></ul><p name="d139" id="d139" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Undersampling Major Class</strong></p><ul class="postList"><li name="632c" id="632c" class="graf graf--li graf-after--p">Random undersampling</li></ul><p name="9a79" id="9a79" class="graf graf--p graf-after--li">The simplest undersampling technique involves randomly selecting examples from the majority class and deleting them from the training dataset. This can lead to deleted useful information.</p><ul class="postList"><li name="3032" id="3032" class="graf graf--li graf-after--p">Near miss undersampling</li></ul><p name="012a" id="012a" class="graf graf--p graf-after--li">Select samples based on the distance of majority class examples to minority class examples. Typically, there are three methods for this technique,</p><figure name="3cb6" id="3cb6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UjZbkM6oIyuQ5ytqs-Y51A.png" data-width="1418" data-height="290" src="https://cdn-images-1.medium.com/max/800/1*UjZbkM6oIyuQ5ytqs-Y51A.png"><figcaption class="imageCaption">Source: <a href="http://glemaitre.github.io/imbalanced-learn/auto_examples/under-sampling/plot_nearmiss.html" data-href="http://glemaitre.github.io/imbalanced-learn/auto_examples/under-sampling/plot_nearmiss.html" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">link</a></figcaption></figure><p name="0ef4" id="0ef4" class="graf graf--p graf-after--figure">a. <code class="markup--code markup--p-code">NearMiss-1</code> : Select majority class examples based on the minimum average distance to the <code class="markup--code markup--p-code">n_neighbors</code> closest minority class examples.</p><p name="8741" id="8741" class="graf graf--p graf-after--p">b. <code class="markup--code markup--p-code">NearMiss-2</code> : Select majority class examples based on the minimum average distance to the <code class="markup--code markup--p-code">n_neighbors</code> furthest minority class examples.</p><p name="3aae" id="3aae" class="graf graf--p graf-after--p">c. <code class="markup--code markup--p-code">NearMiss-3</code> : Select majority class examples based on the minimum average distance to all the minority class examples.</p><figure name="090e" id="090e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vX8-sc2dL2vfndbDDEnpPw.png" data-width="1400" data-height="270" src="https://cdn-images-1.medium.com/max/800/1*vX8-sc2dL2vfndbDDEnpPw.png"><figcaption class="imageCaption">Note that we have assigned n_neighbors = 3 in the case above</figcaption></figure><ul class="postList"><li name="0a29" id="0a29" class="graf graf--li graf-after--figure">Condensed Nearest Neighbors (CNN)</li></ul><p name="739c" id="739c" class="graf graf--p graf-after--li">CNN seeks a subset of a collection of samples that results in no loss in model performance. This is implemented by enumerating the examples in the majority class and trying to classify them with a dataset starting with the minority class.</p><pre name="4e82" id="4e82" class="graf graf--pre graf-after--p">// Note: negative is major, positive is minor<br>Step 1: E = {positive examples}<br>Step 2: Loop through the whole majority dataset<br>Step 2-1: If we can not classify correctly by E with nearest neighbor algorithm, put data in E<br>Step 2-2: Else, discard the current data<br>Step 3: Break the loop when we traversed the whole majority set<br>Step 4: Return E as the balanced set</pre><ul class="postList"><li name="ff56" id="ff56" class="graf graf--li graf-after--pre">Tomek Links Extension</li></ul><p name="ea7e" id="ea7e" class="graf graf--p graf-after--li">We will not talk about this one in detail but this is very popular recently. Tomek defines the decision boundary where we can sample around. Note that x1 and x2 are considered to have a Tomek Link if a) the nearest neighbor of x1 is x2; 2) the nearest neighbor of x2 is x1; and 3) x1 and x2 belong to different classes. An application of Tomek Link is called <strong class="markup--strong markup--p-strong">One-Sided Selection</strong>,</p><pre name="6db1" id="6db1" class="graf graf--pre graf-after--p">Step 1: D = random({negative examples})<br>Step 2: E = {positive examples}<br>Step 3: Classify each x in D with E<br>Step 4: Move all the misclassified x into E<br>Step 5: If x == Tomek Links, E.remove(x)<br>Step 6: Return E</pre><p name="771a" id="771a" class="graf graf--p graf-after--pre">Step 5 removes from <code class="markup--code markup--p-code">E</code> all negative examples participating in Tomek Links. This removes those negative examples which are considered borderline or noisy. Remember that all positive examples are retained in the set.</p><ul class="postList"><li name="1194" id="1194" class="graf graf--li graf-after--p">Edited Nearest Neighbor Rule (ENN)</li></ul><pre name="ecb7" id="ecb7" class="graf graf--pre graf-after--li">Step 1: For x in E = {training set examples}<br>Step 2: NNs = x.getThreeNearestNeighbor()<br>Step 3: If x in major and kNN_clf(x, NNs) != x, E.remove(x)<br>Step 4: If x in minor and kNN_clf(x, NNs) != x, E.remove(NNs)<br>Step 5: Back to Step 1<br>Step 6: Return E</pre><p name="37c5" id="37c5" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(3) Over-sampling Minor Class</strong></p><ul class="postList"><li name="445b" id="445b" class="graf graf--li graf-after--p">Random oversampling</li></ul><p name="0e5e" id="0e5e" class="graf graf--p graf-after--li">Randomly duplicate examples from the minority class.</p><ul class="postList"><li name="d7b7" id="d7b7" class="graf graf--li graf-after--p">Synthetic Minority Oversampling Technique (SMOTE)</li></ul><p name="f518" id="f518" class="graf graf--p graf-after--li">The minority class is over-sampled by creating synthetic examples along with the line segments of k minority class nearest neighbors.</p><p name="4952" id="4952" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Metrics for Imbalanced Data</strong></p><p name="e51b" id="e51b" class="graf graf--p graf-after--p graf--trailing">We should not use accuracy for imbalanced data, instead, we have to consider the recall rate (sensitivity).</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/734e1ece8fe6"><time class="dt-published" datetime="2021-12-15T18:04:52.763Z">December 15, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/intro-to-machine-learning-8-data-processing-methods-734e1ece8fe6" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>