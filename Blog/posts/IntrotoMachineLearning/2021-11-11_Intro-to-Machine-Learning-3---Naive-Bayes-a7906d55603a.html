<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Intro to Machine Learning 3 | Naive Bayes</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Intro to Machine Learning 3 | Naive Bayes</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Intro to Machine Learning
</section>
<section data-field="body" class="e-content">
<section name="85f1" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="f380" id="f380" class="graf graf--h3 graf--leading graf--title">Intro to Machine Learning 3 | Naive Bayes</h3><figure name="1b63" id="1b63" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*JToEwmTgDBzf9Fza.png" data-width="1250" data-height="700" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*JToEwmTgDBzf9Fza.png"></figure><ol class="postList"><li name="d683" id="d683" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Spam-Ham Classification</strong></li></ol><p name="e1fe" id="e1fe" class="graf graf--p graf-after--li">Before we begin, let’s see our basic example for this model. Suppose we have tons of emails and we have already known that 75% of them are hams and 25% of them are spam.</p><p name="96bd" id="96bd" class="graf graf--p graf-after--p">If we randomly pick one email from our mailbox, can we know if this email is spam or ham?</p><p name="fdcf" id="fdcf" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Maximum A Priori Classifier</strong></p><p name="7fb0" id="7fb0" class="graf graf--p graf-after--p">Let’s suppose we do not have any more information about the email we have picked up, and the only information we know is the prior probability.</p><figure name="91cd" id="91cd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*GKpTdLVHKOGzeDpXOszHZg.png" data-width="1342" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*GKpTdLVHKOGzeDpXOszHZg.png"></figure><p name="aff4" id="aff4" class="graf graf--p graf-after--figure">So the a priori classifier should be,</p><ul class="postList"><li name="a427" id="a427" class="graf graf--li graf-after--p">Predict Ham if,</li></ul><figure name="f4d7" id="f4d7" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*V8i2ZMV-VdRvTVRxQEqPRw.png" data-width="1342" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*V8i2ZMV-VdRvTVRxQEqPRw.png"></figure><ul class="postList"><li name="7d61" id="7d61" class="graf graf--li graf-after--figure">Predict Spam if,</li></ul><figure name="597e" id="597e" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*_NVrt0Zk74VIVMnvSNq0aQ.png" data-width="1342" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*_NVrt0Zk74VIVMnvSNq0aQ.png"></figure><p name="e633" id="e633" class="graf graf--p graf-after--figure">This is also,</p><figure name="93be" id="93be" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8lb18IU0HYleZwvXXFvqhg.png" data-width="1342" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*8lb18IU0HYleZwvXXFvqhg.png"><figcaption class="imageCaption">where c is the category of emails (i.e. spam or ham)</figcaption></figure><p name="36d3" id="36d3" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">3. Maximum Likelihood (MLE) Classifier</strong></p><p name="f8fa" id="f8fa" class="graf graf--p graf-after--p">Let’s suppose we do have some extra information about the email because we know some words in that email. Suppose the words are,</p><pre name="fca2" id="fca2" class="graf graf--pre graf-after--p">Viagra sale</pre><p name="46a9" id="46a9" class="graf graf--p graf-after--pre">So the likelihood classifier based on this information should be,</p><ul class="postList"><li name="05ab" id="05ab" class="graf graf--li graf-after--p">Predict Ham if,</li></ul><figure name="70d6" id="70d6" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*NoF_9Adga0EFw1owp8yN-w.png" data-width="1342" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*NoF_9Adga0EFw1owp8yN-w.png"></figure><ul class="postList"><li name="9ccb" id="9ccb" class="graf graf--li graf-after--figure">Predict Spam if,</li></ul><figure name="9cd0" id="9cd0" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*cR6A25LOa96IzVAlG1ZTrQ.png" data-width="1342" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*cR6A25LOa96IzVAlG1ZTrQ.png"></figure><p name="e59d" id="e59d" class="graf graf--p graf-after--figure">This is also,</p><figure name="d430" id="d430" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AKVXogGpnfMDCyzDNWzQWw.png" data-width="1334" data-height="82" src="https://cdn-images-1.medium.com/max/800/1*AKVXogGpnfMDCyzDNWzQWw.png"><figcaption class="imageCaption">where d is the bag of word information (i.e. Viagra and sale)</figcaption></figure><p name="4b0f" id="4b0f" class="graf graf--p graf-after--figure">Although this model works well, it doesn’t take into the knowledge of the a priori probability.</p><p name="3c3a" id="3c3a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4. Maximum A Posteriori (MAP) Classifier</strong></p><p name="888c" id="888c" class="graf graf--p graf-after--p">If we want to leverage the a priori probability to our classifier, we may need to use the maximum a posteriori classifier. By Bayes’ Theorem, the a posteriori probability is defined by,</p><figure name="2ab2" id="2ab2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gjtGNscuvv2DkQddlKXG1A.png" data-width="1342" data-height="118" src="https://cdn-images-1.medium.com/max/800/1*gjtGNscuvv2DkQddlKXG1A.png"></figure><p name="38c6" id="38c6" class="graf graf--p graf-after--figure">So for this classifier, we would</p><ul class="postList"><li name="a130" id="a130" class="graf graf--li graf-after--p">Predict Ham if,</li></ul><figure name="8d2d" id="8d2d" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*YY82NUuU1HG4l82jNMPOYw.png" data-width="1452" data-height="118" src="https://cdn-images-1.medium.com/max/800/1*YY82NUuU1HG4l82jNMPOYw.png"></figure><ul class="postList"><li name="dc52" id="dc52" class="graf graf--li graf-after--figure">Predict Spam if,</li></ul><figure name="a03f" id="a03f" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*cIWPan3sR2gz-Def7DnI2g.png" data-width="1452" data-height="118" src="https://cdn-images-1.medium.com/max/800/1*cIWPan3sR2gz-Def7DnI2g.png"></figure><p name="fd41" id="fd41" class="graf graf--p graf-after--figure">Because the denominators are just the same, we can cancel them out directly. So we will,</p><ul class="postList"><li name="e32b" id="e32b" class="graf graf--li graf-after--p">Predict Ham if,</li></ul><figure name="515b" id="515b" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*9cNKuTHApOWp1cyGXLCraw.png" data-width="1436" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*9cNKuTHApOWp1cyGXLCraw.png"></figure><ul class="postList"><li name="c54c" id="c54c" class="graf graf--li graf-after--figure">Predict Spam if,</li></ul><figure name="aadc" id="aadc" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*GKl2QshsspwETA6tIjdTcQ.png" data-width="1436" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*GKl2QshsspwETA6tIjdTcQ.png"></figure><p name="0ee5" id="0ee5" class="graf graf--p graf-after--figure">This is also,</p><figure name="1ede" id="1ede" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AeyLqlxq3bqPFbiBBTZFFw.png" data-width="1334" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*AeyLqlxq3bqPFbiBBTZFFw.png"></figure><p name="be14" id="be14" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">5. Naive Assumption</strong></p><p name="761a" id="761a" class="graf graf--p graf-after--p">The naive assumption means <strong class="markup--strong markup--p-strong">conditional independence</strong> for all the words. This is applied because we don’t want to exhaust all the combinations of words in that document.</p><figure name="9865" id="9865" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*GQ6C6bMn_qexUbKXq8zTTQ.png" data-width="1334" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*GQ6C6bMn_qexUbKXq8zTTQ.png"></figure><p name="7203" id="7203" class="graf graf--p graf-after--figure">For example, we can have,</p><figure name="b2a8" id="b2a8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VAhhfa96wjmnI1hlDgkerA.png" data-width="1334" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*VAhhfa96wjmnI1hlDgkerA.png"></figure><p name="fe52" id="fe52" class="graf graf--p graf-after--figure">So the maximum a posteriori (MAP) classifier could then be,</p><figure name="bbb0" id="bbb0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6PRgZay072pmRq12RZgHxA.png" data-width="1334" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*6PRgZay072pmRq12RZgHxA.png"><figcaption class="imageCaption">where w is the word in that bag of words d</figcaption></figure><p name="287d" id="287d" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">6. Fixed-Length Word-Count Vector</strong></p><p name="f1f4" id="f1f4" class="graf graf--p graf-after--p">Note that in the last classifier, we have to construct a different classifier for each document, this is because of the product of word likelihoods in each document. Rather than arbitrary-length word vectors for each document d, it’s much easier to use fixed-length vectors of size <code class="markup--code markup--p-code">|V|</code> with all the words across the documents. So the classifier becomes,</p><figure name="ca4d" id="ca4d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*LiQtBMx7J6J_Wqw3BAI7Vg.png" data-width="1350" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*LiQtBMx7J6J_Wqw3BAI7Vg.png"><figcaption class="imageCaption">where V is the vocabulary across documents</figcaption></figure><p name="3dd4" id="3dd4" class="graf graf--p graf-after--figure">Here we have the function <em class="markup--em markup--p-em">n_w</em>(<em class="markup--em markup--p-em">d</em>) depending on <em class="markup--em markup--p-em">d</em>, and this means how many times the word <em class="markup--em markup--p-em">w</em> appears in document <em class="markup--em markup--p-em">d</em>.</p><p name="2295" id="2295" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">7. Estimating Likelihood</strong></p><ul class="postList"><li name="a959" id="a959" class="graf graf--li graf-after--p">For long documents</li></ul><p name="fd5f" id="fd5f" class="graf graf--p graf-after--li">The likelihood <strong class="markup--strong markup--p-strong">P</strong>(<em class="markup--em markup--p-em">w</em>|<em class="markup--em markup--p-em">c</em>) can be estimated by the number of times <em class="markup--em markup--p-em">w</em> appears in all documents from class <em class="markup--em markup--p-em">c</em> divided by the total number of words (including repeats) in all documents from class <em class="markup--em markup--p-em">c</em>.</p><figure name="51ed" id="51ed" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0tnJt4al-FuxDt-PfIu3pg.png" data-width="1350" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*0tnJt4al-FuxDt-PfIu3pg.png"></figure><ul class="postList"><li name="93d3" id="93d3" class="graf graf--li graf-after--figure">For short documents (like tweets)</li></ul><p name="cb12" id="cb12" class="graf graf--p graf-after--li">The likelihood <strong class="markup--strong markup--p-strong">P</strong>(<em class="markup--em markup--p-em">w</em>|<em class="markup--em markup--p-em">c</em>) can be estimated by the number of documents with <em class="markup--em markup--p-em">w</em> in class <em class="markup--em markup--p-em">c</em> divided by the number of docs in <em class="markup--em markup--p-em">c</em>.</p><figure name="15cf" id="15cf" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5JhvuAmxvR8nst7m5M7VOQ.png" data-width="1350" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*5JhvuAmxvR8nst7m5M7VOQ.png"></figure><p name="e62c" id="e62c" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">8. Laplace Smoothing for 0 Likelihoods</strong></p><p name="07ec" id="07ec" class="graf graf--p graf-after--p">Because we can have a product shrinking to 0 if there is at least one word in the vocabulary that doesn’t appear in the document, which also means at least one likelihood would be 0. To deal with this problem, we have to apply <strong class="markup--strong markup--p-strong">Laplace Smoothing</strong> (aka. additive smoothing) technique to the likelihoods.</p><p name="4698" id="4698" class="graf graf--p graf-after--p">The Laplace smoothing is defined by,</p><figure name="6eef" id="6eef" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mTyHI5vURdwY6BtjdUqfmA.png" data-width="1340" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*mTyHI5vURdwY6BtjdUqfmA.png"><figcaption class="imageCaption">Laplace smoothing when α = 1, Lidstone smoothing when α &lt; 1</figcaption></figure><p name="35f0" id="35f0" class="graf graf--p graf-after--figure">Leverage this to our likelihood,</p><figure name="9296" id="9296" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rLi0jGJznSd16nyIiKLnMA.png" data-width="1350" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*rLi0jGJznSd16nyIiKLnMA.png"><figcaption class="imageCaption">Let α = 1 and d should be the length of vocabulary |V|</figcaption></figure><p name="be85" id="be85" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">9. Unknown Word Problem</strong></p><p name="f3a0" id="f3a0" class="graf graf--p graf-after--p">Now we have one remaining problem. Suppose we would like to predict the class of a document and that document has some words that never exist in the vocabulary (i.e. “unknown” words). The solution is that we can map all unknown w to a wildcard word in V so then <code class="markup--code markup--p-code">wordcount(unknown, c) = 0</code> is ok but <code class="markup--code markup--p-code">|V|</code> is 1 word longer.</p><figure name="6f1e" id="6f1e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*IBxcQJRK7fB9ha4qId416w.png" data-width="1350" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*IBxcQJRK7fB9ha4qId416w.png"></figure><p name="19c1" id="19c1" class="graf graf--p graf-after--figure">So the likelihood for an unknown word should be,</p><figure name="541d" id="541d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VbfERrA4FbYiCwmKHgffpg.png" data-width="1340" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*VbfERrA4FbYiCwmKHgffpg.png"></figure><p name="9a42" id="9a42" class="graf graf--p graf-after--figure">Also, for a word that exists in the vocabulary but doesn’t exist in the document, the likelihood should also be,</p><figure name="9f4e" id="9f4e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*LuzTJMtAEbTwpeFKZR2dkA.png" data-width="1340" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*LuzTJMtAEbTwpeFKZR2dkA.png"></figure><p name="8dcc" id="8dcc" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">10. Floating Point Underflow Problem</strong></p><p name="ac00" id="ac00" class="graf graf--p graf-after--p">Let’s imagine a situation when we have 30,000 words across documents in class <em class="markup--em markup--p-em">c</em>, and the vocabulary length is 1,200. Then the likelihood for a word exists in the vocabulary but doesn’t exist in the document or an unknown word should be,</p><pre name="3519" id="3519" class="graf graf--pre graf-after--p">p(w|c) = 1 / (30000 + 1200 + 1) = 0.000033</pre><p name="b38d" id="b38d" class="graf graf--p graf-after--pre">For any specific document, we must have this situation several times and the final product will be too small for us to compare (because the computer can only accept a certain range of precision). In order to avoid these vanishing floating-point values from the product, commonly we will take the <strong class="markup--strong markup--p-strong">log-likelihood method</strong>. Then the classifier becomes,</p><figure name="0010" id="0010" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Th36CCvSniUuTKh7ox53_w.png" data-width="1340" data-height="120" src="https://cdn-images-1.medium.com/max/800/1*Th36CCvSniUuTKh7ox53_w.png"></figure><p name="7ef3" id="7ef3" class="graf graf--p graf-after--figure">This is called the <strong class="markup--strong markup--p-strong">Multinomial Naive Bayes classifier</strong>.</p><p name="f839" id="f839" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">11. Categorical Naive Bayes Classifier</strong></p><p name="a246" id="a246" class="graf graf--p graf-after--p">Now that we have completed the discussion of the spam-ham classification example, let’s see a more generalized version of the Naive Bayes Classifier.</p><figure name="1ac7" id="1ac7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CcBdBXdFRFoTuq-ewshnbQ.png" data-width="1340" data-height="170" src="https://cdn-images-1.medium.com/max/800/1*CcBdBXdFRFoTuq-ewshnbQ.png"></figure><p name="4066" id="4066" class="graf graf--p graf-after--figure">Let’s see an example. Suppose we have the label = {Apple, Banana, Strawberry}, Color = {Red, Green, Yellow}, Weight = {&gt; 20g, &lt; 20g}. Suppose we have the following data set,</p><pre name="d302" id="d302" class="graf graf--pre graf-after--p">Label            Color           Weight<br>----------       -----------     ------------<br>Banana           Yellow          &gt;20g<br>Banana           Green           &gt;20g<br>Banana           Yellow          &gt;20g<br>Banana           Yellow          &gt;20g<br>Banana           Yellow          &gt;20g<br>Apple            Red             &gt;20g<br>Apple            Green           &gt;20g<br>Apple            Green           &gt;20g<br>Apple            Red             &gt;20g<br>Strawberry       Red             &lt;20g<br>Strawberry       Red             &lt;20g<br>Strawberry       Red             &lt;20g<br>Strawberry       Red             &lt;20g</pre><p name="8cd7" id="8cd7" class="graf graf--p graf-after--pre">Based on this dataset, let’s predict the class of the following fruits by Naive Bayes,</p><pre name="1ff7" id="1ff7" class="graf graf--pre graf-after--p">Color           Weight<br>----------      ------------<br>Yellow          &lt;20g<br>Green           &lt;20g</pre><p name="f763" id="f763" class="graf graf--p graf-after--pre">Let’s first calculate the parameters,</p><pre name="011b" id="011b" class="graf graf--pre graf-after--p">P(Banana) = 5/13<br>P(Apple) = 4/13<br>P(Strawberry) = 4/13</pre><pre name="4645" id="4645" class="graf graf--pre graf-after--pre">P(Green|Banana) = 1/5<br>P(Yellow|Banana) = 4/5<br>P(Green|Apple) = 1/2<br>P(Red|Apple) = 1/2<br>P(Red|Strawberry) = 1<br>P(&gt;20|Banana) = 1<br>P(&gt;20|Apple) = 1<br>P(&lt;20|Strawberry) = 1</pre><pre name="4cb8" id="4cb8" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">Otherwise,</strong><br>P(x|y) = 0</pre><p name="990d" id="990d" class="graf graf--p graf-after--pre">By Laplace smoothing,</p><pre name="c64c" id="c64c" class="graf graf--pre graf-after--p">P(Green|Banana) = 2/8 = 1/4<br>P(Yellow|Banana) = 5/8<br>P(Red|Banana) = 1/8</pre><pre name="da37" id="da37" class="graf graf--pre graf-after--pre">P(Green|Apple) = 3/7<br>P(Yellow|Apple) = 1/7<br>P(Red|Apple) = 3/7</pre><pre name="0107" id="0107" class="graf graf--pre graf-after--pre">P(Green|Strawberry) = 1/7<br>P(Yellow|Strawberry) = 1/7<br>P(Red|Strawberry) = 5/7</pre><pre name="e0dc" id="e0dc" class="graf graf--pre graf-after--pre">P(&gt;20|Banana) = 6/7<br>P(&lt;20|Banana) = 1/7</pre><pre name="2dbd" id="2dbd" class="graf graf--pre graf-after--pre">P(&gt;20|Apple) = 5/6<br>P(&lt;20|Apple) = 1/6</pre><pre name="6878" id="6878" class="graf graf--pre graf-after--pre">P(&gt;20|Strawberry) = 1/6<br>P(&lt;20|Strawberry) = 5/6</pre><p name="fcd3" id="fcd3" class="graf graf--p graf-after--pre">For case #1, it will predict <code class="markup--code markup--p-code">Strawberry</code> because</p><pre name="642c" id="642c" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">y = Banana     : </strong><br>log(P(Banana))+log(P(Yellow|Banana))+log(P(&lt;20|Banana)) = -3.37</pre><pre name="205c" id="205c" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">y = Apple      : <br></strong>log(P(Apple))+log(P(Yellow|Apple))+log(P(&lt;20|Apple)) = -4.92</pre><pre name="3cda" id="3cda" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">y = Strawberry :</strong><br>log(P(Strawberry))+log(P(Yellow|Strawberry))+log(P(&lt;20|Strawberry)) = -3.31</pre><p name="6e8a" id="6e8a" class="graf graf--p graf-after--pre">For case #2, it will predict <code class="markup--code markup--p-code">Strawberry</code> because</p><pre name="2db0" id="2db0" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">y = Banana     : </strong><br>log(P(Banana))+log(P(Green|Banana))+log(P(&lt;20|Banana)) = -4.29</pre><pre name="4132" id="4132" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">y = Apple      : <br></strong>log(P(Apple))+log(P(Green|Apple))+log(P(&lt;20|Apple)) = -3.82</pre><pre name="21e8" id="21e8" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">y = Strawberry :</strong><br>log(P(Strawberry))+log(P(Green|Strawberry))+log(P(&lt;20|Strawberry)) = -3.31</pre><p name="d45c" id="d45c" class="graf graf--p graf-after--pre">Therefore, based on the given dataset, we can have,</p><pre name="d59d" id="d59d" class="graf graf--pre graf-after--p">Color           Weight            Pred<br>----------      ------------      ----------<br>Yellow          &lt;20g              Strawberry<br>Green           &lt;20g              Strawberry</pre><p name="f9cc" id="f9cc" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">12. Categorical Naive Bayes Classifier in Python</strong></p><p name="fa47" id="fa47" class="graf graf--p graf-after--p">Now, let’s repeat our result in of Naive Bayes classifier <code class="markup--code markup--p-code">CategoricalNB</code> above in Python.</p><pre name="d284" id="d284" class="graf graf--pre graf-after--p">from sklearn.naive_bayes import CategoricalNB</pre><pre name="098c" id="098c" class="graf graf--pre graf-after--pre"># Our fruit data<br>X = [[1, 1],<br>     [0, 1], <br>     [1, 1],<br>     [1, 1],<br>     [1, 1], <br>     [2, 1], <br>     [0, 1], <br>     [0, 1], <br>     [2, 1],<br>     [2, 0],<br>     [2, 0],<br>     [2, 0],<br>     [2, 0]]<br>y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]</pre><pre name="569b" id="569b" class="graf graf--pre graf-after--pre">clf = CategoricalNB()<br>clf.fit(X, y)</pre><pre name="8c4d" id="8c4d" class="graf graf--pre graf-after--pre"># predict [Yellow, &lt;20g] and [green, &lt;20g]<br>print(clf.predict([[1, 0], [0, 0]]))</pre><p name="8dc2" id="8dc2" class="graf graf--p graf-after--pre">The output should be,</p><pre name="af11" id="af11" class="graf graf--pre graf-after--p">[2 2]     # category 2 means Strawberry</pre><p name="9f0c" id="9f0c" class="graf graf--p graf-after--pre">We can also check the estimate parameters by <code class="markup--code markup--p-code">clf.coef_</code>,</p><pre name="a282" id="a282" class="graf graf--pre graf-after--p">import math</pre><pre name="6a2f" id="6a2f" class="graf graf--pre graf-after--pre">for i in clf.coef_[0]:<br>    print()<br>    for j in i:<br>        print(math.exp(j))</pre><pre name="a484" id="a484" class="graf graf--pre graf-after--pre">print(&quot;------------------&quot;)</pre><pre name="e7d5" id="e7d5" class="graf graf--pre graf-after--pre">for i in clf.coef_[1]:<br>    print()<br>    for j in i:<br>        print(math.exp(j))</pre><p name="3916" id="3916" class="graf graf--p graf-after--pre">The output should be,</p><pre name="87d2" id="87d2" class="graf graf--pre graf-after--p">0.25000000000000006<br>0.625<br>0.12500000000000003<br><br>0.42857142857142866<br>0.14285714285714288<br>0.42857142857142866<br><br>0.14285714285714288<br>0.14285714285714288<br>0.7142857142857143<br>------------------<br><br>0.14285714285714288<br>0.8571428571428572<br><br>0.16666666666666669<br>0.8333333333333333<br><br>0.8333333333333333<br>0.16666666666666669</pre><p name="33ad" id="33ad" class="graf graf--p graf-after--pre">This is exactly the same as the estimated likelihood after Laplace smoothing we have calculated.</p><p name="28ae" id="28ae" class="graf graf--p graf-after--p">However, for the word counts for text classification (e.g. spam or ham example) as we discussed at the beginning, it is better to use <code class="markup--code markup--p-code">MultinomialNB</code> by,</p><pre name="aaf8" id="aaf8" class="graf graf--pre graf-after--p">from sklearn.naive_bayes import MultinomialNB</pre><p name="e4fa" id="e4fa" class="graf graf--p graf-after--pre graf--trailing"><code class="markup--code markup--p-code">CategoricalNB</code> is more commonly used for classification with discrete features that are categorically distributed (e.g. the fruit example).</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/a7906d55603a"><time class="dt-published" datetime="2021-11-11T17:12:43.639Z">November 11, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/intro-to-machine-learning-3-naive-bayes-a7906d55603a" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>