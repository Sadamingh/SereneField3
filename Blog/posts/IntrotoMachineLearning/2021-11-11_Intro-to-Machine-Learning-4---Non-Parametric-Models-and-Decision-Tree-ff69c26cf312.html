<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Intro to Machine Learning 4 | Non-Parametric Models and Decision Tree</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Intro to Machine Learning 4 | Non-Parametric Models and Decision Tree</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Intro to Machine Learning
</section>
<section data-field="body" class="e-content">
<section name="5fb8" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="b90f" id="b90f" class="graf graf--h3 graf--leading graf--title">Intro to Machine Learning 4 | Non-Parametric Models and Decision Tree</h3><figure name="5158" id="5158" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*o7Pzz4WYFs1MoY7L.png" data-width="1250" data-height="700" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*o7Pzz4WYFs1MoY7L.png"></figure><ol class="postList"><li name="0f43" id="0f43" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Parametric Models Vs. Non-Parametric Models</strong></li></ol><p name="733f" id="733f" class="graf graf--p graf-after--li">Parametric models have a finite number of parameters depending on the feature properties. For example, suppose we have <em class="markup--em markup--p-em">N</em> boolean features (i.e. X) and a boolean label (i.e. Y)</p><ul class="postList"><li name="4958" id="4958" class="graf graf--li graf-after--p">Logistic Regression: N+1 parameters</li><li name="2e80" id="2e80" class="graf graf--li graf-after--li">Navie Bayes Classifier: (2N + 1) * 2 parameters</li><li name="0753" id="0753" class="graf graf--li graf-after--li">Neural Nets</li></ul><p name="9735" id="9735" class="graf graf--p graf-after--li">Non-parametric models mean we have an unbounded number of parameters depending on the dataset. This means if the number of model parameters changes with different <em class="markup--em markup--p-em">N</em> records, it’s nonparametric.</p><ul class="postList"><li name="1a0c" id="1a0c" class="graf graf--li graf-after--p"><em class="markup--em markup--li-em">k</em>NN</li><li name="82b6" id="82b6" class="graf graf--li graf-after--li">Random Forest</li><li name="db3d" id="db3d" class="graf graf--li graf-after--li">Gradient Boosting Machine</li></ul><p name="bf7f" id="bf7f" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. <em class="markup--em markup--p-em">k</em>-Nearest Neighbourhoods (<em class="markup--em markup--p-em">k</em>NN)</strong></p><p name="096e" id="096e" class="graf graf--p graf-after--p">kNN is less often used in practice, but it’s knowing how they work and kNN can still be very effective,</p><ul class="postList"><li name="3351" id="3351" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">k</em>NN Regressor</strong>: get <em class="markup--em markup--li-em">k </em>observations closest to unknown <em class="markup--em markup--li-em">x</em> using Euclidean distance then predict average y from those <em class="markup--em markup--li-em">k’s</em></li><li name="32aa" id="32aa" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">k</em>NN Classifier</strong>: get <em class="markup--em markup--li-em">k</em> observations closest to unknown <em class="markup--em markup--li-em">x</em> using Euclidean distance then predict the most common class from those <em class="markup--em markup--li-em">k’s</em></li></ul><p name="4c42" id="4c42" class="graf graf--p graf-after--li">The benefits of <em class="markup--em markup--p-em">k</em>NN are that,</p><ul class="postList"><li name="af88" id="af88" class="graf graf--li graf-after--p">No Training Period: the <em class="markup--em markup--li-em">k</em>NN learns from the training set only when it makes predictions</li><li name="03d0" id="03d0" class="graf graf--li graf-after--li">New data can be added seamlessly</li><li name="bedb" id="bedb" class="graf graf--li graf-after--li">Easy to implement</li></ul><p name="c807" id="c807" class="graf graf--p graf-after--li">The downsides of <em class="markup--em markup--p-em">k</em>NN are that,</p><ul class="postList"><li name="c7c2" id="c7c2" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Does not work well with large sets</strong>: the cost of calculating distance can be very high</li><li name="c6e5" id="c6e5" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Does not work well with high dimensions</strong>: it is more expensive to calculate the distance for high dimensions</li><li name="578f" id="578f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Need feature scaling</strong>: the <em class="markup--em markup--li-em">k</em>NN approach requires standardization and normalization</li><li name="226b" id="226b" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Sensitive to noise</strong>: need extra work for removing outliers and missing values</li></ul><p name="c127" id="c127" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">3. <em class="markup--em markup--p-em">k</em>NN Vs. Decision Tree</strong></p><p name="906a" id="906a" class="graf graf--p graf-after--p">One critical reason why we don’t use <em class="markup--em markup--p-em">k</em>NN in practice is the high cost of calculating the Euclidean distance.</p><figure name="8aa3" id="8aa3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*a5SyAtqn7LQCbnxc31US5A.png" data-width="1822" data-height="814" src="https://cdn-images-1.medium.com/max/800/1*a5SyAtqn7LQCbnxc31US5A.png"><figcaption class="imageCaption">Source: <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" data-href="http://scott.fortmann-roe.com/docs/BiasVariance.html" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></figcaption></figure><p name="7615" id="7615" class="graf graf--p graf-after--figure">To make the boundaries easier, let’s avoid inefficiency and the distance metric requirement of <em class="markup--em markup--p-em">k</em>NN by <strong class="markup--strong markup--p-strong">partitioning</strong> feature space into <strong class="markup--strong markup--p-strong">rectangular hypervolumes</strong> and then predicting the most common <em class="markup--em markup--p-em">y</em> (or label) in a hypervolume. These partition rules can be encoded as a tree called a decision tree. With the help of the decision tree, we can deal with the following problems,</p><ul class="postList"><li name="2c91" id="2c91" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Multiple Records Tolerance</strong>: just put similar records in the same leaf</li><li name="2b29" id="2b29" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Deal with Inexact Feature Match</strong></li><li name="ad60" id="ad60" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Reduce the High Cost of Calculating Distance</strong></li></ul><p name="ccef" id="ccef" class="graf graf--p graf-after--li">However, the decision tree still has a bunch of problems,</p><ul class="postList"><li name="1696" id="1696" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Overfitting</strong>: if there are no other constraints, the decision tree tends to split the feature space by rectangular hyper volumes until each leaf has a single observation. We commonly deal with it by adding constraints, enhancing <strong class="markup--strong markup--li-strong">bagging</strong> (using <strong class="markup--strong markup--li-strong">bootstrapping</strong> for weakening a decision tree), or assembly randomness (also called <strong class="markup--strong markup--li-strong">ensemble learning</strong>, e.g. random forest).</li></ul><figure name="4660" id="4660" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*Y0LfgGXz3Laf-FselTCQ6w.png" data-width="1872" data-height="402" src="https://cdn-images-1.medium.com/max/800/1*Y0LfgGXz3Laf-FselTCQ6w.png"></figure><p name="a202" id="a202" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">4. The Structure of Decision Tree</strong></p><ul class="postList"><li name="050a" id="050a" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Growing Direction</strong>: Upside down</li><li name="1d27" id="1d27" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Root Node</strong>: root node is the topmost node in a tree data structure</li><li name="e561" id="e561" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Internal Node</strong> (aka. <strong class="markup--strong markup--li-strong">split</strong>): internal nodes have one or more child nodes, and they are used to test features (or test variables at split points)</li><li name="bd20" id="bd20" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Leaf Node</strong> (aka. <strong class="markup--strong markup--li-strong">leaf</strong>): leaf node has no child nodes, and they are used to predict the region mean (for regressor) or predict the most common target category or mode (for classification)</li><li name="aa49" id="aa49" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Layer</strong>: each stage of nodes is called a layer, for example in the following figure, we have 3 layers</li><li name="7907" id="7907" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Depth</strong>: how many layers do we have in the tree</li></ul><figure name="3c9f" id="3c9f" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*IMX5B0T7snuxnvaEf4ifAw.png" data-width="1610" data-height="402" src="https://cdn-images-1.medium.com/max/800/1*IMX5B0T7snuxnvaEf4ifAw.png"></figure><p name="16d6" id="16d6" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">5. Different Splitting Methods</strong></p><ul class="postList"><li name="9ebe" id="9ebe" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Variance Reduction</strong> (Mean Square Error, MSE): for Decision Tree Regressor</li></ul><p name="39ea" id="39ea" class="graf graf--p graf-after--li">MSE for the mean model is the same as variance, and this technique can be used to split the node.</p><figure name="2b36" id="2b36" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EI4kEeVwB4gP3KaASQW4Vw.png" data-width="1342" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*EI4kEeVwB4gP3KaASQW4Vw.png"></figure><ul class="postList"><li name="f669" id="f669" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Entropy Reduction</strong>: for Decision Tree Classifier</li></ul><p name="c158" id="c158" class="graf graf--p graf-after--li">Entropy is a way to measure the chaos within the system, and a higher entropy means a higher uncertainty of the class <em class="markup--em markup--p-em">y </em>in that sample. So we have to reduce the entropy if we want to purify the sample.</p><figure name="829d" id="829d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*L_Jwgh86jZ8pRuCE-ljEfQ.png" data-width="1344" data-height="116" src="https://cdn-images-1.medium.com/max/800/1*L_Jwgh86jZ8pRuCE-ljEfQ.png"></figure><ul class="postList"><li name="6061" id="6061" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Gini Impurity Reduction</strong>: for Decision Tree Classifier</li></ul><p name="ce75" id="ce75" class="graf graf--p graf-after--li">Gini Impurity is another technique we can use to measure the class purity of a sample. It measures the uncertainty of the class <em class="markup--em markup--p-em">y </em>in that sample. Higher Gini Impurity means the sample is more uncertain, so we have to reduce the Gini Impurity if we want to purify the sample.</p><figure name="5e84" id="5e84" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*QBmwlxm-rEis6mrpF6_kuQ.png" data-width="1344" data-height="116" src="https://cdn-images-1.medium.com/max/800/1*QBmwlxm-rEis6mrpF6_kuQ.png"><figcaption class="imageCaption">where p_i is the proportion of each class in that node sample</figcaption></figure><p name="037c" id="037c" class="graf graf--p graf-after--figure">Note that Gini Impurity is similar to entropy but faster,</p><figure name="1a99" id="1a99" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kYsi4_3gcht-hcZxDrsZqA.png" data-width="1506" data-height="380" src="https://cdn-images-1.medium.com/max/800/1*kYsi4_3gcht-hcZxDrsZqA.png"></figure><p name="e7a0" id="e7a0" class="graf graf--p graf-after--figure">Let’s now see an example. Suppose we have a node sample of five values y = [1, 1, 1, 2, 0], and the predicted value of this node is 1.</p><p name="2ace" id="2ace" class="graf graf--p graf-after--p">The variance of this sample should be (if regressor),</p><pre name="748a" id="748a" class="graf graf--pre graf-after--p">variance = (0 + 0 + 0 + 1 + 1) / 5 = 0.4</pre><p name="2242" id="2242" class="graf graf--p graf-after--pre">The Gini Impurity should be (if classifier),</p><pre name="f8e7" id="f8e7" class="graf graf--pre graf-after--p">Gini Imputrity = 1 - (9/25 + 1/25 + 1/25) = 14/25 = 0.56</pre><p name="9fab" id="9fab" class="graf graf--p graf-after--pre">The Entropy should be (if classifier),</p><pre name="bf02" id="bf02" class="graf graf--pre graf-after--p">Entropy = -(log_2(3/5)*3/5 + log_2(1/5)*1/5 + log_2(1/5)*1/5)<br>= 1.37</pre><p name="6704" id="6704" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">6. Decision Tree in Python</strong></p><p name="24f3" id="24f3" class="graf graf--p graf-after--p">The decision tree model can be applied for regressing or classifying. In <code class="markup--code markup--p-code">sklearn</code>, these are,</p><ul class="postList"><li name="0e32" id="0e32" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Decision Tree Regressor</strong></li></ul><pre name="1365" id="1365" class="graf graf--pre graf-after--li">from sklearn.tree import DecisionTreeRegressor</pre><ul class="postList"><li name="9be3" id="9be3" class="graf graf--li graf-after--pre"><strong class="markup--strong markup--li-strong">Decision Tree Classifier</strong></li></ul><pre name="c488" id="c488" class="graf graf--pre graf-after--li">from sklearn.tree import DecisionTreeClassifier</pre><p name="6ea7" id="6ea7" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(1) Splitting Methods</strong></p><p name="bded" id="bded" class="graf graf--p graf-after--p">For the regressor, the splitting method is defined by <code class="markup--code markup--p-code">criterion</code> and we can choose among,</p><pre name="8a4d" id="8a4d" class="graf graf--pre graf-after--p">criterion = [“squared_error”, “friedman_mse”, “absolute_error”, “poisson”]</pre><p name="53b9" id="53b9" class="graf graf--p graf-after--pre">And the default one is <code class="markup--code markup--p-code">squared_error</code> , which is the MSE we have discussed.</p><p name="9957" id="9957" class="graf graf--p graf-after--p">For the classifier, the splitting method is also defined by <code class="markup--code markup--p-code">criterion</code> and we can choose between,</p><pre name="4641" id="4641" class="graf graf--pre graf-after--p">criterion = [“gini”, “entropy”]</pre><p name="cde9" id="cde9" class="graf graf--p graf-after--pre">The default one is the Gini Impurity and we can change it to Entropy.</p><p name="3201" id="3201" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Hyperparameters</strong></p><p name="3421" id="3421" class="graf graf--p graf-after--p">As we have discussed, the decision model always overfits, and we have to develop some methods for increasing the generality.</p><ul class="postList"><li name="7a99" id="7a99" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">max_depth</code> : the depth of the tree. Decrease to avoid overfitting, and increase to increase accuracy</li><li name="b31b" id="b31b" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">min_samples_leaf</code> : the minimum sample size per leaf. Increase to avoid overfitting, and decrease to increase accuracy</li><li name="15af" id="15af" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">max_features</code> : the number of features to consider when looking for the best split. Decrease to avoid overfitting, and increase to increase accuracy</li><li name="2cdc" id="2cdc" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">min_samples_split</code> : the minimum sample size we need for a split. Increase to avoid overfitting, and decrease to increase accuracy</li></ul><p name="b48c" id="b48c" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Plot the Decision Tree</strong></p><pre name="d13e" id="d13e" class="graf graf--pre graf-after--p">import matplotlib.pyplot as plt<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn import tree</pre><pre name="b785" id="b785" class="graf graf--pre graf-after--pre"># Generate X and y<br>X_train = []<br>X = sorted([random.uniform(-3, 3) for i in range(50)])<br>for i in X:<br>    X_train.append([i])</pre><pre name="e163" id="e163" class="graf graf--pre graf-after--pre">y = []<br># Partition rules<br>for value in X:<br>    if value &lt; -1.3:<br>        y.append(0)<br>    elif (value &gt;= -1.3 and value &lt; -0.3):<br>        y.append(random.randint(0,1))<br>    elif (value &gt;= -0.3 and value &lt; 0.3):<br>        y.append(1)<br>    elif (value &gt;= 0.3 and value &lt; 1.3):<br>        y.append(random.randint(1,2))<br>    else:<br>        y.append(2)</pre><pre name="ebc3" id="ebc3" class="graf graf--pre graf-after--pre">clf = DecisionTreeClassifier(min_samples_leaf=10)<br>clf.fit(X_train, y)</pre><pre name="e2e1" id="e2e1" class="graf graf--pre graf-after--pre">fig, ax = plt.subplots(1,1,figsize=(8,8))<br>tree.plot_tree(clf, ax=ax)<br>plt.show()</pre><p name="79b2" id="79b2" class="graf graf--p graf-after--pre">The output would be (this result can be different because we generate our training set randomly),</p><figure name="0df5" id="0df5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KKLv7--tc7EtBrtw5WhLeA.png" data-width="1506" data-height="460" src="https://cdn-images-1.medium.com/max/800/1*KKLv7--tc7EtBrtw5WhLeA.png"></figure><p name="fd0b" id="fd0b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Plot the Decision Boundary</strong></p><pre name="f45a" id="f45a" class="graf graf--pre graf-after--p">colors = [&quot;b&quot;, &quot;r&quot;, &quot;g&quot;]</pre><pre name="63b0" id="63b0" class="graf graf--pre graf-after--pre">fig, ax = plt.subplots(1,1,figsize=(10,5))<br>for i, value in enumerate(X):<br>    ax.scatter(value, 0, c=colors[y[i]], alpha=0.5, lw=0)<br>for i in clf.tree_.threshold:<br>    if i != -2:<br>        ax.axvline(i, lw=0.4)<br>plt.tight_layout()<br>plt.show()</pre><figure name="6926" id="6926" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*nZrUiF-TCQPOL5SodjxVvA.png" data-width="1518" data-height="432" src="https://cdn-images-1.medium.com/max/800/1*nZrUiF-TCQPOL5SodjxVvA.png"></figure><p name="c6af" id="c6af" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Plot the Decision in a Better Way by </strong><code class="markup--code markup--p-code"><strong class="markup--strong markup--p-strong">dtreeviz</strong></code></p><p name="2b1b" id="2b1b" class="graf graf--p graf-after--p">By using the <a href="https://github.com/parrt/dtreeviz" data-href="https://github.com/parrt/dtreeviz" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">dtreeviz</a> Tool, we can draw some better graphs about decision trees. For example,</p><pre name="a2b5" id="a2b5" class="graf graf--pre graf-after--p">from dtreeviz.trees import *</pre><pre name="e3e0" id="e3e0" class="graf graf--pre graf-after--pre">viz = dtreeviz(clf,<br>               np.array(X_train), np.array(y),<br>               feature_names = &quot;Random values&quot;,<br>               target_name=&#39;Partition Rule&#39;) <br>viz</pre><figure name="0356" id="0356" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*_yFyEDu5XmCMzRs0zfSdPg.png" data-width="1256" data-height="528" src="https://cdn-images-1.medium.com/max/800/1*_yFyEDu5XmCMzRs0zfSdPg.png"></figure><pre name="c610" id="c610" class="graf graf--pre graf-after--figure">fig, ax = plt.subplots(1, 1, figsize=(10,5))<br>viz = ctreeviz_univar(clf,<br>                      np.array(X_train), np.array(y),<br>                      feature_names = &quot;Random values&quot;,<br>                      target_name=&#39;Partition Rule&#39;,<br>                      show={&#39;splits&#39;},<br>                      ax=ax)<br>plt.show()</pre><figure name="5eb7" id="5eb7" class="graf graf--figure graf-after--pre graf--trailing"><img class="graf-image" data-image-id="1*3ve212vOQlVK98MGnvwgpw.png" data-width="1256" data-height="586" src="https://cdn-images-1.medium.com/max/800/1*3ve212vOQlVK98MGnvwgpw.png"></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/ff69c26cf312"><time class="dt-published" datetime="2021-11-11T17:14:07.493Z">November 11, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/intro-to-machine-learning-4-non-parametric-models-and-decision-tree-ff69c26cf312" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>