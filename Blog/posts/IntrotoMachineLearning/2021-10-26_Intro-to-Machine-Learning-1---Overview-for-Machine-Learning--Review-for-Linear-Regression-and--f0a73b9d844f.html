<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Intro to Machine Learning 1 | Overview for Machine Learning, Review for Linear Regression and…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Intro to Machine Learning 1 | Overview for Machine Learning, Review for Linear Regression and…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Intro to Machine Learning
</section>
<section data-field="body" class="e-content">
<section name="1c8e" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="87b7" id="87b7" class="graf graf--h3 graf--leading graf--title">Intro to Machine Learning 1 | Overview for Machine Learning, Review for Linear Regression and Logistic Regression</h3><figure name="42c2" id="42c2" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*qfGTOHjY1BHysHZT_HCNuA.png" data-width="1250" data-height="700" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*qfGTOHjY1BHysHZT_HCNuA.png"><figcaption class="imageCaption">illusion by <a href="https://dribbble.com/patraubo" data-href="https://dribbble.com/patraubo" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Pat Raubo</a></figcaption></figure><ol class="postList"><li name="b672" id="b672" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Overview</strong></li></ol><p name="9e5c" id="9e5c" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Regressor Vs. Classifier</strong></p><p name="577a" id="577a" class="graf graf--p graf-after--p">They are actually two sides of the same coin. If the target is numerical, the model is a <strong class="markup--strong markup--p-strong">regressor.</strong> If the target is categorical, the model is a <strong class="markup--strong markup--p-strong">classifier</strong>. Regressors draw through data, classifiers draw between clusters.</p><p name="2ebe" id="2ebe" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Parameters and Hyperparameters</strong></p><ul class="postList"><li name="539d" id="539d" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Parameters</strong>: In machine learning, the parameters are part of the model, and they are actually the information we learned from the historical data.</li><li name="bfd2" id="bfd2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Hyperparameters</strong>: In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. For example, the height of a decision tree, neighborhood number <em class="markup--em markup--li-em">k</em> in the KNN, the maximum number of trees in a random forest, minimum number of leaves in the tree, or learning rate.</li></ul><p name="931d" id="931d" class="graf graf--p graf-after--li">In short, the model in machine learning is just an algorithm plus its parameters.</p><p name="5015" id="5015" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Features and Feature Engineering</strong></p><p name="c2b3" id="c2b3" class="graf graf--p graf-after--p">Good features are usually more important than the model. Suppose we have an audio markup project to distinguish between the audio spoken by a person about the word “Zero” and “No”. The word “Zero” has two syllables and “No” has only one syllable. So even if we have no real machine learning models, we can classify the audio of these two words based on the length.</p><p name="8bed" id="8bed" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Feature engineering</strong> focuses on synthesizing new features from existing features. And we have to remove the missing values as well. An example of this is the missing market sales data on holidays and Sundays.</p><p name="fc9e" id="fc9e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Metrics for Model Performance</strong></p><ul class="postList"><li name="0e20" id="0e20" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Training Data Set</strong></li><li name="d058" id="d058" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Testing Data Set</strong></li></ul><p name="334c" id="334c" class="graf graf--p graf-after--li">Suppose we have a midterm exam and if the professor gives the exact same questions before the exam, everyone can do pretty well. However, if the students are tested by some sort of new questions with the same structure, the students may not be able to have that good performance. That’s the reason why we typically split the training set and the testing set.</p><p name="1b8d" id="1b8d" class="graf graf--p graf-after--p">In python, we commonly do testing-training split by <code class="markup--code markup--p-code">sklearn</code>,</p><pre name="559c" id="559c" class="graf graf--pre graf-after--p">from sklearn.model_selection import train_test_split</pre><pre name="17ed" id="17ed" class="graf graf--pre graf-after--pre">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</pre><ul class="postList"><li name="6ce6" id="6ce6" class="graf graf--li graf-after--pre"><strong class="markup--strong markup--li-strong">Train, validate, test</strong>: these are some ways we can measure the performance of our project.</li></ul><p name="8ff3" id="8ff3" class="graf graf--p graf-after--li">Let’s imagine an example of an unlucky prediction on the vehicles. The test set happens to be all the trucks, but the training set includes all the cars. Then probably we are going to conclude that we can not have a very good performance</p><p name="fbbd" id="fbbd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Simplified ML Process</strong></p><ul class="postList"><li name="98aa" id="98aa" class="graf graf--li graf-after--p">Understand the problem</li><li name="cbc7" id="cbc7" class="graf graf--li graf-after--li">Acquire the data</li><li name="f0e1" id="f0e1" class="graf graf--li graf-after--li">Split into train, validation, and test sets</li><li name="350b" id="350b" class="graf graf--li graf-after--li">Sniff data, clean, deal with missing data</li><li name="47fe" id="47fe" class="graf graf--li graf-after--li">Convert non-numeric features to numeric</li><li name="0c01" id="0c01" class="graf graf--li graf-after--li">Repeat until satisfied: (1) Train a model using the training set and specific hyperparameters; (2) Tune model and do feature engineering with the validation set</li><li name="b70f" id="b70f" class="graf graf--li graf-after--li">Assess model performance on a test set</li></ul><p name="dd9a" id="dd9a" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Review of Linear Regression</strong></p><p name="0d51" id="0d51" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Reasons for Linear Models</strong></p><ul class="postList"><li name="610b" id="610b" class="graf graf--li graf-after--p">Simple, interpretable, super-fast, can’t be beaten for linear relationships</li><li name="f6ab" id="f6ab" class="graf graf--li graf-after--li">Form the basis of neural networks</li><li name="3d8b" id="3d8b" class="graf graf--li graf-after--li">etc.</li></ul><p name="8043" id="8043" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Linear Regression Notations</strong></p><ul class="postList"><li name="75d6" id="75d6" class="graf graf--li graf-after--p">Number Notations</li></ul><figure name="4a17" id="4a17" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*BCnvJzv-8ddhVGBuRlwHZA.png" data-width="1868" data-height="174" src="https://cdn-images-1.medium.com/max/800/1*BCnvJzv-8ddhVGBuRlwHZA.png"></figure><ul class="postList"><li name="4ffb" id="4ffb" class="graf graf--li graf-after--figure">Vectors Notations</li></ul><figure name="6eb3" id="6eb3" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*TnKOCudopvvzwHTuL5S7vA.png" data-width="1684" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*TnKOCudopvvzwHTuL5S7vA.png"></figure><ul class="postList"><li name="b64a" id="b64a" class="graf graf--li graf-after--figure">Argument with 1 trick</li></ul><pre name="74ff" id="74ff" class="graf graf--pre graf-after--li">n = X.shape[0]<br>B0 = np.ones(shape=(n, 1))<br>X = np.hstack([B0, X])</pre><ul class="postList"><li name="4d5e" id="4d5e" class="graf graf--li graf-after--pre">Mean-squared Error</li></ul><figure name="f9e1" id="f9e1" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*UZgfCzW1CCf8IFBmzgQslg.png" data-width="2434" data-height="170" src="https://cdn-images-1.medium.com/max/800/1*UZgfCzW1CCf8IFBmzgQslg.png"></figure><ul class="postList"><li name="ac90" id="ac90" class="graf graf--li graf-after--figure">Loss function: loss function calculate the cost we have after modeling the data. This means that we would like to minimize the loss function in an optimization problem. For linear regression, the loss function equals its mean squared error (MSE).</li></ul><figure name="104c" id="104c" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*aysv0z05vTQRBa5h3SEeTQ.png" data-width="1504" data-height="72" src="https://cdn-images-1.medium.com/max/800/1*aysv0z05vTQRBa5h3SEeTQ.png"></figure><p name="7962" id="7962" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Optimizing Linear Models</strong></p><p name="57a7" id="57a7" class="graf graf--p graf-after--p">To minimize the value of the loss function, a popular approach is to minimize the gradient of the loss function. The gradient of the linear regression model is,</p><figure name="22fc" id="22fc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*j95RpzrbX6YHDywehYmeTQ.png" data-width="966" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*j95RpzrbX6YHDywehYmeTQ.png"></figure><p name="435c" id="435c" class="graf graf--p graf-after--figure">To minimize this gradient, we have to use an iteration approach called gradient descent. Each of the iteration steps should be regulated by the following recurrence relation,</p><figure name="1d82" id="1d82" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1Qjg-fmQfwboBsBuQBLIfQ.png" data-width="966" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*1Qjg-fmQfwboBsBuQBLIfQ.png"></figure><p name="9b05" id="9b05" class="graf graf--p graf-after--figure">where η is a scalar called <strong class="markup--strong markup--p-strong">learning rate</strong> that controls the magnitude of the step we take at each step <em class="markup--em markup--p-em">t</em>. We’re done optimizing when we find a β where all slopes are flat (or zero).</p><figure name="78d0" id="78d0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*QhfbRICQ8m1AcbKm5ouQag.png" data-width="966" data-height="42" src="https://cdn-images-1.medium.com/max/800/1*QhfbRICQ8m1AcbKm5ouQag.png"></figure><p name="2395" id="2395" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Solving Linear Models by Python</strong></p><pre name="71e3" id="71e3" class="graf graf--pre graf-after--p">lm = LinearRegression()<br>lm.fit(X_train, y_train)</pre><pre name="6e60" id="6e60" class="graf graf--pre graf-after--pre"># compute the R-square value on the testing set<br>print(lm.score(X_test, y_test))</pre><pre name="8a1f" id="8a1f" class="graf graf--pre graf-after--pre"># predict the values<br>print(lm.predict(X_new))</pre><p name="1b37" id="1b37" class="graf graf--p graf-after--pre">The score R-square for the linear regression will fluctuate if we have a relatively small data set. We can fix this problem by adding more data.</p><p name="205a" id="205a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Review of Logistic Regression</strong></p><p name="7d71" id="7d71" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) 1-Dimension Logistic Regression</strong></p><p name="e98c" id="e98c" class="graf graf--p graf-after--p">Linear regression doesn’t work for discrete Y values and we will have really bad performance. One idea is to cut it off by some threshold, but this causes some discontinuity in the result. Finally, in practice, what we do is to use a <code class="markup--code markup--p-code">Sigmoid(z)</code> function so that we have a much better transition from class 0 to class 1.</p><pre name="9171" id="9171" class="graf graf--pre graf-after--p">def Sigmoid(z):<br>    return 1 / (1 + exp(-z))</pre><p name="a118" id="a118" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(2) Logistic Regression Notations</strong></p><ul class="postList"><li name="e637" id="e637" class="graf graf--li graf-after--p">Sigmond Function</li></ul><figure name="12c7" id="12c7" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*FjlHZavsRSIqiFHXpTVzfw.png" data-width="718" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*FjlHZavsRSIqiFHXpTVzfw.png"></figure><ul class="postList"><li name="0338" id="0338" class="graf graf--li graf-after--figure">Linear equation with Sigmond Function</li></ul><figure name="f821" id="f821" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*hqueP3f002hmXZWOSwHOVw.png" data-width="718" data-height="63" src="https://cdn-images-1.medium.com/max/800/1*hqueP3f002hmXZWOSwHOVw.png"></figure><ul class="postList"><li name="5a92" id="5a92" class="graf graf--li graf-after--figure">Log odds: this is something that can be proved based on the definition of the odds</li></ul><figure name="e751" id="e751" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*-0iycIro4bAj-rI0RjBxHw.png" data-width="762" data-height="40" src="https://cdn-images-1.medium.com/max/800/1*-0iycIro4bAj-rI0RjBxHw.png"></figure><ul class="postList"><li name="4b0c" id="4b0c" class="graf graf--li graf-after--figure">Log loss function: we would like to maximize the likelihood for logistic regression, and this means that we can also minimize the negative likelihood of it. So the loss function should be the negative log-likelihood.</li></ul><figure name="8f8a" id="8f8a" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*omznoI5mzSzgAIKUerO-qw.png" data-width="981" data-height="75" src="https://cdn-images-1.medium.com/max/800/1*omznoI5mzSzgAIKUerO-qw.png"></figure><p name="0153" id="0153" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Optimizing Logistic Model Parameters</strong></p><p name="e427" id="e427" class="graf graf--p graf-after--p">The gradient of the logistic model loss function is,</p><figure name="ebf5" id="ebf5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3M9dG17OI_rC0HiN183_1g.png" data-width="871" data-height="41" src="https://cdn-images-1.medium.com/max/800/1*3M9dG17OI_rC0HiN183_1g.png"></figure><p name="78eb" id="78eb" class="graf graf--p graf-after--figure">Based on our discussion, we can perform the gradient descend iteration</p><figure name="02c7" id="02c7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1Qjg-fmQfwboBsBuQBLIfQ.png" data-width="966" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*1Qjg-fmQfwboBsBuQBLIfQ.png"></figure><p name="08bc" id="08bc" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Solving Logistic Models by Python</strong></p><pre name="5b81" id="5b81" class="graf graf--pre graf-after--p">lg = LogisticRegression(solver = &#39;lbfgs&#39;, max_iter=1000)<br>lg.fit(X_train, y_train)      # regularization by default</pre><pre name="b6ed" id="b6ed" class="graf graf--pre graf-after--pre"># get the score of the model <br>lg.score(X_test, y_test)</pre><pre name="dfc9" id="dfc9" class="graf graf--pre graf-after--pre"># predict the probability for each category<br>lg.predict_proba(X_test, y_test)</pre><pre name="f633" id="f633" class="graf graf--pre graf-after--pre"># predict the result based on the highest probability<br>lg.predict(X_test, y_test)</pre><p name="96e2" id="96e2" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">4. Machine Learn Lab 1</strong></p><p name="36ae" id="36ae" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Basic Machine learning pipeline</strong></p><ul class="postList"><li name="9c20" id="9c20" class="graf graf--li graf-after--p">Get the data (SQL, Spark)</li><li name="0284" id="0284" class="graf graf--li graf-after--li">Data Wrangling (standardization, imputing not available values)</li><li name="76f7" id="76f7" class="graf graf--li graf-after--li">Feature analysis and engineering</li><li name="0699" id="0699" class="graf graf--li graf-after--li">Model building, fitting, and selection</li><li name="567f" id="567f" class="graf graf--li graf-after--li">Model Evaluation</li><li name="c13c" id="c13c" class="graf graf--li graf-after--li">Productionize a model</li></ul><p name="614d" id="614d" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Dividing the Data Set</strong></p><p name="352d" id="352d" class="graf graf--p graf-after--p">We are interested in <strong class="markup--strong markup--p-strong">generalization error</strong> not only the metrics we have for the model in case the overfitting and the underfitting. So we can basically test our models based on the validation set and the test set.</p><ul class="postList"><li name="7cb2" id="7cb2" class="graf graf--li graf-after--p">Train set: part of the dataset used for training and to fit the model</li><li name="779b" id="779b" class="graf graf--li graf-after--li">Validation set: part of the dataset used to select a specific model, we always use this to find the proper hyperparameter. The validation set is an independent dataset used to select models (complexity, tuning hyperparameters).</li><li name="7ee1" id="7ee1" class="graf graf--li graf-after--li">Test set: part of the dataset used to estimate error</li></ul><p name="9f72" id="9f72" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Scaling Data</strong></p><p name="96ca" id="96ca" class="graf graf--p graf-after--p">Sometimes we have multiple features and one or several of them may have a heavy tail. In this case, we may think about using a data scaler.</p><figure name="7260" id="7260" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_JpW1bswTRHFghoCcJZO1Q.png" data-width="1900" data-height="372" src="https://cdn-images-1.medium.com/max/800/1*_JpW1bswTRHFghoCcJZO1Q.png"></figure><p name="58e6" id="58e6" class="graf graf--p graf-after--figure">Basically, there are two kinds of scalers,</p><ul class="postList"><li name="e014" id="e014" class="graf graf--li graf-after--p">Standard Scaler: it removes the mean and scaled to unit variance</li><li name="7405" id="7405" class="graf graf--li graf-after--li">Min-Max Scaler: transforms each value to a given range. For example [0, 1]</li></ul><p name="688d" id="688d" class="graf graf--p graf-after--li">Note that we have to deal with the outliers before we apply any scalers.</p><p name="deda" id="deda" class="graf graf--p graf-after--p">We have to think about scaling the data especially,</p><ul class="postList"><li name="738b" id="738b" class="graf graf--li graf-after--p">When using linear models with regularization: Most are done otherwise regularization is <strong class="markup--strong markup--li-strong">unfair</strong></li><li name="210c" id="210c" class="graf graf--li graf-after--li">When using linear models without regularization: It is advisable because models<strong class="markup--strong markup--li-strong"> converge faster</strong></li><li name="250d" id="250d" class="graf graf--li graf-after--li">When using <strong class="markup--strong markup--li-strong">neural networks</strong></li><li name="c265" id="c265" class="graf graf--li graf-after--li">When using nearest neighbor type algorithms (or anything with <strong class="markup--strong markup--li-strong">distances</strong>)</li></ul><p name="1319" id="1319" class="graf graf--p graf-after--li">There are also some cases when the scaling is not necessary,</p><ul class="postList"><li name="5052" id="5052" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Tree-based algorithms</strong>: for example, random forest and gradient boosting trees. These algorithms are also good at dealing with outliers.</li></ul><p name="bcb3" id="bcb3" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Example: Regularized Regression</strong></p><figure name="d3cc" id="d3cc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*orX8Om2E86kQzlI5mqPZaw.png" data-width="1676" data-height="206" src="https://cdn-images-1.medium.com/max/800/1*orX8Om2E86kQzlI5mqPZaw.png"></figure><p name="2065" id="2065" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) The Definition of Model Fitting</strong></p><p name="2b41" id="2b41" class="graf graf--p graf-after--p">Fit the model means to minimize some formula result (e.g. the value of the loss function) or some parameters.</p><p name="bac6" id="bac6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Scikit-learn Package</strong></p><p name="728c" id="728c" class="graf graf--p graf-after--p">Scikit-learn was initially developed by David Cournapeau as a Google summer of code project in 2007. The project now has more than 30 active contributors and has had paid sponsorship from INRIA, Google, Tinyclues, and the Python Software Foundation.</p><figure name="b8f9" id="b8f9" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="0*55ImWP3kqhRyrkv9.png" data-width="2000" data-height="1246" src="https://cdn-images-1.medium.com/max/800/0*55ImWP3kqhRyrkv9.png"><figcaption class="imageCaption">Source: <a href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" data-href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener" target="_blank">https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html</a></figcaption></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/f0a73b9d844f"><time class="dt-published" datetime="2021-10-26T05:09:31.429Z">October 26, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/intro-to-machine-learning-1-overview-for-machine-learning-review-for-linear-regression-and-f0a73b9d844f" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>