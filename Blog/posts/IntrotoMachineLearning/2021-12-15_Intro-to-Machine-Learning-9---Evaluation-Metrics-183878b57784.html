<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Intro to Machine Learning 9 | Evaluation Metrics</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Intro to Machine Learning 9 | Evaluation Metrics</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Intro to Machine Learning
</section>
<section data-field="body" class="e-content">
<section name="696e" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="028a" id="028a" class="graf graf--h3 graf--leading graf--title">Intro to Machine Learning 9 | Evaluation Metrics</h3><figure name="80b5" id="80b5" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*JlZ1NeEROFreHppa.png" data-width="1250" data-height="700" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*JlZ1NeEROFreHppa.png"></figure><ol class="postList"><li name="e1b3" id="e1b3" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Regression Metrics</strong></li></ol><p name="6ab9" id="6ab9" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Mean Square Error (MSE)</strong></p><figure name="2eee" id="2eee" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NdP187-OjW7s4sv9wACsRw.png" data-width="1344" data-height="118" src="https://cdn-images-1.medium.com/max/800/1*NdP187-OjW7s4sv9wACsRw.png"></figure><p name="7ab8" id="7ab8" class="graf graf--p graf-after--figure">The best constant function that would minimize this error is,</p><figure name="7837" id="7837" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*x4JGee1mEVaC8Dn4XcdHvg.png" data-width="1344" data-height="118" src="https://cdn-images-1.medium.com/max/800/1*x4JGee1mEVaC8Dn4XcdHvg.png"></figure><p name="aca7" id="aca7" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Root Mean Square Error (RMSE)</strong></p><figure name="86f7" id="86f7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zE-M-SSseap3aQAfg-KVRg.png" data-width="1344" data-height="142" src="https://cdn-images-1.medium.com/max/800/1*zE-M-SSseap3aQAfg-KVRg.png"></figure><p name="073f" id="073f" class="graf graf--p graf-after--figure">The best constant function that would minimize this error is,</p><figure name="4d0e" id="4d0e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*x4JGee1mEVaC8Dn4XcdHvg.png" data-width="1344" data-height="118" src="https://cdn-images-1.medium.com/max/800/1*x4JGee1mEVaC8Dn4XcdHvg.png"></figure><p name="ac45" id="ac45" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) R-Squared (RÂ²)</strong></p><figure name="3105" id="3105" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6nblbLQ_pMAPsN6ID0OTdQ.png" data-width="1344" data-height="126" src="https://cdn-images-1.medium.com/max/800/1*6nblbLQ_pMAPsN6ID0OTdQ.png"></figure><p name="7611" id="7611" class="graf graf--p graf-after--figure">The best constant function that would optimize this value is,</p><figure name="d985" id="d985" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EfhQEpQi_oTTDF7SqcI2Nw.png" data-width="1344" data-height="118" src="https://cdn-images-1.medium.com/max/800/1*EfhQEpQi_oTTDF7SqcI2Nw.png"></figure><p name="b385" id="b385" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Mean Absolute Error (MAE)</strong></p><figure name="5ef2" id="5ef2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VKIPHi76NnjQDf2Lc926vw.png" data-width="1344" data-height="118" src="https://cdn-images-1.medium.com/max/800/1*VKIPHi76NnjQDf2Lc926vw.png"></figure><p name="4aaa" id="4aaa" class="graf graf--p graf-after--figure">The best constant function that would minimize this error is,</p><figure name="8477" id="8477" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Yra7MnZkIZjBXTk920wC4A.png" data-width="1344" data-height="82" src="https://cdn-images-1.medium.com/max/800/1*Yra7MnZkIZjBXTk920wC4A.png"></figure><p name="fd15" id="fd15" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Mean Absolute Error Vs. Root Mean Square Error</strong></p><ul class="postList"><li name="f591" id="f591" class="graf graf--li graf-after--p">RMSE and MSE give a relatively high weight to large errors</li><li name="25dc" id="25dc" class="graf graf--li graf-after--li">MAE is used as a Metric and Loss function</li><li name="d355" id="d355" class="graf graf--li graf-after--li">Both MAE and RMSE measure average model prediction error in units of the variable of interest</li><li name="8c63" id="8c63" class="graf graf--li graf-after--li">Use MAE when we have outliers</li></ul><p name="abb6" id="abb6" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Classification Metrics</strong></p><p name="48a1" id="48a1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Log Loss (Entropy)</strong></p><p name="daa8" id="daa8" class="graf graf--p graf-after--p">For multi-class classification,</p><figure name="1448" id="1448" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*exVu-xiCtimiWCs9TrBCVw.png" data-width="1344" data-height="130" src="https://cdn-images-1.medium.com/max/800/1*exVu-xiCtimiWCs9TrBCVw.png"></figure><p name="c9bb" id="c9bb" class="graf graf--p graf-after--figure">Especially when the number of classes = 2,</p><figure name="8777" id="8777" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*oSEOqBugrCYrsJsIHGIk7Q.png" data-width="1344" data-height="130" src="https://cdn-images-1.medium.com/max/800/1*oSEOqBugrCYrsJsIHGIk7Q.png"></figure><p name="163e" id="163e" class="graf graf--p graf-after--figure">For multi-label classification,</p><figure name="9f80" id="9f80" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1AuAjYq-IQvvwUNUYurRrg.png" data-width="1344" data-height="130" src="https://cdn-images-1.medium.com/max/800/1*1AuAjYq-IQvvwUNUYurRrg.png"></figure><p name="930a" id="930a" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Confusion Matrix</strong></p><figure name="1c82" id="1c82" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yLChBKyP-20MmIxxbolH-A.png" data-width="1396" data-height="510" src="https://cdn-images-1.medium.com/max/800/1*yLChBKyP-20MmIxxbolH-A.png"></figure><ul class="postList"><li name="44a2" id="44a2" class="graf graf--li graf-after--figure">True positive: correctly predict the positive class</li><li name="c3ac" id="c3ac" class="graf graf--li graf-after--li">True negative: correctly predict the negative class</li><li name="a485" id="a485" class="graf graf--li graf-after--li">False positive: wrongly predict the positive class, also called type I error</li><li name="3532" id="3532" class="graf graf--li graf-after--li">False negative: wrongly predict the negative class, also called type II error</li></ul><p name="963d" id="963d" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Confusion Matrix Metrics</strong></p><ul class="postList"><li name="24b9" id="24b9" class="graf graf--li graf-after--p">Accuracy</li></ul><pre name="593f" id="593f" class="graf graf--pre graf-after--li">Accuracy = TP + TN / (TP + TN + FP + FN)</pre><ul class="postList"><li name="3a53" id="3a53" class="graf graf--li graf-after--pre">Recall / Sensitivity / True positive rate</li></ul><pre name="4207" id="4207" class="graf graf--pre graf-after--li">Sensitivity = TP / (TP + FN)</pre><ul class="postList"><li name="6723" id="6723" class="graf graf--li graf-after--pre">Precision</li></ul><pre name="86c4" id="86c4" class="graf graf--pre graf-after--li">Precision = TP / (TP + FP)</pre><ul class="postList"><li name="e964" id="e964" class="graf graf--li graf-after--pre">False positive rate</li></ul><pre name="469b" id="469b" class="graf graf--pre graf-after--li">False positive rate = FP / (FP + TN)</pre><ul class="postList"><li name="3da1" id="3da1" class="graf graf--li graf-after--pre">True negative rate</li></ul><pre name="6310" id="6310" class="graf graf--pre graf-after--li">True negative rate = TN / (FP + TN)</pre><ul class="postList"><li name="a3df" id="a3df" class="graf graf--li graf-after--pre">F1 score</li></ul><p name="0358" id="0358" class="graf graf--p graf-after--li">The formula for the standard F1-score is the harmonic mean of the precision and recall.</p><figure name="8c88" id="8c88" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yiciinXjnuG09i9KosWiPA.png" data-width="1340" data-height="118" src="https://cdn-images-1.medium.com/max/800/1*yiciinXjnuG09i9KosWiPA.png"></figure><p name="dd44" id="dd44" class="graf graf--p graf-after--figure">Or,</p><figure name="7056" id="7056" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rf4IEBx9H-sVa3o1lbz6PA.png" data-width="1340" data-height="118" src="https://cdn-images-1.medium.com/max/800/1*rf4IEBx9H-sVa3o1lbz6PA.png"></figure><ul class="postList"><li name="0c99" id="0c99" class="graf graf--li graf-after--figure">FÎ² score: a more general case of F1 score</li></ul><figure name="929c" id="929c" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*ZM3cDFmq5jcJNvdKV1_WgQ.png" data-width="1340" data-height="118" src="https://cdn-images-1.medium.com/max/800/1*ZM3cDFmq5jcJNvdKV1_WgQ.png"></figure><p name="2e82" id="2e82" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Area Under Curve (AUC)</strong></p><p name="fc5d" id="fc5d" class="graf graf--p graf-after--p">There are two ways of interpreting AUC. One is the area under the ROC curve or PR curve, which can also be noted as <strong class="markup--strong markup--p-strong">ROC-AUC</strong> or <strong class="markup--strong markup--p-strong">PR-AUC</strong>. Another definition is the ordering of the pairs, which is the probability the model will score a randomly chosen positive class higher than a randomly chosen negative class. For this definition, the expression should be,</p><pre name="3990" id="3990" class="graf graf--pre graf-after--p">AUC = # of ordered pairs / total # of pairs</pre><p name="82b7" id="82b7" class="graf graf--p graf-after--pre">The pair here is defined as positive-negative label pairs and there are in total,</p><pre name="fce8" id="fce8" class="graf graf--pre graf-after--p">Total # of pairs = # of pos label * # of neg label</pre><p name="dceb" id="dceb" class="graf graf--p graf-after--pre">A wrongly ordered pair is defined when we have the true labels ordered by the soft predictions and then count how many 1s we have in front of 0s.</p><pre name="761d" id="761d" class="graf graf--pre graf-after--p">0 0 0 1 0 1 1 1 -&gt;  1 wrongly ordered pair<br>0 0 1 0 0 1 1 1 -&gt;  2 wrongly ordered pair<br>0 0 1 0 1 0 1 1 -&gt;  3 wrongly ordered pair<br>1 0 1 0 1 0 1 0 -&gt; 10 wrongly ordered pair<br>1 1 1 1 0 0 0 0 -&gt; 16 wrongly ordered pair</pre><p name="60f2" id="60f2" class="graf graf--p graf-after--pre">AUC can also be calculated as,</p><pre name="f7f5" id="f7f5" class="graf graf--pre graf-after--p">AUC = 1 - # of wrongly ordered pairs / total # of pairs</pre><p name="ad09" id="ad09" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(5) Receiver Operating Characteristic (ROC)</strong></p><p name="a42f" id="a42f" class="graf graf--p graf-after--p">To manually compute a ROC curve, we have to set up some thresholds and use them for hard predictions. For each threshold, we can then calculate the false positive rate and the true positive rate (recall). Finally, we should put them together on a plot and link them together. That is the ROC line we are going to have.</p><figure name="4742" id="4742" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*vCkD2jgpncr83Gcn.gif" data-width="700" data-height="700" src="https://cdn-images-1.medium.com/max/800/0*vCkD2jgpncr83Gcn.gif"><figcaption class="imageCaption">Source: <a href="https://stackoverflow.com/questions/50848163/manually-calculate-auc" data-href="https://stackoverflow.com/questions/50848163/manually-calculate-auc" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://stackoverflow.com/questions/50848163/manually-calculate-auc</a></figcaption></figure><p name="d427" id="d427" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(6) Precision-Recall AUC (PR-AUC)</strong></p><p name="10bc" id="10bc" class="graf graf--p graf-after--p">Similar to the ROC-AUC curve, if we have precision vs recall instead of recall vs false positive rate, we can generate a PR curve and the area under this curve is the PR-AUC matric.</p><figure name="3afc" id="3afc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3qxuph_gMW2nc1I-jHk_hQ.png" data-width="1264" data-height="352" src="https://cdn-images-1.medium.com/max/800/1*3qxuph_gMW2nc1I-jHk_hQ.png"></figure><p name="833d" id="833d" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(7) ROC-AUC (AUROC) vs PR-AUC (AUPRC)</strong></p><ul class="postList"><li name="5493" id="5493" class="graf graf--li graf-after--p">PR-AUC is more sensitive to the improvements for the positive class, so choose this when you care more about the positive values. Especially for the case when we have imbalanced data.</li><li name="9a8d" id="9a8d" class="graf graf--li graf-after--li graf--trailing">Instead, we suggest using ROC-AUC because it cares equally about the positive and negative classes.</li></ul></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/183878b57784"><time class="dt-published" datetime="2021-12-15T18:08:45.215Z">December 15, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/intro-to-machine-learning-9-evaluation-metrics-183878b57784" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>