<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Probability and Statistics 5 | Joint Probability, Independence of Random Variables, and Condition…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Probability and Statistics 5 | Joint Probability, Independence of Random Variables, and Condition…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Probability and Statistics
</section>
<section data-field="body" class="e-content">
<section name="0bfe" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="c194" id="c194" class="graf graf--h3 graf--leading graf--title">Probability and Statistics 5 | <strong class="markup--strong markup--h3-strong">Joint Probability, Independence of Random Variables, and Condition Probability in Bivariate Context</strong></h3><figure name="eedb" id="eedb" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*YTeSIjiYirbk4CZavQXCqg.jpeg" data-width="1602" data-height="1141" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*YTeSIjiYirbk4CZavQXCqg.jpeg"></figure><ol class="postList"><li name="b39b" id="b39b" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Joint Probability</strong></li></ol><p name="46fc" id="46fc" class="graf graf--p graf-after--li">If X is a random variable with density <em class="markup--em markup--p-em">f</em>x(<em class="markup--em markup--p-em">x</em>) and Y is a random variable with density <em class="markup--em markup--p-em">f</em>Y(<em class="markup--em markup--p-em">y</em>), how would we describe the joint behavior of the tuple (X, Y) at the same time? The answer is joint PDFs (density functions) and joint CDFs.</p><p name="2a31" id="2a31" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of the Joint Probability Density Functions (2 r.v.)</strong></p><p name="71ed" id="71ed" class="graf graf--p graf-after--p">A bivariate PDF is a function <em class="markup--em markup--p-em">f</em>: ℝ² → ℝ satisfying the following two properties:</p><ul class="postList"><li name="9932" id="9932" class="graf graf--li graf-after--p">Non-negativity</li></ul><figure name="1d22" id="1d22" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*4XI2Ab7YuEwfoqT8H0CWeQ.png" data-width="1744" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*4XI2Ab7YuEwfoqT8H0CWeQ.png"></figure><ul class="postList"><li name="4d07" id="4d07" class="graf graf--li graf-after--figure">Unity</li></ul><figure name="c9c9" id="c9c9" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*9edJCLYTN1gsONDbx8SJ-g.png" data-width="1744" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*9edJCLYTN1gsONDbx8SJ-g.png"></figure><p name="340c" id="340c" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) The Definition of the Joint Probability Density Functions (<em class="markup--em markup--p-em">n</em> r.v.)</strong></p><p name="7e5d" id="7e5d" class="graf graf--p graf-after--p">If X1, …, Xn are random variables with densities <em class="markup--em markup--p-em">f</em>X1(<em class="markup--em markup--p-em">x</em>1), <em class="markup--em markup--p-em">f</em>X2(<em class="markup--em markup--p-em">x2</em>), …, <em class="markup--em markup--p-em">f</em>Xn(<em class="markup--em markup--p-em">x</em>n), their joint density is given by some function <em class="markup--em markup--p-em">f</em>: ℝⁿ→ ℝ with,</p><ul class="postList"><li name="1508" id="1508" class="graf graf--li graf-after--p">Non-negativity</li></ul><figure name="7a77" id="7a77" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*maaxPYPyH5JyGB4Daupdew.png" data-width="1744" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*maaxPYPyH5JyGB4Daupdew.png"></figure><ul class="postList"><li name="a139" id="a139" class="graf graf--li graf-after--figure">Unity</li></ul><figure name="bfe9" id="bfe9" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*fm6TIrjXZ1JGaOgJ6eBc1Q.png" data-width="1744" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*fm6TIrjXZ1JGaOgJ6eBc1Q.png"></figure><p name="aa90" id="aa90" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) The Definition of the Bivariate Cumulative Distribution Function (2 r.v.)</strong></p><p name="9a21" id="9a21" class="graf graf--p graf-after--p">Suppose that X ~ <em class="markup--em markup--p-em">f</em>X and Y ~ <em class="markup--em markup--p-em">f</em>Y, the bivariate cumulative distribution function CDF of (X, Y) is defined as:</p><figure name="f915" id="f915" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*f_wmKzhHl_Zk9Qzh8mhIFw.png" data-width="1692" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*f_wmKzhHl_Zk9Qzh8mhIFw.png"></figure><p name="6db4" id="6db4" class="graf graf--p graf-after--figure">Note that the bivariate cumulative distribution function has the following 6 facts:</p><ul class="postList"><li name="9f15" id="9f15" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Fact #1</strong></li></ul><figure name="efb5" id="efb5" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*-z3Xmvq0devq2g6s-mFwqQ.png" data-width="1692" data-height="88" src="https://cdn-images-1.medium.com/max/800/1*-z3Xmvq0devq2g6s-mFwqQ.png"></figure><ul class="postList"><li name="08bf" id="08bf" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Fact #2</strong></li></ul><figure name="2c01" id="2c01" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*YzPPjBw7ExPUfY1hJ-PNsw.png" data-width="1692" data-height="88" src="https://cdn-images-1.medium.com/max/800/1*YzPPjBw7ExPUfY1hJ-PNsw.png"></figure><ul class="postList"><li name="f5e8" id="f5e8" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Fact #3: </strong>The function F must be non-decreasing.</li><li name="c6d3" id="c6d3" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Fact #4:</strong> The function F must be right-continuous.</li><li name="f8e7" id="f8e7" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Fact #5</strong></li></ul><figure name="0dfb" id="0dfb" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*oBF3j99SwCCU6LDiMFODUA.png" data-width="1692" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*oBF3j99SwCCU6LDiMFODUA.png"></figure><ul class="postList"><li name="e4e8" id="e4e8" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Fact #6</strong></li></ul><figure name="c169" id="c169" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*lWfgQqXuNICCjQLHC0cBzQ.png" data-width="1692" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*lWfgQqXuNICCjQLHC0cBzQ.png"></figure><p name="48ff" id="48ff" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) The Definition of the Multivariate Cumulative Distribution Function (<em class="markup--em markup--p-em">n</em> r.v.)</strong></p><p name="a021" id="a021" class="graf graf--p graf-after--p">Suppose that Xi ~ <em class="markup--em markup--p-em">f</em>i = <em class="markup--em markup--p-em">f</em>xi, for i = 1, 2, …, n. The joint CDF of (X1, X2, …, Xn ) is a function F: ℝⁿ→ ℝ, defined as,</p><figure name="a3ea" id="a3ea" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dEzD5X12kfMcg5VA25iYeA.png" data-width="1692" data-height="108" src="https://cdn-images-1.medium.com/max/800/1*dEzD5X12kfMcg5VA25iYeA.png"></figure><p name="a4fa" id="a4fa" class="graf graf--p graf-after--figure">It is important to know that the multivariate CDF inherits properties analogous to the bivariate CDF, so we do not have to state again here but it is crucial to keep these properties in mind. For example,</p><figure name="57ae" id="57ae" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_82QsTOUjVHAF4j72kHPcg.png" data-width="1692" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*_82QsTOUjVHAF4j72kHPcg.png"></figure><p name="8e93" id="8e93" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">2. Independent Random Variables</strong></p><p name="ebaa" id="ebaa" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Recall: Independence of Two Variables</strong></p><p name="8021" id="8021" class="graf graf--p graf-after--p">What does the independence of random variables look like in light of those definitions? Now, let’s recall that A and B are independent events, that then, we have,</p><figure name="e7f9" id="e7f9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*iUx1mUpnaxgn6X39wv8DQA.png" data-width="1692" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*iUx1mUpnaxgn6X39wv8DQA.png"></figure><p name="6b02" id="6b02" class="graf graf--p graf-after--figure">It is not a proper way to assign the event as the capitalized letter X and Y because it is usually unclear about the notation. For example,</p><figure name="15c9" id="15c9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tpvPvNrV5ZUtjM0op1zhSw.png" data-width="1692" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*tpvPvNrV5ZUtjM0op1zhSw.png"></figure><p name="2304" id="2304" class="graf graf--p graf-after--figure">this is not a good way to define independent variables because it would cause some ambiguous here. For the reason that X and Y are usually used to represent the random variables and here what we want to define is to treat X and Y as the event sets. So there has been inconsistency in our notation of X and Y, so it is definitely not a good idea to write things like that!</p><p name="60c1" id="60c1" class="graf graf--p graf-after--p">Suppose we have a demand to link the event set A with the random variable X (the conditions of r.v. X over outcomes is treated as out event set A), we can define the event set A as,</p><figure name="161e" id="161e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*K1-t0kqNufARH40aK0Apsg.png" data-width="1692" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*K1-t0kqNufARH40aK0Apsg.png"></figure><p name="3ba8" id="3ba8" class="graf graf--p graf-after--figure">So now we have a method to link the conditional random variable X with set A and it is not random any more if we write things like,</p><figure name="f8d8" id="f8d8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zk-M0260jkUzR0G0kuXhbA.png" data-width="1692" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*zk-M0260jkUzR0G0kuXhbA.png"></figure><p name="161a" id="161a" class="graf graf--p graf-after--figure">therefore, if we also define that,</p><figure name="5289" id="5289" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5EPmKVQyZQeIM4LUTPiLoA.png" data-width="1692" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*5EPmKVQyZQeIM4LUTPiLoA.png"></figure><p name="e39b" id="e39b" class="graf graf--p graf-after--figure">We can then apply the definition of independence to this specific case as,</p><figure name="db67" id="db67" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ovt16Rui-sS45kZmH1uW5w.png" data-width="1692" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*Ovt16Rui-sS45kZmH1uW5w.png"></figure><p name="28da" id="28da" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Formal Definition of Independent Random Variables</strong></p><p name="9031" id="9031" class="graf graf--p graf-after--p">If, for every interval [<em class="markup--em markup--p-em">a</em>, <em class="markup--em markup--p-em">b</em>]∈ℝ and [<em class="markup--em markup--p-em">c</em>, <em class="markup--em markup--p-em">d</em>]∈ℝ, if we have</p><figure name="1103" id="1103" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*I-SHGt0SaNlrEXLEr4hIRg.png" data-width="1692" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*I-SHGt0SaNlrEXLEr4hIRg.png"></figure><p name="70ad" id="70ad" class="graf graf--p graf-after--figure">or we can also write as,</p><figure name="6620" id="6620" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*74yJn2wDiFerhP5FQMHPCQ.png" data-width="1692" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*74yJn2wDiFerhP5FQMHPCQ.png"></figure><p name="8cd4" id="8cd4" class="graf graf--p graf-after--figure">then, X and Y are <strong class="markup--strong markup--p-strong">independent random variables</strong>. An analogous definition holds in a multivariate context.</p><p name="d4c8" id="d4c8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Factorization Property of CDF: </strong>A Direct Consequence by Independent Random Variables</p><p name="ed39" id="ed39" class="graf graf--p graf-after--p">Choose that [<em class="markup--em markup--p-em">a</em>, <em class="markup--em markup--p-em">b</em>] ∈ (-∞, s] and [c, d] ∈ (-∞, t], if X and Y are independent, then,</p><figure name="2d3a" id="2d3a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pqTrGpfvM6lXSntEQ-yKOw.png" data-width="1692" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*pqTrGpfvM6lXSntEQ-yKOw.png"></figure><p name="9271" id="9271" class="graf graf--p graf-after--figure">which is also,</p><figure name="3e43" id="3e43" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*awV9vvfVT52El-VA2xvK4Q.png" data-width="1692" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*awV9vvfVT52El-VA2xvK4Q.png"></figure><p name="1d57" id="1d57" class="graf graf--p graf-after--figure">This is called the factorization property of CDFs under independence.</p><p name="cde9" id="cde9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Recall: the Relationship between the Density Function and CDF</strong></p><p name="de17" id="de17" class="graf graf--p graf-after--p">If X is a random variable, let its density be fX(<em class="markup--em markup--p-em">x</em>) and Its CDF be FX(<em class="markup--em markup--p-em">t</em>), by definition, If X is continuous, then</p><figure name="f9b8" id="f9b8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*d1hLvTfFR5uKm_G6Pfhf4w.png" data-width="1500" data-height="110" src="https://cdn-images-1.medium.com/max/800/1*d1hLvTfFR5uKm_G6Pfhf4w.png"></figure><p name="c58d" id="c58d" class="graf graf--p graf-after--figure">Based on this fact, we can also derive the conclusion that,</p><figure name="6b98" id="6b98" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4PK8dwTF8Hp6bUI-MsAOPg.png" data-width="1500" data-height="92" src="https://cdn-images-1.medium.com/max/800/1*4PK8dwTF8Hp6bUI-MsAOPg.png"></figure><p name="0aaa" id="0aaa" class="graf graf--p graf-after--figure">Proof:</p><figure name="9c75" id="9c75" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0F0TxCcYhxIzdykXYC-VLw.png" data-width="1500" data-height="108" src="https://cdn-images-1.medium.com/max/800/1*0F0TxCcYhxIzdykXYC-VLw.png"></figure><p name="437f" id="437f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Factorization Property of Densities (PDF)</strong></p><p name="334c" id="334c" class="graf graf--p graf-after--p">So what happens in higher dimensions, say 2 dimensions? Let’s suppose that the random variable X and Y are independent. Based on our discovery of factorization property of CDF, that,</p><figure name="ce2f" id="ce2f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*awV9vvfVT52El-VA2xvK4Q.png" data-width="1692" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*awV9vvfVT52El-VA2xvK4Q.png"></figure><p name="4ad7" id="4ad7" class="graf graf--p graf-after--figure">Then we calculate the partial derivative of F(<em class="markup--em markup--p-em">s</em>, <em class="markup--em markup--p-em">t</em>) on variable <em class="markup--em markup--p-em">s</em>, <em class="markup--em markup--p-em">t </em>and draw the conclusion that,</p><figure name="705b" id="705b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*hugesEgjSmZHBuyD9HXL-A.png" data-width="1500" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*hugesEgjSmZHBuyD9HXL-A.png"></figure><p name="42e8" id="42e8" class="graf graf--p graf-after--figure">Proof:</p><figure name="1f2a" id="1f2a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*P8pUr--zRCT8yjycE0wT7Q.png" data-width="1500" data-height="108" src="https://cdn-images-1.medium.com/max/800/1*P8pUr--zRCT8yjycE0wT7Q.png"></figure><p name="8476" id="8476" class="graf graf--p graf-after--figure">then, we can have,</p><figure name="e878" id="e878" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3NjwoOPflNaocMaLdTjsmg.png" data-width="1500" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*3NjwoOPflNaocMaLdTjsmg.png"></figure><p name="5d7e" id="5d7e" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(6) Property of Expectation on Independent Random Variables</strong></p><p name="e35c" id="e35c" class="graf graf--p graf-after--p">Let X and Y be independent random variables, then we have,</p><figure name="e595" id="e595" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HbiidP9NVE5MUu1sziB1qg.png" data-width="1324" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*HbiidP9NVE5MUu1sziB1qg.png"></figure><p name="7e9b" id="7e9b" class="graf graf--p graf-after--figure">Proof:</p><p name="abf4" id="abf4" class="graf graf--p graf-after--p">By the definition of expectation, we can then have,</p><figure name="5d7c" id="5d7c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vD6c2xPmqBh9pM_03XWxAg.png" data-width="1436" data-height="108" src="https://cdn-images-1.medium.com/max/800/1*vD6c2xPmqBh9pM_03XWxAg.png"></figure><p name="6de7" id="6de7" class="graf graf--p graf-after--figure">But since X is independent of Y, we have,</p><figure name="2430" id="2430" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PEDaUNngnOK8TlhZrxUBuw.png" data-width="1436" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*PEDaUNngnOK8TlhZrxUBuw.png"></figure><p name="8040" id="8040" class="graf graf--p graf-after--figure">then,</p><figure name="b73f" id="b73f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*B8gFNysUdCMAIZpN_GSkyQ.png" data-width="1436" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*B8gFNysUdCMAIZpN_GSkyQ.png"></figure><p name="6e6a" id="6e6a" class="graf graf--p graf-after--figure">then,</p><figure name="2375" id="2375" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WdrgU44v23-w2BkhLWe9rg.png" data-width="1436" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*WdrgU44v23-w2BkhLWe9rg.png"></figure><p name="5132" id="5132" class="graf graf--p graf-after--figure">then,</p><figure name="3faa" id="3faa" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_IL8YfrbCQzcdggUathgeQ.png" data-width="1436" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*_IL8YfrbCQzcdggUathgeQ.png"></figure><p name="774b" id="774b" class="graf graf--p graf-after--figure">then,</p><figure name="c0bd" id="c0bd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*X_yikYL9rkpckdIUxlAqEQ.png" data-width="1436" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*X_yikYL9rkpckdIUxlAqEQ.png"></figure><p name="7faf" id="7faf" class="graf graf--p graf-after--figure">thus,</p><figure name="7807" id="7807" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*X7PHOyzDU5vbV5keZQadcQ.png" data-width="1436" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*X7PHOyzDU5vbV5keZQadcQ.png"></figure><p name="480a" id="480a" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">WARNING:</strong> The converse statement is not true, i.e., if X and Y are random variables such that,</p><figure name="36f6" id="36f6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UxV74xBKm56JF6Msdlf_2g.png" data-width="1436" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*UxV74xBKm56JF6Msdlf_2g.png"></figure><p name="d930" id="d930" class="graf graf--p graf-after--figure">it doesn’t necessarily mean that X⊥Y. (⊥ is the notation of “being independent of ”). For example, suppose random variable X ~ N(0, 1) and set Y = X². Clearly Y depends on X, which is also to say X and Y are not independent. But,</p><figure name="d01e" id="d01e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*hcGMxWW6a8yY5BbSw0WJkA.png" data-width="1436" data-height="112" src="https://cdn-images-1.medium.com/max/800/1*hcGMxWW6a8yY5BbSw0WJkA.png"></figure><p name="9055" id="9055" class="graf graf--p graf-after--figure">and,</p><figure name="eda0" id="eda0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*398FKGtPHg2h9k3Cy8edFA.png" data-width="1436" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*398FKGtPHg2h9k3Cy8edFA.png"></figure><p name="0566" id="0566" class="graf graf--p graf-after--figure">so,</p><figure name="a97c" id="a97c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EBpoBxrSXd1EuebQ3ArbWg.png" data-width="1436" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*EBpoBxrSXd1EuebQ3ArbWg.png"></figure><p name="26af" id="26af" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(7) Verify Independence for All Random Variables in a Sequence</strong></p><p name="56d0" id="56d0" class="graf graf--p graf-after--p">Suppose we want to verify that X1, … , Xn is an independent sequence of random variables, does it suffice to check that X<em class="markup--em markup--p-em">i </em>⊥ X<em class="markup--em markup--p-em">j</em>, ∀ <em class="markup--em markup--p-em">i ≠</em> <em class="markup--em markup--p-em">j </em>∈{1,2, …, <em class="markup--em markup--p-em">n</em>}?</p><p name="9802" id="9802" class="graf graf--p graf-after--p">In other words, if we want to check pairwise independence of a sequence to confirm the independence of the whole sequence, that works for me to check the exclusivities. Remember there was a lovely lazy track because of the special wired property of the empty set like if A ∩ B = ∅, then that ∩ anything else to be the empty set. So to establish visualized exclusivity of a sequence, we can just check out the pairs because the three-way intersection always includes the two-way intersection, which is the empty set. So by proving the mutual exclusivity for pairs, you then get anything else for free.</p><p name="7465" id="7465" class="graf graf--p graf-after--p">For example,</p><figure name="37bb" id="37bb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*LrHJWPlCmyzTiFYkYcm0QA.png" data-width="1436" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*LrHJWPlCmyzTiFYkYcm0QA.png"></figure><p name="e13c" id="e13c" class="graf graf--p graf-after--figure">The bad news is no, and the property of exclusivities doesn’t happen here! If we check the independence of all pairs in a sequence, it won’t give us the conclusion that we can talk about it to the whole sequence freely. We have to, in theory, check all combinations of the Xi’s for the property that probabilities of intersections equal products of probabilities (or, for random variables. factorizations of joint densities).</p><p name="9076" id="9076" class="graf graf--p graf-after--p">Suppose we have events A, B, and C, and A and B are mutually exclusive, B and C are mutually exclusive, and then C and A are mutually exclusive, we can freely derive that,</p><figure name="50a0" id="50a0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KgCniMJfS0ww5WFtsMIFXg.png" data-width="1436" data-height="48" src="https://cdn-images-1.medium.com/max/800/1*KgCniMJfS0ww5WFtsMIFXg.png"></figure><p name="2bc7" id="2bc7" class="graf graf--p graf-after--figure">But for events A, B, and C, and A and B are independent, B and C independent, and then C and A are independent, it is clear we could not derive that,</p><figure name="3418" id="3418" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*E1GNdccXKZt2mUOtI2ZQoA.png" data-width="1436" data-height="48" src="https://cdn-images-1.medium.com/max/800/1*E1GNdccXKZt2mUOtI2ZQoA.png"></figure><p name="14b3" id="14b3" class="graf graf--p graf-after--figure">So if it doesn’t hold for events, it doesn’t hold for the random variables, so it doesn’t suffice to check merely in pairs, and we have to check all combinations!</p><p name="b8ff" id="b8ff" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Geometrics: The Shape Independent Support Set</strong></p><p name="1faa" id="1faa" class="graf graf--p graf-after--p">What does the shape of the support set of the tuple (X, Y) (for example) tell us about the possibility that X and Y are independent?</p><figure name="5344" id="5344" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rGgECWrXr7Xug_rKLAcdmg.png" data-width="1462" data-height="656" src="https://cdn-images-1.medium.com/max/800/1*rGgECWrXr7Xug_rKLAcdmg.png"></figure><p name="9a0b" id="9a0b" class="graf graf--p graf-after--figure">For the first graph (Square), given X =<em class="markup--em markup--p-em"> x</em>∈[0, 1], we then have Y = <em class="markup--em markup--p-em">y</em> ∈[0, 1], because we cannot know any new information by given X =<em class="markup--em markup--p-em"> x</em> as a condition, so random variables X and Y are possible to be independent.</p><p name="afbb" id="afbb" class="graf graf--p graf-after--p">For the second graph (Quarter circle), X =<em class="markup--em markup--p-em"> x</em>∈[0, 1], when X = 1, we can derive that Y = 0. Because the value of random variable Y is going to depend on the random variable X, so the random variables X and Y are not independent.</p><p name="690f" id="690f" class="graf graf--p graf-after--p">For the third graph (Diamond), it is quite similar to the second graph that the value of random variable X could control the value of random variable Y, so the random variables X and Y are not independent.</p><p name="b42f" id="b42f" class="graf graf--p graf-after--p">For the last graph (Squares), it is quite similar to the first graph that the value of random variable X could not control the value of random variable Y, so the random variables X and Y are possible to be independent.</p><p name="1bb1" id="1bb1" class="graf graf--p graf-after--p">So in conclusion, the key requirement is that a necessary condition for the independence of random variables is that the support set of their joint density must be defined on a multi-dimensional rectangle, i.e. any number of intervals that are unioned, interested, complement, etc, and then crossed (in a cartesian) with another such set. Then, you <strong class="markup--strong markup--p-strong">may</strong> be able to define the random variables so that they are independent over the resulting support set. (We <strong class="markup--strong markup--p-strong">must </strong>also check the factorization condition of the random variables X and Y.)</p><p name="b9fd" id="b9fd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Condition Probability in Bivariate Context</strong></p><p name="94e7" id="94e7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Bivariate Condition Probability of Continuous Random Variables</strong></p><p name="1de1" id="1de1" class="graf graf--p graf-after--p">Suppose that we have continuous random variables X and Y. Let <em class="markup--em markup--p-em">f</em>(<em class="markup--em markup--p-em">x</em>, <em class="markup--em markup--p-em">y</em>) be a joint PDF. The conditional density of X given Y = y is,</p><figure name="6c07" id="6c07" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NaC9zNavK-DP3UQKm4i0jQ.png" data-width="1500" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*NaC9zNavK-DP3UQKm4i0jQ.png"></figure><p name="700d" id="700d" class="graf graf--p graf-after--figure">Proof:</p><p name="0708" id="0708" class="graf graf--p graf-after--p">Recall the definition of the conditional probability is ℙ(A|B)=ℙ(A ∩ B)/ℙ(B), which means that for given events A and B, we can do the probability of A given B. What we finally get is an intersection probability normalized by the probability of the conditioning event.</p><p name="bdcc" id="bdcc" class="graf graf--p graf-after--p">So for the continuous random variable X and Y, the intersection probability on X = <em class="markup--em markup--p-em">x</em> and Y = <em class="markup--em markup--p-em">y</em> could be calculated by their joint density function. This is basically the meaning of the joint density (the joint behavior of tuple X, Y = (<em class="markup--em markup--p-em">x</em>, <em class="markup--em markup--p-em">y</em>)) and this is also the thing that controls how X and Y can happen at the same time.</p><p name="5f8d" id="5f8d" class="graf graf--p graf-after--p">Then we suppose that the event A = {X = <em class="markup--em markup--p-em">x</em>} and B = {Y =<em class="markup--em markup--p-em"> y</em>}, so the intersecting probability (<strong class="markup--strong markup--p-strong">joint density</strong>) of X and Y is one natrual thing to go as,</p><figure name="4583" id="4583" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3HxYaiVv4UMwJLgRIbxlRQ.png" data-width="1324" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*3HxYaiVv4UMwJLgRIbxlRQ.png"></figure><p name="f11f" id="f11f" class="graf graf--p graf-after--figure">Then we want to normalize this probability by conditional event, which is event B. This is going to make sense by dividing the joint density by the density function of random variable Y, and this is also called the <strong class="markup--strong markup--p-strong">marginal density</strong> for just one random variable as the conditioning events.</p><figure name="446f" id="446f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ctqPGw-NJtEoIWLXYylQdQ.png" data-width="1324" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*ctqPGw-NJtEoIWLXYylQdQ.png"></figure><p name="508a" id="508a" class="graf graf--p graf-after--figure">Because we don’t want to make things confusing so that we use <em class="markup--em markup--p-em">f</em> again for the conditional probability. We would like to use this notation as a subscript, X|Y, to represent the random variable with respect to the notation A|B. Finally, we also use this notation in the argument of this function as <em class="markup--em markup--p-em">x</em>|<em class="markup--em markup--p-em">y</em> here. So, we have</p><figure name="ed38" id="ed38" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NaC9zNavK-DP3UQKm4i0jQ.png" data-width="1500" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*NaC9zNavK-DP3UQKm4i0jQ.png"></figure><p name="06f5" id="06f5" class="graf graf--p graf-after--figure">Note:</p><p name="4228" id="4228" class="graf graf--p graf-after--p">It is quite important that the expressions</p><figure name="51f3" id="51f3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*krAkBVpd4EM-mNGqKN_uxw.png" data-width="1324" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*krAkBVpd4EM-mNGqKN_uxw.png"></figure><p name="745b" id="745b" class="graf graf--p graf-after--figure">are both bivariate functions but they are very different bivariate functions. The second one is a joint density of random variable X and Y, and what separates them is a comma which we usually separate arguments in a function. The first one uses a bar sign to separate arguments so that we can know whether or not it is a conditional density versus the joint density by the fact that whether there is a bar or comma, or has it got a subscript to tell you which random variable you are dealing with here.</p><p name="4271" id="4271" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Bivariate Discrete Probability of Discrete Random Variables</strong></p><p name="26d5" id="26d5" class="graf graf--p graf-after--p">Suppose that we have continuous random variables X and Y. Let <em class="markup--em markup--p-em">p</em>(<em class="markup--em markup--p-em">x</em>, <em class="markup--em markup--p-em">y</em>) be a joint PMF. The conditional density of X given the condition Y = y is,</p><figure name="9e9d" id="9e9d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tlaHMUPwmU4VxcX5GEWMJA.png" data-width="1500" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*tlaHMUPwmU4VxcX5GEWMJA.png"></figure><p name="3975" id="3975" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Draw the Marginal Probability Function from the Joint Probability Function</strong></p><p name="50cf" id="50cf" class="graf graf--p graf-after--p">Sometimes we only have the joint function of random variables X and Y, but if we want to calculate the conditional probability, we must have the marginal probability function. The method that we can calculate the marginal function by giving a joint probability function is as follows.</p><p name="aa31" id="aa31" class="graf graf--p graf-after--p">For continuous random variables X and Y, we have,</p><figure name="0068" id="0068" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PS30dmA6R3F9YfAinLZwtA.png" data-width="1324" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*PS30dmA6R3F9YfAinLZwtA.png"></figure><figure name="0198" id="0198" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*peIMEiv_fyhf3-DwSe61NQ.png" data-width="1324" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*peIMEiv_fyhf3-DwSe61NQ.png"></figure><p name="bf97" id="bf97" class="graf graf--p graf-after--figure">For discrete random variables X and Y, we have,</p><figure name="ace3" id="ace3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xCuUPZyvfA_Yi_C1U0lSlw.png" data-width="1324" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*xCuUPZyvfA_Yi_C1U0lSlw.png"></figure><figure name="fd40" id="fd40" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*SbK79XRZM_ZoxNZUUMb-_w.png" data-width="1324" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*SbK79XRZM_ZoxNZUUMb-_w.png"></figure><p name="f2ba" id="f2ba" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) The Expectation of Conditional Probability with Continuous Random Variables</strong></p><p name="8fc8" id="8fc8" class="graf graf--p graf-after--p">Assume that X and Y are continuous random variables, it is then clear to have the bivariate conditional probability of them based on the previous discussion, which is,</p><figure name="9871" id="9871" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NaC9zNavK-DP3UQKm4i0jQ.png" data-width="1500" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*NaC9zNavK-DP3UQKm4i0jQ.png"></figure><p name="25c3" id="25c3" class="graf graf--p graf-after--figure">So if we define the expectation of random variable X given the condition Y = <em class="markup--em markup--p-em">y</em>, the expectation could be,</p><figure name="92ec" id="92ec" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*a-lDh8fHsR7vPS3DxIO2qA.png" data-width="1324" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*a-lDh8fHsR7vPS3DxIO2qA.png"></figure><p name="9374" id="9374" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Note that the X|Y=<em class="markup--em markup--p-em">y</em> is exactly a random variable!</strong></p><p name="8f5e" id="8f5e" class="graf graf--p graf-after--p">Let’s suppose we assign y as a constant, like π or something else, the expectation of X given Y = π will definitely be a constant. But here, if we use an abstract <em class="markup--em markup--p-em">y</em> to replace the π here, the result will be a function with an argument <em class="markup--em markup--p-em">y</em>. So that we can also write this expectation as a function formation φ(y).</p><p name="f71d" id="f71d" class="graf graf--p graf-after--p">This definition of expectation is going to be really important during the whole regression part, and it has many applications in practice. For example, we may be given a bunch of features that will be of many dimensions and we may want to compute average or dependence variables given or conditioned by something, like indicating variables or indicating the membership in a group. For the classification problem, there will be a number of variables to calculate how many bucks people are going to spend on your website or number of times that the members use uber a year or something like that. In the regression part, that will be a hyperplane model based on this expectation, no need to mention other points of view related to machine learning or statistical techniques.</p><p name="9471" id="9471" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) The Variance of Conditional Probability with Continuous Random Variables</strong></p><p name="549c" id="549c" class="graf graf--p graf-after--p">So now, how can we define <em class="markup--em markup--p-em">Var</em>(X|Y=<em class="markup--em markup--p-em">y</em>)? Recall what we have already learned previously. We have the famous short cut of variance as,</p><figure name="5202" id="5202" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NnO-tDPAjSaPCSwL6QHd-Q.png" data-width="1324" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*NnO-tDPAjSaPCSwL6QHd-Q.png"></figure><p name="db5f" id="db5f" class="graf graf--p graf-after--figure">so that,</p><figure name="f7df" id="f7df" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PxT6sW_PakqpinCTUnC9Pw.png" data-width="1324" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*PxT6sW_PakqpinCTUnC9Pw.png"></figure><p name="1252" id="1252" class="graf graf--p graf-after--figure">therefore, because we have already got conditional first moment𝔼[X|Y=y]², we have to calculate the conditional second moment 𝔼[X²|Y=y] then,</p><figure name="3f0b" id="3f0b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*7s2_F_ko9M4QsV51EoHeEg.png" data-width="1324" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*7s2_F_ko9M4QsV51EoHeEg.png"></figure><p name="1e47" id="1e47" class="graf graf--p graf-after--figure">Thus, we have,</p><figure name="5b20" id="5b20" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UZ9ZyFT5RTBJilDojUeYNg.png" data-width="1324" data-height="110" src="https://cdn-images-1.medium.com/max/800/1*UZ9ZyFT5RTBJilDojUeYNg.png"></figure><p name="1941" id="1941" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(6) Abstract Examples of Conditional Probability</strong></p><p name="b3c9" id="b3c9" class="graf graf--p graf-after--p">Suppose we have a random vector (X, X2, …, X5), how would we define (a) the conditional density of (X1, X2, X4, X5) given X3 and (b) the conditional density (X1, X4, X5) given (X2, X3)?</p><p name="e59e" id="e59e" class="graf graf--p graf-after--p">(a) The answer is,</p><figure name="d5c1" id="d5c1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WTjQ5U2RPfPhjYJg1cDoKg.png" data-width="1436" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*WTjQ5U2RPfPhjYJg1cDoKg.png"></figure><p name="9d9e" id="9d9e" class="graf graf--p graf-after--figure">(b) The answer is,</p><figure name="5fdf" id="5fdf" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*52o52DtA4K5ChMtx3NOT0w.png" data-width="1436" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*52o52DtA4K5ChMtx3NOT0w.png"></figure><p name="c0f3" id="c0f3" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(7) Concrete Examples of Conditional Probability</strong></p><p name="8e66" id="8e66" class="graf graf--p graf-after--p">Suppose that the joint PDF of X and Y is given by,</p><figure name="6863" id="6863" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*hzb5nUAuS-pLmvPLFDUFlw.png" data-width="1436" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*hzb5nUAuS-pLmvPLFDUFlw.png"></figure><p name="a961" id="a961" class="graf graf--p graf-after--figure">for 0 &lt; <em class="markup--em markup--p-em">x</em> &lt; 1 and 0 &lt; <em class="markup--em markup--p-em">y</em> &lt; 1,</p><p name="706a" id="706a" class="graf graf--p graf-after--p">(a) what is f_(X|Y)(<em class="markup--em markup--p-em">x</em>|<em class="markup--em markup--p-em">y</em>)?</p><p name="83e8" id="83e8" class="graf graf--p graf-after--p">Ans:</p><figure name="9f09" id="9f09" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kX7katLEyTmen_54sXfTRw.png" data-width="1436" data-height="156" src="https://cdn-images-1.medium.com/max/800/1*kX7katLEyTmen_54sXfTRw.png"></figure><p name="e7a3" id="e7a3" class="graf graf--p graf-after--figure">so that in conclusion,</p><figure name="f7c7" id="f7c7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vfwwmnPb55pTzEzVON1Hdg.png" data-width="1436" data-height="230" src="https://cdn-images-1.medium.com/max/800/1*vfwwmnPb55pTzEzVON1Hdg.png"></figure><p name="c304" id="c304" class="graf graf--p graf-after--figure">(b) what is the probability that X∈(1/2, 2/3) given Y = 1/10?</p><p name="15d4" id="15d4" class="graf graf--p graf-after--p">Ans:</p><figure name="4f3c" id="4f3c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-ncqzOSpuP6Vpgav-wsfCg.png" data-width="1436" data-height="158" src="https://cdn-images-1.medium.com/max/800/1*-ncqzOSpuP6Vpgav-wsfCg.png"></figure><p name="ab97" id="ab97" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(8) Conditional Probability on Independent Continuous Random Variables</strong></p><p name="a434" id="a434" class="graf graf--p graf-after--p">Suppose that we have X and Y as independent continuous random variables, and based on the factorization properties of densities, we have,</p><figure name="264e" id="264e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PEDaUNngnOK8TlhZrxUBuw.png" data-width="1436" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*PEDaUNngnOK8TlhZrxUBuw.png"></figure><p name="1dfa" id="1dfa" class="graf graf--p graf-after--figure">Then, based on the definition of the conditional probability of X and Y, we could have the formula as,</p><figure name="61ff" id="61ff" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NaC9zNavK-DP3UQKm4i0jQ.png" data-width="1500" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*NaC9zNavK-DP3UQKm4i0jQ.png"></figure><p name="ef34" id="ef34" class="graf graf--p graf-after--figure">thus, we have,</p><figure name="88de" id="88de" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-ilqSxyS2ug59KONjB9osg.png" data-width="1462" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*-ilqSxyS2ug59KONjB9osg.png"></figure><p name="e616" id="e616" class="graf graf--p graf-after--figure">Similarly,</p><figure name="51b6" id="51b6" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*5QpKfFMwGDahPjth2V7rqA.png" data-width="1462" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*5QpKfFMwGDahPjth2V7rqA.png"></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/c7fe9e7eafd6"><time class="dt-published" datetime="2020-08-31T00:37:09.187Z">August 31, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/probability-and-statistics-5-joint-probability-independence-of-random-variables-and-condition-c7fe9e7eafd6" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>