<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Probability and Statistics 6 | Maximum Likelihood Estimation and Central Limit Theorem</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Probability and Statistics 6 | Maximum Likelihood Estimation and Central Limit Theorem</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Probability and Statistics
</section>
<section data-field="body" class="e-content">
<section name="e669" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1653" id="1653" class="graf graf--h3 graf--leading graf--title">Probability and Statistics 6 | <strong class="markup--strong markup--h3-strong">Maximum Likelihood Estimation and Central Limit Theorem</strong></h3><figure name="90b5" id="90b5" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*YTeSIjiYirbk4CZavQXCqg.jpeg" data-width="1602" data-height="1141" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*YTeSIjiYirbk4CZavQXCqg.jpeg"></figure><ol class="postList"><li name="5600" id="5600" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Maximum Likelihood Estimation</strong></li></ol><p name="bb2a" id="bb2a" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Distribution Function of Maximizing: An Example</strong></p><p name="7409" id="7409" class="graf graf--p graf-after--p">Suppose that the circuits in some systems have a lifetime that is exponentially distributed with parameter λ.</p><figure name="469e" id="469e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*L3ADV91uoTEHL_jNV9La_Q.png" data-width="1318" data-height="340" src="https://cdn-images-1.medium.com/max/800/1*L3ADV91uoTEHL_jNV9La_Q.png"></figure><p name="6ad4" id="6ad4" class="graf graf--p graf-after--figure">Suppose that the system runs on <em class="markup--em markup--p-em">n</em> circuits and it operates until <strong class="markup--strong markup--p-strong">all</strong> <em class="markup--em markup--p-em">n</em> circuits die. Assume that the circuits are independent of one another. The question is that what is the distribution (i.e. CDF) of the lifetime of the system?</p><p name="5a21" id="5a21" class="graf graf--p graf-after--p">Ans:</p><p name="afcc" id="afcc" class="graf graf--p graf-after--p">Let Xi be the lifetime of the <em class="markup--em markup--p-em">i</em>-th, then the random vector (X1, X2, …, Xn) has all the information we need to determine the lifetime of the system. So we are defining a random variable Z = max{X1, X2, …, Xn} and we want the probability distribution of random variable Z as,</p><figure name="5907" id="5907" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vRawlrfPZ2Y2R508T5s-aA.png" data-width="1318" data-height="48" src="https://cdn-images-1.medium.com/max/800/1*vRawlrfPZ2Y2R508T5s-aA.png"></figure><p name="6684" id="6684" class="graf graf--p graf-after--figure">Then we have,</p><figure name="85ff" id="85ff" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-V6SYeAtmJsBgap720TpdQ.png" data-width="1318" data-height="48" src="https://cdn-images-1.medium.com/max/800/1*-V6SYeAtmJsBgap720TpdQ.png"></figure><p name="6b73" id="6b73" class="graf graf--p graf-after--figure">then, by definition of maximizing,</p><figure name="879f" id="879f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RGuS7XJgNZbIXejO46MgPA.png" data-width="1318" data-height="48" src="https://cdn-images-1.medium.com/max/800/1*RGuS7XJgNZbIXejO46MgPA.png"></figure><p name="e1ef" id="e1ef" class="graf graf--p graf-after--figure">then, by independence,</p><figure name="ac7a" id="ac7a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kEWFG1gUg-Mb7KK8MLWCsA.png" data-width="1318" data-height="48" src="https://cdn-images-1.medium.com/max/800/1*kEWFG1gUg-Mb7KK8MLWCsA.png"></figure><p name="d72a" id="d72a" class="graf graf--p graf-after--figure">then, by definition of CDF,</p><figure name="cf3e" id="cf3e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5aBopY4qgRn6g7W6_YL9yg.png" data-width="1318" data-height="48" src="https://cdn-images-1.medium.com/max/800/1*5aBopY4qgRn6g7W6_YL9yg.png"></figure><p name="2309" id="2309" class="graf graf--p graf-after--figure">then, by X<em class="markup--em markup--p-em">i</em> ~ <em class="markup--em markup--p-em">Exp</em>(λ)</p><figure name="ffbb" id="ffbb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3QvAzdoTVSU0yvlvPnmxsw.png" data-width="1318" data-height="52" src="https://cdn-images-1.medium.com/max/800/1*3QvAzdoTVSU0yvlvPnmxsw.png"></figure><p name="47e8" id="47e8" class="graf graf--p graf-after--figure">then, finally, we have,</p><figure name="1581" id="1581" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EW2me0AuAFZjTnHBbfhivg.png" data-width="1318" data-height="52" src="https://cdn-images-1.medium.com/max/800/1*EW2me0AuAFZjTnHBbfhivg.png"></figure><p name="39f3" id="39f3" class="graf graf--p graf-after--figure">So this is the distribution function of maximizing, and if we differentiated it with respect to t, we can get the PDF as,</p><figure name="2c75" id="2c75" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2-UL2YTv_tvr8QcPScgqUg.png" data-width="1318" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*2-UL2YTv_tvr8QcPScgqUg.png"></figure><p name="befd" id="befd" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Distribution Function of Minimizing</strong></p><p name="ece4" id="ece4" class="graf graf--p graf-after--p">We have discussed the distribution function of maximizing, and now we would like to define the distribution function of minimizing. It is quite similar and our basic idea is to change the minimizing to the form of maximizing.</p><p name="46bc" id="46bc" class="graf graf--p graf-after--p">For example, if the system runs on <em class="markup--em markup--p-em">n</em> circuits and it stops operating when at least one circuits die. Assume that the circuits are independent of one another. The question is that what is the distribution (i.e. CDF) of the lifetime of the system?</p><p name="d93d" id="d93d" class="graf graf--p graf-after--p">Suppose if we have random variable W defined by,</p><figure name="092f" id="092f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Q5muD47hrcMzYShnwjqkPw.png" data-width="1318" data-height="50" src="https://cdn-images-1.medium.com/max/800/1*Q5muD47hrcMzYShnwjqkPw.png"></figure><p name="8016" id="8016" class="graf graf--p graf-after--figure">We can then have,</p><figure name="9699" id="9699" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*sJ7WqI2XPGbGkdnqWHFakA.png" data-width="1318" data-height="50" src="https://cdn-images-1.medium.com/max/800/1*sJ7WqI2XPGbGkdnqWHFakA.png"></figure><p name="c4b7" id="c4b7" class="graf graf--p graf-after--figure">then,</p><figure name="9fae" id="9fae" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tXA-bFEA7sWFT-x2k1fS5Q.png" data-width="1318" data-height="50" src="https://cdn-images-1.medium.com/max/800/1*tXA-bFEA7sWFT-x2k1fS5Q.png"></figure><p name="5e23" id="5e23" class="graf graf--p graf-after--figure">then,</p><figure name="6c91" id="6c91" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KW1VDzB6_8Qie32FtF-neg.png" data-width="1318" data-height="50" src="https://cdn-images-1.medium.com/max/800/1*KW1VDzB6_8Qie32FtF-neg.png"></figure><p name="72b7" id="72b7" class="graf graf--p graf-after--figure">then,</p><figure name="0a1f" id="0a1f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qaT5GRRtKIxNlJfHE-KwvQ.png" data-width="1318" data-height="50" src="https://cdn-images-1.medium.com/max/800/1*qaT5GRRtKIxNlJfHE-KwvQ.png"></figure><p name="2bc2" id="2bc2" class="graf graf--p graf-after--figure">then,</p><figure name="a800" id="a800" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BHNX7v1Fh6FxJKbsWHeBZQ.png" data-width="1318" data-height="50" src="https://cdn-images-1.medium.com/max/800/1*BHNX7v1Fh6FxJKbsWHeBZQ.png"></figure><p name="fb45" id="fb45" class="graf graf--p graf-after--figure">then,</p><figure name="a883" id="a883" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4CJCTgUswTigAN0tyNGqzQ.png" data-width="1318" data-height="50" src="https://cdn-images-1.medium.com/max/800/1*4CJCTgUswTigAN0tyNGqzQ.png"></figure><p name="124d" id="124d" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) The Definition of Statistic</strong></p><p name="b1c0" id="b1c0" class="graf graf--p graf-after--p">The most common example of statistics are things like,</p><figure name="97f3" id="97f3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dxY7TSKGoPsQXLopYp111Q.png" data-width="1252" data-height="158" src="https://cdn-images-1.medium.com/max/800/1*dxY7TSKGoPsQXLopYp111Q.png"></figure><p name="8613" id="8613" class="graf graf--p graf-after--figure">or,</p><figure name="2d39" id="2d39" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1JdedmclGoTElIU9XTFqiw.png" data-width="1388" data-height="128" src="https://cdn-images-1.medium.com/max/800/1*1JdedmclGoTElIU9XTFqiw.png"></figure><p name="47e1" id="47e1" class="graf graf--p graf-after--figure">in general, we call the abstract function of these things that intended to be estimators for some underlying population parameters as θ.</p><p name="6d94" id="6d94" class="graf graf--p graf-after--p">The official definition of the statistic θ is just a function of a random sample that could be used to find the underlying population parameters. The random sample is going to be defined below.</p><p name="f5e2" id="f5e2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) The Definition of Random Sample</strong></p><p name="d97a" id="d97a" class="graf graf--p graf-after--p">A random sample is just a (typically finite) sequence of <strong class="markup--strong markup--p-strong">independent</strong> and <strong class="markup--strong markup--p-strong">identically distributed</strong> random variables. Independence means that all combinations of random variables are independent and identical distribution means that all the random variable X follows the same distribution.</p><p name="860c" id="860c" class="graf graf--p graf-after--p">Two random variables are identically distributed are not the same as two random variables that are identically equal. Two random variables are identically distributed means that they have the same CDF function without themselves being identically equal. Two random variables are identically equal means that these two variables are for the same results.</p><p name="383b" id="383b" class="graf graf--p graf-after--p">We have just worked on a random sample, for example, in the first example above, the random vector (X1, X2, …, Xn) could be treated as a random sample because all X<em class="markup--em markup--p-em">i </em>~ <em class="markup--em markup--p-em">Exp</em>(λ) and independent, so it can be viewed as such.</p><p name="e74b" id="e74b" class="graf graf--p graf-after--p">Also, in this example, max is just one function over this random sample, which is defined as, what we have said above, statistic.</p><p name="29ab" id="29ab" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Examples of Statistics</strong></p><p name="6ae2" id="6ae2" class="graf graf--p graf-after--p">There are many possible statistics,</p><figure name="7605" id="7605" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xjSebH35_3jkhGEqUIf-4g.png" data-width="1362" data-height="426" src="https://cdn-images-1.medium.com/max/800/1*xjSebH35_3jkhGEqUIf-4g.png"></figure><p name="8d65" id="8d65" class="graf graf--p graf-after--figure">So how do I find good statistics, for example, statistics that are useful for estimating underlying <strong class="markup--strong markup--p-strong">population parameters</strong>?</p><p name="5fd8" id="5fd8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) The Definition of Population Parameters</strong></p><p name="5c1b" id="5c1b" class="graf graf--p graf-after--p">The population parameters governing a random variable are just those parameters that fully determine its distribution (aka. CDF).</p><ul class="postList"><li name="d503" id="d503" class="graf graf--li graf-after--p">Normal Distribution</li></ul><figure name="dd39" id="dd39" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*JqpQIylQqPHCAwtLT1TnnQ.png" data-width="1362" data-height="82" src="https://cdn-images-1.medium.com/max/800/1*JqpQIylQqPHCAwtLT1TnnQ.png"></figure><ul class="postList"><li name="ee20" id="ee20" class="graf graf--li graf-after--figure">Stable Random Variable</li></ul><figure name="3d78" id="3d78" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*YazXmRsWogAoQABjdosOWA.png" data-width="1362" data-height="82" src="https://cdn-images-1.medium.com/max/800/1*YazXmRsWogAoQABjdosOWA.png"></figure><ul class="postList"><li name="e17b" id="e17b" class="graf graf--li graf-after--figure">Exponential Distribution</li></ul><figure name="1fcb" id="1fcb" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*xeP9AblFIoJBWQsTS2FFxQ.png" data-width="1362" data-height="82" src="https://cdn-images-1.medium.com/max/800/1*xeP9AblFIoJBWQsTS2FFxQ.png"></figure><ul class="postList"><li name="deef" id="deef" class="graf graf--li graf-after--figure">Bernoulli Distribution</li></ul><figure name="9fc1" id="9fc1" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*cCthTgG7jnnIl4HdObrvyg.png" data-width="1362" data-height="82" src="https://cdn-images-1.medium.com/max/800/1*cCthTgG7jnnIl4HdObrvyg.png"></figure><p name="c0f8" id="c0f8" class="graf graf--p graf-after--figure">So the thing we should keep in mind here is the idea of the platonic form: there is a true world and it has perfect form and humans can quite access to it or see it, which, under a religious view, we may say “God knows” and maybe only god really knows what these parameters are and we don’t know, but we can take a peek of them through collecting data and estimating.</p><p name="8e57" id="8e57" class="graf graf--p graf-after--p">If we want to, for example, suppose that incomes follow an exponential distribution with parameter λ, the question may be what is the true λ. There is actually an ideal λ and it does exist, but it is never accessible. So we have to estimate it through a collection of data or using statistics, and usually, we asked about the uncertainty of the estimates so that we end up building confidence intervals, which will be a topic in the following sections.</p><p name="9955" id="9955" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) The Definition of Likelihood Function</strong></p><p name="172a" id="172a" class="graf graf--p graf-after--p">Suppose that Xi is a random variable with density <em class="markup--em markup--p-em">f</em>Xi(θ; <em class="markup--em markup--p-em">x</em>i). This notation of density function is an evaluation depending on two collections of variables. One is the actual position on the x-axis, order through X1, X2, X3, … in real space while you are evaluating the density function. But the other thing that influences the density function is the statistics or parameters that control it. So in case of a normal distribution, there are three things that define a normal density, we get to tell an x-location where you would like to evaluate, you have to tell μ and you have to tell σ². So θ is intended here in the density function to represent the parameters that control the density and <em class="markup--em markup--p-em">x</em>i’s is a particular location of x the x-axis.</p><p name="86f1" id="86f1" class="graf graf--p graf-after--p">For a random sample X1, X2, …, Xn with <strong class="markup--strong markup--p-strong">common</strong> density <em class="markup--em markup--p-em">f</em>Xi(θ; <em class="markup--em markup--p-em">x</em>i), <em class="markup--em markup--p-em">i</em> = 1, 2, …, n, the likelihood function is defined as,</p><figure name="9ff8" id="9ff8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vUk4vkvemAecd4gWLuPMsg.png" data-width="1362" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*vUk4vkvemAecd4gWLuPMsg.png"></figure><p name="5e37" id="5e37" class="graf graf--p graf-after--figure">And because the random variables X1, X2, …, and Xn belongs to a random sample (X1, X2, …, Xn), then by definition we can know that all these random variables are independent. Thus, we can then have,</p><figure name="2ca7" id="2ca7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WsbDB8Y8G5LXES9TYXm-ww.png" data-width="1362" data-height="144" src="https://cdn-images-1.medium.com/max/800/1*WsbDB8Y8G5LXES9TYXm-ww.png"></figure><p name="dc61" id="dc61" class="graf graf--p graf-after--figure">which is also,</p><figure name="3d34" id="3d34" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*aX3gzG7UeR6vzXDmByNX6Q.png" data-width="1362" data-height="122" src="https://cdn-images-1.medium.com/max/800/1*aX3gzG7UeR6vzXDmByNX6Q.png"></figure><p name="47c7" id="47c7" class="graf graf--p graf-after--figure">the second expression is because all the Xi’s follow the same density function, so we don’t have to distinguish each density <em class="markup--em markup--p-em">f</em> with a subscript Xi, and we can just eliminate the subscript and use <em class="markup--em markup--p-em">f</em> for each of random variables.</p><p name="45c3" id="45c3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Maximizing the Value of Likelihood Function</strong></p><p name="1a13" id="1a13" class="graf graf--p graf-after--p">Let’s say that Yale draws 10 random variables and she knows that the random variables are from an exponential distribution. The god secretly knows that the parameter λ of this exponential distribution is 1/3 and the mean of a sample from this distribution is quite around 3 (by the property of exponential distribution).</p><figure name="bae7" id="bae7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_duokJZTKWBUEQ0nk4AFyg.png" data-width="1552" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*_duokJZTKWBUEQ0nk4AFyg.png"></figure><p name="a137" id="a137" class="graf graf--p graf-after--figure">So if Yale would like to use a density function to describe these data in the bast way, it is okay to choose any λ as a parameter even though the result may be poor. What she can do on her best, is to choose a θ with which the density function best describes the density of the data. From the definition above, if we want to have a maximum probability density given parameter λ as a variable, what we can do is to maximize the likelihood function and then derive λ when the likelihood is maximized.</p><figure name="c74b" id="c74b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*I6gHjcsKPesZTJvMEeVDEg.png" data-width="1552" data-height="72" src="https://cdn-images-1.medium.com/max/800/1*I6gHjcsKPesZTJvMEeVDEg.png"></figure><p name="67ae" id="67ae" class="graf graf--p graf-after--figure">So the definition is that suppose we have a random sample X1, X2, …, Xn. You regard it as fixed. How do you choose a θ that best comports with, or is most compatible with these data? The answer is to choose a θ that maximizes,</p><figure name="95c7" id="95c7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DwAf9U7cTc-o6AURNJ1aFA.png" data-width="1362" data-height="118" src="https://cdn-images-1.medium.com/max/800/1*DwAf9U7cTc-o6AURNJ1aFA.png"></figure><p name="f1f7" id="f1f7" class="graf graf--p graf-after--figure">so, we will choose,</p><figure name="3a99" id="3a99" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*05tvMv1JQl39LFAuW-0PwQ.png" data-width="1362" data-height="78" src="https://cdn-images-1.medium.com/max/800/1*05tvMv1JQl39LFAuW-0PwQ.png"></figure><p name="20a4" id="20a4" class="graf graf--p graf-after--figure">The notation of ^ on any parameters or functions is the thing that we estimate or guess, not the real value based on the population.</p><p name="9fe5" id="9fe5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) Log-likelihood Function:</strong> A Simpler Solution for Differential</p><p name="98b5" id="98b5" class="graf graf--p graf-after--p">Maximizing or minimizing a function that is the product of functions is yucky. (Why?) Because it requires the use of the <strong class="markup--strong markup--p-strong">product rule or </strong>maybe the <strong class="markup--strong markup--p-strong">chain rule</strong> as well<strong class="markup--strong markup--p-strong">, </strong>which can be sloppy.</p><figure name="33b5" id="33b5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*JbrIZMgOU2CANBuYhIMS5g.png" data-width="1684" data-height="186" src="https://cdn-images-1.medium.com/max/800/1*JbrIZMgOU2CANBuYhIMS5g.png"></figure><p name="d511" id="d511" class="graf graf--p graf-after--figure">We would like things to be more elegant and simpler compared with the differential rules, so what’s going to be the solution here? Or something that we don’t have to use the product rule, but something much much easier than that?</p><p name="e17c" id="e17c" class="graf graf--p graf-after--p">The solution is to use a log-algorithm! So the likelihood function with log outside (based on the log-algorithm) is so-called the log-likelihood function. It is defined by given the log-likelihood of random sample X1, …, Xn, given θ, is the function as,</p><figure name="3256" id="3256" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NThcB0BoQ2bL2bN1TaL4Wg.png" data-width="1684" data-height="128" src="https://cdn-images-1.medium.com/max/800/1*NThcB0BoQ2bL2bN1TaL4Wg.png"></figure><p name="a14f" id="a14f" class="graf graf--p graf-after--figure">Note we mean that the log process is legal if we get the same result of estimated θ when we do maximization to both <em class="markup--em markup--p-em">L</em> (argmax <em class="markup--em markup--p-em">L</em>)and <em class="markup--em markup--p-em">l </em>(argmax <em class="markup--em markup--p-em">l</em>), and, argmax <em class="markup--em markup--p-em">L </em>and<em class="markup--em markup--p-em"> </em>argmax <em class="markup--em markup--p-em">l </em>are equivalent to each other. The reason why can this <em class="markup--em markup--p-em">log</em> process be legal for our likelihood function is that this guy <em class="markup--em markup--p-em">log</em>(<em class="markup--em markup--p-em">x</em>) is an increasing function (aka. non-decreasing function).</p><figure name="5adb" id="5adb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VFZKnzg_E0oT4Gqa_RFo5A.png" data-width="1684" data-height="464" src="https://cdn-images-1.medium.com/max/800/1*VFZKnzg_E0oT4Gqa_RFo5A.png"></figure><p name="f2c5" id="f2c5" class="graf graf--p graf-after--figure">In short, we can say that this is legal because the monotonic increasing functions do not change the value of their extrema.</p><p name="862f" id="862f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) The Definition of Maximum Likelihood Estimator (MLE)</strong></p><p name="1b75" id="1b75" class="graf graf--p graf-after--p">For a random sample X1, X2, …, Xn (i.i.d. sequence of random variables, i.i.d is the abbreviation of “independent identical distribution”) with common density <em class="markup--em markup--p-em">f</em>(θ; Xi), we call,</p><figure name="265f" id="265f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MMHPZQHMv_B_e3UFEMHvww.png" data-width="1684" data-height="130" src="https://cdn-images-1.medium.com/max/800/1*MMHPZQHMv_B_e3UFEMHvww.png"></figure><p name="ba4d" id="ba4d" class="graf graf--p graf-after--figure">the maximum likelihood estimator (MLE) for statistic θ.</p><p name="af70" id="af70" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(11) Steps Of Doing Maximum Likelihood Estimation</strong></p><ul class="postList"><li name="03b9" id="03b9" class="graf graf--li graf-after--p">Form the likelihood function <em class="markup--em markup--li-em">L</em></li><li name="3ca0" id="3ca0" class="graf graf--li graf-after--li">Create the log-likelihood function<em class="markup--em markup--li-em"> l</em></li><li name="e377" id="e377" class="graf graf--li graf-after--li">Compute the partial derivatives with the respect to each unknown parameter</li><li name="ff63" id="ff63" class="graf graf--li graf-after--li">Set each resulting expression equal to zero</li><li name="fd7d" id="fd7d" class="graf graf--li graf-after--li">Solve the resulting equation or set of equators to identify critical points</li><li name="08b3" id="08b3" class="graf graf--li graf-after--li">Use second-order conditions (i.e. a second derivative test, e.g. might need to use Hessian) to determine which critical point is maximum. (Possibly need to check endpoints)</li></ul><p name="f579" id="f579" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(12) Maximum Likelihood Estimation on Normal Distribution</strong></p><p name="5ab7" id="5ab7" class="graf graf--p graf-after--p">Suppose that X1, …, Xn is a random sample from a normal distribution with unknown mean μ and known variance σ², we can calculate the estimator of μ based on the steps above. We won’t go into any details here, but the conclusion of this is going to be,</p><figure name="4861" id="4861" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mogqDAtwSTIagiq_koQbvA.png" data-width="1388" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*mogqDAtwSTIagiq_koQbvA.png"></figure><p name="fa01" id="fa01" class="graf graf--p graf-after--figure">Moreover, if you solve the same problem with σ² unknown as well, you will find the following joint solution as,</p><figure name="64ab" id="64ab" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*h77-40pTkf4qBkpl2XbMKA.png" data-width="1388" data-height="192" src="https://cdn-images-1.medium.com/max/800/1*h77-40pTkf4qBkpl2XbMKA.png"></figure><p name="ac83" id="ac83" class="graf graf--p graf-after--figure">But it is not what we normally saw for an estimator of the variance. While actually, what the MLE for variance is not the exact formula that we are going to use for sample variance as,</p><figure name="35e6" id="35e6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*za8CAfUFelfSNDIwdcUBqg.png" data-width="1388" data-height="128" src="https://cdn-images-1.medium.com/max/800/1*za8CAfUFelfSNDIwdcUBqg.png"></figure><p name="6f33" id="6f33" class="graf graf--p graf-after--figure">This is because the MLE produces possible statistics that may not be the “best” in some way, for example, they could be biased. So because those statistics produced by MLE is not unbiased, the bias adjustments may be required here.</p><p name="76e1" id="76e1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Central Limit Theorem</strong></p><p name="0603" id="0603" class="graf graf--p graf-after--p">We now know how to find (possible good) statistics. We might like to know things about them. For example, their distributions. It turns out that the CLT (Central Limit Theorem) plays a key role in describing the distribution of statistics like the distribution of the random variable</p><figure name="a705" id="a705" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*GRFDS4MWbFo7sjJ8bOjZTg.png" data-width="1388" data-height="76" src="https://cdn-images-1.medium.com/max/800/1*GRFDS4MWbFo7sjJ8bOjZTg.png"></figure><p name="d4d1" id="d4d1" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(1) The Definition of Central Limit Theorem</strong></p><p name="4441" id="4441" class="graf graf--p graf-after--p">Suppose that X1, …, Xn is a random sample, for example, an independent identically distributed sequence of random variables.</p><p name="fd1c" id="fd1c" class="graf graf--p graf-after--p">Call the common mean μ of the random variables μ = 𝔼[Xi].</p><p name="39da" id="39da" class="graf graf--p graf-after--p">Call the common <strong class="markup--strong markup--p-strong">finite</strong> variance by σ² = <em class="markup--em markup--p-em">Var</em>(Xi)&lt;∞.</p><p name="05b5" id="05b5" class="graf graf--p graf-after--p">For large <em class="markup--em markup--p-em">n</em> (i.e. as <em class="markup--em markup--p-em">n</em> tends to infinity), the following results approximately hold,</p><figure name="6b36" id="6b36" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rIFy8WTzQIV4AzaXpMEm8Q.png" data-width="1284" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*rIFy8WTzQIV4AzaXpMEm8Q.png"></figure><p name="9067" id="9067" class="graf graf--p graf-after--figure">or,</p><figure name="d263" id="d263" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FNKm3Sy-zX_n43vw0msYZg.png" data-width="1388" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*FNKm3Sy-zX_n43vw0msYZg.png"></figure><p name="c51a" id="c51a" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Empirical Rule of Symmetric Interval</strong></p><p name="f23e" id="f23e" class="graf graf--p graf-after--p">On a <strong class="markup--strong markup--p-strong">normal distribution</strong>, about 68% of data will be within one standard deviation of the mean, about 95% will be within two standard deviations of the mean, and about 99.7% will be within three standard deviations of the mean.</p><figure name="d55c" id="d55c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DCMPWkST1sKUOt9VAutvAw.png" data-width="1496" data-height="388" src="https://cdn-images-1.medium.com/max/800/1*DCMPWkST1sKUOt9VAutvAw.png"></figure><p name="07b0" id="07b0" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Central Limit Theorem: An Example</strong></p><p name="b954" id="b954" class="graf graf--p graf-after--p">Suppose that 88 people draw 88 Xi ~<em class="markup--em markup--p-em"> Exp</em>(2), and each of them independently draws exactly one Xi. So,</p><p name="fd08" id="fd08" class="graf graf--p graf-after--p">(a) What is the approximate distribution of ΣXi?</p><p name="d6ca" id="d6ca" class="graf graf--p graf-after--p">Ans: Based on the CLT, the sum of the random variables Xi ~ <em class="markup--em markup--p-em">Exp</em>(2), ΣXi, which is also a random variable, follows the normal distribution.</p><p name="d82d" id="d82d" class="graf graf--p graf-after--p">(b)What’s the mean, variance, and the standard deviation of ΣXi?</p><p name="3424" id="3424" class="graf graf--p graf-after--p">Ans: because we have,</p><figure name="bca0" id="bca0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FNKm3Sy-zX_n43vw0msYZg.png" data-width="1388" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*FNKm3Sy-zX_n43vw0msYZg.png"></figure><p name="b215" id="b215" class="graf graf--p graf-after--figure">So,</p><ul class="postList"><li name="35ba" id="35ba" class="graf graf--li graf-after--p">Mean = n/λ = 88/2 = 44.</li><li name="1521" id="1521" class="graf graf--li graf-after--li">Variance = 2n/λ²=88/4=22</li><li name="0ad2" id="0ad2" class="graf graf--li graf-after--li">Standard Deviation = sqrt(Variance) = sqrt(22)</li></ul><p name="0fa7" id="0fa7" class="graf graf--p graf-after--li">(c) Using the Empirical Rule, which symmetric interval would be expected to find ΣXi in 95% of the time?</p><p name="48f2" id="48f2" class="graf graf--p graf-after--p graf--trailing">Ans: Based on the Empirical Rule, approximately 95% of the time the random sum ΣXi will be in the interval [44–2sqrt(22), 44+2sqrt(22)].</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/c47fbb70df14"><time class="dt-published" datetime="2020-08-31T08:02:24.926Z">August 31, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/probability-and-statistics-6-maximum-likelihood-estimation-and-central-limit-theorem-c47fbb70df14" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>