<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Operating System 26 | Distributed Shared Memory, Designing DSM, and Consistency Models</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Operating System 26 | Distributed Shared Memory, Designing DSM, and Consistency Models</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Operating System
</section>
<section data-field="body" class="e-content">
<section name="b320" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="33ea" id="33ea" class="graf graf--h3 graf--leading graf--title">Operating System 26 | <strong class="markup--strong markup--h3-strong">Distributed Shared Memory, Designing DSM, and Consistency Models</strong></h3><figure name="359c" id="359c" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*TgfwQsJ1b0Yb8yVl.png" data-width="1508" data-height="794" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*TgfwQsJ1b0Yb8yVl.png"></figure><ol class="postList"><li name="2259" id="2259" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Introduction to Distributed Shared Memory (DSM)</strong></li></ol><p name="cacc" id="cacc" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Recall: the DFS States, Nodes, and Services</strong></p><p name="406b" id="406b" class="graf graf--p graf-after--p">In the previous lesson, we talked about DFS, and we said that the DFS was an example of the distributed file service where the <strong class="markup--strong markup--p-strong">states</strong> (aka. files) were stored on some <strong class="markup--strong markup--p-strong">nodes</strong> (aka. file servers), and these nodes were accessed by many distribute nodes’ clients. All the file access operations open, read, and write on these files where then the <strong class="markup--strong markup--p-strong">service</strong> was provided by these servers, and the clients will then request from them.</p><p name="1c9e" id="1c9e" class="graf graf--p graf-after--p">Our primary focus of the DFS was the <strong class="markup--strong markup--p-strong">caching</strong> mechanisms. These are useful so that server data is cached on the clients, which will improve the <strong class="markup--strong markup--p-strong">performance</strong> and <strong class="markup--strong markup--p-strong">scalability </strong>supported by the servers.</p><p name="f74c" id="f74c" class="graf graf--p graf-after--p">What we didn’t talk about is that when all the nodes in the system are both servers used to store files as well as clients used to send requests to other nodes in the system. In such a scenario, every single one of the nodes would be performing all of the services. The <strong class="markup--strong markup--p-strong">distributed shared memory</strong> (aka. <strong class="markup--strong markup--p-strong">DSM</strong>) we are going to talk about in this section is this kind of application without a clear distinction between client nodes and server nodes.</p><p name="0919" id="0919" class="graf graf--p graf-after--p">For instance, nowadays, most of the big data applications we heard about today distribute the states across all nodes. The processors on these nodes run certain applications such as the analytics codes, data mining codes, searching for patterns, or etc., and they access the state that’s distributed across all the other nodes in the system.</p><p name="8624" id="8624" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) The Definition of Peers</strong></p><p name="d79f" id="d79f" class="graf graf--p graf-after--p">The key about these DSM applications is that every node in the system owns some proportion of the state. This means that there is some state that’s locally stored in a particular node or that’s generated by computation that’s running on that node. So the overall application state is the <strong class="markup--strong markup--p-strong">union</strong> of all of these pieces that are present on every one node, and all the nodes in the system are called <strong class="markup--strong markup--p-strong">peers</strong>. All of the peers require,</p><ul class="postList"><li name="0774" id="0774" class="graf graf--li graf-after--p">to access to the state anywhere in the system</li><li name="be53" id="be53" class="graf graf--li graf-after--li">to provide access to their local storage state</li></ul><p name="1ffc" id="1ffc" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) The Definition of Distributed Shared Memory (DSM)</strong></p><p name="8dab" id="8dab" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">DSM</strong> is a service that manages the memory across multiple nodes so that applications that are running on top will have an illusion that they’re running on a shared memory machine like an SMP processor. Each node in the system will have the following features,</p><ul class="postList"><li name="8e5d" id="8e5d" class="graf graf--li graf-after--p">it owns some <strong class="markup--strong markup--li-strong">states</strong> (i.e. some portion of the physical memory)</li><li name="9ef6" id="9ef6" class="graf graf--li graf-after--li">it provides the <strong class="markup--strong markup--li-strong">operations</strong> in that state the reads and writes</li><li name="a7f3" id="a7f3" class="graf graf--li graf-after--li">it provides <strong class="markup--strong markup--li-strong">services</strong> like consistency protocols</li></ul><p name="0efa" id="0efa" class="graf graf--p graf-after--li">The main advantage of DSM machines is that we can benefit from <strong class="markup--strong markup--p-strong">scaling</strong> beyond the limitations of how much memory we can include in a single machine. If we look at how the cost of the computer system is affected by the amount of memory (e.g. when we have a high workload). We will see that beyond a certain limit of memory, the cost of scaling starts increasing rapidly. Instead, with DSM, we can simply add additional nodes and achieve shared memory at a much lower cost.</p><p name="47b2" id="47b2" class="graf graf--p graf-after--p">However, the speed of accessing the memory will be much <strong class="markup--strong markup--p-strong">slower</strong> but these additional accessing latency can be hidden by some optimizations. DSM is becoming more relevant today is that <strong class="markup--strong markup--p-strong">commodity-interconnect technologies</strong> offer really low latency among nodes in the system. What’s more, they offer <strong class="markup--strong markup--p-strong">remote direct memory access</strong> (aka. <strong class="markup--strong markup--p-strong">RDMA</strong>) based interfaces that provide a really low latency when accessing the remote memories.</p><p name="b8a1" id="b8a1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Hardware Supported DSM</strong></p><p name="7aae" id="7aae" class="graf graf--p graf-after--p">One type of the DSM is supported by the hardware, and this type is called the <strong class="markup--strong markup--p-strong">hardware supported DSM</strong>. The key component of the hardware supported distributed shared memories is that they rely on some <strong class="markup--strong markup--p-strong">high-end interconnects</strong>. In this model, the OS will manage the larger physical memory and it is allowed to establish V2P memory mappings that correspond to memory locations that are in other nodes.</p><p name="b0c5" id="b0c5" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">network interface card</strong> (<strong class="markup--strong markup--p-strong">NIC</strong>) is the hardware that corresponds to this advanced high-end interconnect technology. The local NIC knows how to translate that memory operation into an actual message that gets sent to the correct remote NICs and the NICs in these nodes will participate in all aspects that are necessary to deal with memory accesses and management of memory consistency. We will also support certain atomic operations just like the atomics that we saw in shared memory systems.</p><figure name="bf29" id="bf29" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*K1F-1ZJwI7Mmx-Z6S31wfg.png" data-width="1360" data-height="308" src="https://cdn-images-1.medium.com/max/800/1*K1F-1ZJwI7Mmx-Z6S31wfg.png"></figure><p name="e77a" id="e77a" class="graf graf--p graf-after--figure">However, nowadays, it is very convenient to rely on the hardware to do everything by NIC, it is typically very expensive and only high-end machines can support this hardware implementation.</p><p name="e47b" id="e47b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Software Supported DSM</strong></p><p name="9409" id="9409" class="graf graf--p graf-after--p">Instead, DSM systems can also be realized in <strong class="markup--strong markup--p-strong">software</strong>. The software has to,</p><ul class="postList"><li name="0786" id="0786" class="graf graf--li graf-after--p">detect which memory accesses are local or remote</li><li name="02ee" id="02ee" class="graf graf--li graf-after--li">create and send those messages to the appropriate node when it is necessary</li><li name="0bd8" id="0bd8" class="graf graf--li graf-after--li">accept messages from other nodes</li><li name="e6be" id="e6be" class="graf graf--li graf-after--li">perform the specific memory operations</li><li name="d257" id="d257" class="graf graf--li graf-after--li">be involved in all of the aspects of memory sharing and consistency support</li></ul><p name="2113" id="2113" class="graf graf--p graf-after--li">This can be done at the level of the OS, or it can be done with the support of a programming language and the runtime product programming language.</p><p name="33e6" id="33e6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Designing DSM</strong></p><p name="f2a8" id="f2a8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of Granularity</strong></p><p name="6970" id="6970" class="graf graf--p graf-after--p">In computer science, <strong class="markup--strong markup--p-strong">granularity</strong> is defined as the ratio of computation time to communication time. The <strong class="markup--strong markup--p-strong">computation time</strong> is the time required to perform the computation of a task and <strong class="markup--strong markup--p-strong">communication time</strong> is the time required to exchange data between processors. When we want to design an efficient computer system, we tend to support more time for computation and less time for communication. Therefore, we will have a better performance if the granularity of the system is higher.</p><p name="4be5" id="4be5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Granularity of Sharing</strong></p><p name="60b9" id="60b9" class="graf graf--p graf-after--p">Now, we are going to have a very different definition of <strong class="markup--strong markup--p-strong">granularity</strong>, which is defined as the <strong class="markup--strong markup--p-strong">smallest scale</strong> (or unit) we need to have for memory sharing among different peers. We have to keep in mind this definition we are going to use in the following discussions is quite different when we talk about the granularity ratio.</p><p name="2252" id="2252" class="graf graf--p graf-after--p">To design a DSM system, the first thing we have to think about is the granularity of sharing. In SMP systems, the granularity of sharing is a <strong class="markup--strong markup--p-strong">cache line</strong>. However, we can not modify this model if the cache line has been shared with other peers because if all the writes will be sent by the network, it will become too expensive and the overheads will become too high. Therefore, we have some other options. The granularity of sharing of DSM may be,</p><ul class="postList"><li name="6028" id="6028" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Variables</strong>: This is not a good idea because the variables can be a few bytes long (e.g. integers) and those settings will still result in very high overheads.</li><li name="dd90" id="dd90" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Pages or Objects</strong>: Instead, we can use something larger like an entire page of content or a larger object that begins to make more sense. At the operating system level, it makes sense to integrate some page-based DSM solutions because the OS can understand and track all the necessary messages on page modifications. With some help from the compiler or programming language or runtimes, applications objects can lay out on different pages and the runtime will understand which objects are local and which are remote.</li></ul><p name="c56c" id="c56c" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) False Sharing Problem</strong></p><p name="cdb4" id="cdb4" class="graf graf--p graf-after--p">One important problem of page or object granularity is <strong class="markup--strong markup--p-strong">false sharing</strong>. This means that the data on the <strong class="markup--strong markup--p-strong">same page or object</strong> will be shared even if it is not modified and unnecessary. So this will trigger unnecessary coherence operations or any of the other overheads that are associated with maintaining consistency among these two copies, which will not benefit anyone.</p><p name="57e2" id="57e2" class="graf graf--p graf-after--p">In order to avoid these kinds of situations, the programmer must be <strong class="markup--strong markup--p-strong">careful</strong> about how data is allocated and laid out on pages or how it’s grouped in higher-level objects.</p><p name="654c" id="654c" class="graf graf--p graf-after--p">The other alternative solution is to rely on some <strong class="markup--strong markup--p-strong">smart compiler</strong> that will be able to understand what is really a shared state. And then allocate it within a page or within an object versus what is something that will trigger these false sharing situations.</p><p name="fa5c" id="fa5c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Access Algorithms</strong></p><p name="943f" id="943f" class="graf graf--p graf-after--p">Another important design point in DSM systems is to understand what are the types of <strong class="markup--strong markup--p-strong">access algorithms</strong>. The access algorithms are the kinds of applications that will be running on top of the DSM layer, which includes</p><ul class="postList"><li name="2de3" id="2de3" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Single reader, single writer (SRSW)</strong>: the simplest one, and it aims to provide additional memory. In this case, there really aren’t any consistency or sharing related challenges</li><li name="cf84" id="cf84" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Multiple readers, single writer (MRSW)</strong></li><li name="d246" id="d246" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Multiple readers, multiple writers (MRMW)</strong></li></ul><p name="d832" id="d832" class="graf graf--p graf-after--li">In the last two cases (i.e. MRSW and MRMW), it’s not just about how to read or write to the correct physical memory location in the distributed system, but it’s also about how to,</p><ul class="postList"><li name="5990" id="5990" class="graf graf--li graf-after--p">make sure that the reads return the correctly written the <strong class="markup--strong markup--li-strong">most recently</strong> written value of a particular memory location</li><li name="b8e1" id="b8e1" class="graf graf--li graf-after--li">make sure all of the writes that are performed are <strong class="markup--strong markup--li-strong">correctly ordered</strong></li></ul><p name="19c8" id="19c8" class="graf graf--p graf-after--li">This is also necessary to present the consistent view of the distributed state of the DSM to all of the nodes in the system. MRSW is a special simpler case of the MRMW problem.</p><p name="079a" id="079a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) DSM Performance Metrics</strong></p><p name="32c0" id="32c0" class="graf graf--p graf-after--p">If we think about the core service that’s provided by DSM systems is accessing different memory locations, then it’s obvious that the performance metric that’s relevant for DSM systems is the <strong class="markup--strong markup--p-strong">latency</strong> with which processes running on any one of these nodes can perform such remote memory accesses. It is clear that accessing local memory is faster than remote memory, but we can also have some options to improve the number of cases where local memory’s accessed versus remote.</p><p name="03f6" id="03f6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Migration: Good for SRSW</strong></p><p name="0396" id="0396" class="graf graf--p graf-after--p">One way to maximize the number of local accesses and achieve low latency is to use a technique called <strong class="markup--strong markup--p-strong">migration</strong>. Whenever a process on another node needs to access a remote state, we literally move that state over to that node. This makes sense for SRSW because only one node at a time is able to access the state. However, this requires moving and copying the data over to the remote node, and that incurs some <strong class="markup--strong markup--p-strong">overheads</strong>. If we have to copy all the state data once and once again, we will not benefit from a higher latency even for an SRSW model.</p><figure name="6ca7" id="6ca7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Xcd7Q8gCFih3YvGf23v4RQ.png" data-width="1360" data-height="556" src="https://cdn-images-1.medium.com/max/800/1*Xcd7Q8gCFih3YvGf23v4RQ.png"></figure><p name="62da" id="62da" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(7) Replication: Good for MRMW and MRSW</strong></p><p name="ab77" id="ab77" class="graf graf--p graf-after--p">For the more general case like MRMW, we have to do replications and the state is copied on multiple nodes potentially on all nodes is a more general mechanism.</p><figure name="ae66" id="ae66" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*TyybnD3cNizl9Q1uB9BF8w.png" data-width="1360" data-height="556" src="https://cdn-images-1.medium.com/max/800/1*TyybnD3cNizl9Q1uB9BF8w.png"></figure><p name="dbca" id="dbca" class="graf graf--p graf-after--figure">However, now it requires <strong class="markup--strong markup--p-strong">consistency management</strong> because the same state can be accessed concurrently on multiple nodes, this will add some sort of overheads which will be simply bad for the overall latency. One way to control the overhead is to <strong class="markup--strong markup--p-strong">limit the number of replicates</strong> that can exist in this system since consistency management has overhead that’s proportional with the number of copies that need to be maintained consistently.</p><p name="450f" id="450f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Consistency Management</strong></p><p name="c2f3" id="c2f3" class="graf graf--p graf-after--p">In an SMP system, we also have to maintain consistency among different caches with write-invalidate and write-update. However, these coherence operations are triggered on each write, which will add more overheads in the case of a DSM system. For these reasons, we will look at coherence operations that are more similar to what we have discussed in the DFS.</p><p name="1518" id="1518" class="graf graf--p graf-after--p">One option is to have <strong class="markup--strong markup--p-strong">push-invalidations</strong> when the data item is written to. This is similar to the server-based approach, but the DSM system state management should be done by all peers instead of just the servers. The features of this option are</p><ul class="postList"><li name="2ca4" id="2ca4" class="graf graf--li graf-after--p">proactive</li><li name="f510" id="f510" class="graf graf--li graf-after--li">eager</li><li name="bc7f" id="bc7f" class="graf graf--li graf-after--li">pessimistic</li></ul><p name="3e5d" id="3e5d" class="graf graf--p graf-after--li">The other option is to use a pull modification periodically <strong class="markup--strong markup--p-strong">on-demand</strong> whenever some process needs to access that state locally. The features of this option are</p><ul class="postList"><li name="b0c3" id="b0c3" class="graf graf--li graf-after--p">on-demand or reactive</li><li name="eb89" id="eb89" class="graf graf--li graf-after--li">lazy</li><li name="15bd" id="15bd" class="graf graf--li graf-after--li">optimistic</li></ul><p name="b874" id="b874" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(9) Consistency Architecture</strong></p><p name="8d30" id="8d30" class="graf graf--p graf-after--p">In summary, here are some of the key architectural features of page-based DSM systems.</p><ul class="postList"><li name="0274" id="0274" class="graf graf--li graf-after--p">each node contributes to <strong class="markup--strong markup--li-strong">part of</strong> its main memory pages to DSM</li><li name="6e4a" id="6e4a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">local caches</strong> are maintained to improve the performance by minimizing the latency</li><li name="a659" id="a659" class="graf graf--li graf-after--li">all nodes are responsible for some part of the distributed memory</li><li name="6d49" id="6d49" class="graf graf--li graf-after--li">the home node is responsible for managing accesses and tracking pages’ ownerships</li><li name="20ca" id="20ca" class="graf graf--li graf-after--li">some explicate replications can be considered to assist the load balancing, performance, and reliability</li></ul><p name="7fac" id="7fac" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(10) Indexing Distributed State</strong></p><p name="42a6" id="42a6" class="graf graf--p graf-after--p">One important aspect of distributed shared memory systems is how do we determine where a particular page is. In order to do this, the DSM component has to maintain some <strong class="markup--strong markup--p-strong">metadata</strong>. The page address can be divided into a <strong class="markup--strong markup--p-strong">node identifier</strong> plus the <strong class="markup--strong markup--p-strong">page frame number</strong> and the node identifier also implies the home node.</p><p name="37ff" id="37ff" class="graf graf--p graf-after--p">Considering the address format, the address will directly identify the manager node’s ID (i.e. home node ID), and a <strong class="markup--strong markup--p-strong">global map</strong> can be used by all the nodes on how to find its manager. The manager node will have all the per-page metadata that’s necessary for performing,</p><ul class="postList"><li name="e51c" id="e51c" class="graf graf--li graf-after--p">specific accesses to that page</li><li name="0eac" id="0eac" class="graf graf--li graf-after--li">the consistency of the page</li></ul><p name="364f" id="364f" class="graf graf--p graf-after--li">What’s nice about this approach is that there is no need to consider making any kind of changes to the object identifier and the object can remain identified in the exact same way as it was before we chose to make a change into the manager nodes.</p><p name="6870" id="6870" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(11) DSM Implementing Problem</strong></p><p name="6fe8" id="6fe8" class="graf graf--p graf-after--p">Based on the previous discussions, the DSM must intercept the access to any of the DSM states,</p><ul class="postList"><li name="dc6c" id="dc6c" class="graf graf--li graf-after--p">to send remote messages for requesting accesses</li><li name="7269" id="7269" class="graf graf--li graf-after--li">to trigger coherence messages</li></ul><p name="105f" id="105f" class="graf graf--p graf-after--li">However, when we are accessing a local non-shared state, these operations should be unnecessary and should be avoided. So what we would like to achieve is an implementation where it is possible to<strong class="markup--strong markup--p-strong"> dynamically</strong> <strong class="markup--strong markup--p-strong">engage</strong> whether the distributed shared memory layer will be triggered and will be intercepting any accesses to memory in order to determine what to do about them, or <strong class="markup--strong markup--p-strong">disengage</strong> the distributed shared memory layer if we are performing access to pages which are really not shared and are just local pages accessed on a particular node.</p><p name="5a5d" id="5a5d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(12) DSM Implementing Solution</strong></p><p name="4dfb" id="4dfb" class="graf graf--p graf-after--p">To support this implementation, the DSM benefits from the supports from <strong class="markup--strong markup--p-strong">hardware MMU</strong>. If the state is not found locally or we haven’t got the permission of a state, the current operation will be trapped in the OS and then the OS will detect the cause of that trap and it will then pass that page information to the DSM layer so that the DSM layer can send the messages. Because the DSM has to maintain the coherence of the states, the MMU flags like the dirty bit can be useful for tracking the coherence information.</p><p name="bde7" id="bde7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Consistency Models</strong></p><p name="2e9a" id="2e9a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Importance of the Consistency Model</strong></p><p name="0ee2" id="0ee2" class="graf graf--p graf-after--p">In fact, the exact details of how a DSM system should be designed, or how the coherence mechanisms will be triggered depends on the <strong class="markup--strong markup--p-strong">consistency model</strong>. Let’s first understand what are the consistency models.</p><p name="02fe" id="02fe" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Definition of the Consistency Models</strong></p><p name="0610" id="0610" class="graf graf--p graf-after--p">Consistency models exist in the context of the implementations of applications or services that manage the distributed states, which is a <strong class="markup--strong markup--p-strong">guarantee</strong> that the state changes will behave in a certain way as long as the upper software layers follow a certain set of rules and the memory behaves correctly if and only if software follows specific rules. This means that it will guarantees,</p><ul class="postList"><li name="8186" id="8186" class="graf graf--li graf-after--p">how our memory accesses are going to be ordered</li><li name="9326" id="9326" class="graf graf--li graf-after--li">propagation and visibility of updates</li></ul><p name="8ada" id="8ada" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Strict Consistency</strong></p><p name="a849" id="a849" class="graf graf--p graf-after--p">Strict consistency is a nice theoretical model but it is not possible to guarantee in both the DSM system and the SMP system unless we use some complex locks and synchronizations that will definitely add to the overheads. The strict consistency means that every single update has to be <strong class="markup--strong markup--p-strong">immediately visible</strong> and <strong class="markup--strong markup--p-strong">everywhere visible</strong> and the <strong class="markup--strong markup--p-strong">ordering</strong> of these updates needs to be preserved.</p><p name="f998" id="f998" class="graf graf--p graf-after--p">If we first write x to m1 and then y is written to m2, we should make sure that the reads in P3 should only happen after the writes in P1 and P2 finish. So the reads from memory m1 can only have x, and the read from m2 can only have y.</p><figure name="4cc2" id="4cc2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5-patmF4glREnGNRdmcoew.png" data-width="1360" data-height="284" src="https://cdn-images-1.medium.com/max/800/1*5-patmF4glREnGNRdmcoew.png"></figure><p name="bf22" id="bf22" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Sequential Consistency</strong></p><p name="a219" id="a219" class="graf graf--p graf-after--p">Given that strict consistency is next to impossible to achieve, the next best option with a reasonable cost is <strong class="markup--strong markup--p-strong">sequential consistency</strong>. According to sequential consistency, the memory updates from different processors may be arbitrarily interleaved. However, if we let one process see one ordering of these updates, we have to make sure that all other processes see the <strong class="markup--strong markup--p-strong">exact same ordering</strong> of those updates. For example, the following situations are possible,</p><figure name="fd1d" id="fd1d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HS09MdV-eNDkWLzXJkuVJg.png" data-width="1360" data-height="566" src="https://cdn-images-1.medium.com/max/800/1*HS09MdV-eNDkWLzXJkuVJg.png"></figure><p name="0b4d" id="0b4d" class="graf graf--p graf-after--figure">But the following case is not possible,</p><figure name="aecc" id="aecc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Lu_gmKOthqP3aDCz8Pg9zQ.png" data-width="1360" data-height="360" src="https://cdn-images-1.medium.com/max/800/1*Lu_gmKOthqP3aDCz8Pg9zQ.png"></figure><p name="a11b" id="a11b" class="graf graf--p graf-after--figure">This is because processes P3 and P4 see the same memory in different updating orders.</p><p name="35b9" id="35b9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Causal Consistency</strong></p><p name="689f" id="689f" class="graf graf--p graf-after--p">Causal consistency models guarantee that they will detect the possible<strong class="markup--strong markup--p-strong"> causal relationship</strong> or <strong class="markup--strong markup--p-strong">dependencies</strong> between updates. And if updates are causally related then the memory will guarantee that those rights those update operations will be correctly ordered. For example, in the following example, P4 can not be possible because P2 will first read m1 as x and then set the value y to m2. If we read y from m2, then m1 must be set to x already. This is an example that different read-and-write operations have some causal relationships.</p><figure name="e1a4" id="e1a4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*h_5g3-l5goqSDt6Bn6h8jA.png" data-width="1360" data-height="364" src="https://cdn-images-1.medium.com/max/800/1*h_5g3-l5goqSDt6Bn6h8jA.png"></figure><p name="f191" id="f191" class="graf graf--p graf-after--figure">However, the following case is okay for a causal consistency because there’s actually no causal relationships between operations and for writes that are not causally related or they’re referred to in causal consistency as concurrent writes there are no guarantees.</p><figure name="6725" id="6725" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mC01BD-QJKouIn4vDtkBFg.png" data-width="1360" data-height="364" src="https://cdn-images-1.medium.com/max/800/1*mC01BD-QJKouIn4vDtkBFg.png"></figure><p name="def7" id="def7" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(6) Weak Consistency</strong></p><p name="379f" id="379f" class="graf graf--p graf-after--p">In the weak consistency models, it’s possible to have something beyond just read and write operations when memory’s accessed. However, the dependencies of the operations should be maintained and we have to introduce a mechanism called <strong class="markup--strong markup--p-strong">synchronization points</strong>. This sync point acts like a boundary and all the operations after this point should have to make sure that all the variables in the system for the entire shared memory should be visible. It makes sure that all of the updates that have happened prior to sync the synchronization point will become visible on other processors. And also synchronization point makes sure that all of the updates that have happened on other processors will become visible subsequently at this particular processor in the future.</p><p name="319a" id="319a" class="graf graf--p graf-after--p graf--trailing">The idea benefits from <strong class="markup--strong markup--p-strong">limiting the required data movement</strong> to the required messages and coherence separations that will be exchanged among the nodes in the system. But the downside of this is that the distributed shared memory layer will have to <strong class="markup--strong markup--p-strong">maintain some additional state</strong> to keep track of exactly what are all the different operations that it needs to support and how it needs to behave when it sees a particular type of request.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/5aaf91b06fc5"><time class="dt-published" datetime="2021-04-27T21:25:57.573Z">April 27, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/operating-system-26-distributed-shared-memory-designing-dsm-and-consistency-models-5aaf91b06fc5" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>