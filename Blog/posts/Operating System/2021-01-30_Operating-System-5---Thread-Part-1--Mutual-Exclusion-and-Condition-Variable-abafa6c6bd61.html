<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Operating System 5 | Thread Part 1, Mutual Exclusion and Condition Variable</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Operating System 5 | Thread Part 1, Mutual Exclusion and Condition Variable</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Operating System
</section>
<section data-field="body" class="e-content">
<section name="61ec" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="15aa" id="15aa" class="graf graf--h3 graf--leading graf--title">Operating System 5 | Thread Part 1, <strong class="markup--strong markup--h3-strong">Mutual Exclusion and Condition Variable</strong></h3><figure name="c48f" id="c48f" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*Rv9MeHC2TLpRh54k.png" data-width="1508" data-height="794" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*Rv9MeHC2TLpRh54k.png"></figure><ol class="postList"><li name="a53b" id="a53b" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Threads</strong></li></ol><p name="4a48" id="4a48" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Recall: Process</strong></p><p name="d2c6" id="d2c6" class="graf graf--p graf-after--p">In the previous section, we have talked about processes and process management and we have known that a process is represented with its <strong class="markup--strong markup--p-strong">address space</strong> and its <strong class="markup--strong markup--p-strong">execution context</strong>. The <strong class="markup--strong markup--p-strong">address space</strong> is defined by a range of virtual addresses and different types of process states. The <strong class="markup--strong markup--p-strong">execution contexts</strong> are also stored in the <strong class="markup--strong markup--p-strong">process control block</strong> (PCB) for all the state information (i.e. program counter or stack pointer) of a process.</p><p name="6c4c" id="6c4c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Process For Multi-core Systems</strong></p><p name="f826" id="f826" class="graf graf--p graf-after--p">However, the processes that are represented in this way can only be executed at one CPU (one core) at a given time. If we want a process to be able to execute on multiple CPUs (modern CPUs have multi-core systems), that process has to have multiple execution contexts. Such execution contexts within a single process are so-called <strong class="markup--strong markup--p-strong">threads</strong>.</p><p name="3d4b" id="3d4b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Properties of Threads</strong></p><ul class="postList"><li name="d31f" id="d31f" class="graf graf--li graf-after--p">Threads are <strong class="markup--strong markup--li-strong">active entities</strong> because each thread executing a unit of work on behalf of a process.</li><li name="45a5" id="45a5" class="graf graf--li graf-after--li">Threads can work <strong class="markup--strong markup--li-strong">simultaneously</strong> because we can execute many threads at the same time. Thus, in this case, we have to think about <strong class="markup--strong markup--li-strong">concurrency</strong>.</li><li name="0370" id="0370" class="graf graf--li graf-after--li">Threads require some level of <strong class="markup--strong markup--li-strong">coordination</strong>. For coordination, we are mainly talking about coordinating access to the underlying platform resources like sharing I/O devices, CPU cores, memory, and all other system resources.</li></ul><p name="2dcf" id="2dcf" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Process Vs. Thread</strong></p><p name="9aa6" id="9aa6" class="graf graf--p graf-after--p">A thread is the unit of execution within a process. A process can have anywhere from just one thread to many threads. When a process starts, it is assigned memory and resources. Each thread in the process shares that memory and resources.</p><p name="1497" id="1497" class="graf graf--p graf-after--p">A single process is represented by its <strong class="markup--strong markup--p-strong">address space</strong>, and the address space will contain all the mappings of <strong class="markup--strong markup--p-strong">virtual to physical addresses</strong> for its <strong class="markup--strong markup--p-strong">code</strong>, its <strong class="markup--strong markup--p-strong">data</strong>, and its <strong class="markup--strong markup--p-strong">files</strong>. The process can also be represented by its <strong class="markup--strong markup--p-strong">execution context</strong> that contains the information about the values of the <strong class="markup--strong markup--p-strong">registers</strong>, the <strong class="markup--strong markup--p-strong">stack pointer</strong>, and etc. The operating system represents all this information in a <strong class="markup--strong markup--p-strong">process control block</strong> (PCB).</p><figure name="efb6" id="efb6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*7PxDdfmTrfWEToRQdxugPw.png" data-width="1562" data-height="704" src="https://cdn-images-1.medium.com/max/800/1*7PxDdfmTrfWEToRQdxugPw.png"></figure><p name="c016" id="c016" class="graf graf--p graf-after--figure">Threads represent multiple, independent execution contexts. They are part of the <strong class="markup--strong markup--p-strong">same virtual address space</strong>, which means that they will share all of the virtual addresses of a process. They will share all the codes, all the data, and all the files. However, they will also execute different instructions, access different portions of that address space, operate different portions of the input, and differ in other ways. So each thread will have a different program counter, stack pointer, stack, and thread-specific registers. The PCB that we use to store the information of the thread of a process is more complex because it has to contain all the information among all the threads with the list of <strong class="markup--strong markup--p-strong">thread control blocks </strong>(<strong class="markup--strong markup--p-strong">TCB</strong>s).</p><figure name="5898" id="5898" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MDePboFwOllMHuWe4CwmKw.png" data-width="1976" data-height="734" src="https://cdn-images-1.medium.com/max/800/1*MDePboFwOllMHuWe4CwmKw.png"></figure><p name="98e0" id="98e0" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Advantages of Using Threads</strong></p><ul class="postList"><li name="009f" id="009f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Parallelization for Speedup</strong></li></ul><p name="c4ac" id="c4ac" class="graf graf--p graf-after--li">Suppose we have a 4-core CPU, and we also have a process that has 4 threads executing on this CPU. If these 4 threads have the same code but each of them has a different input, there can then be variations between the program and register contents for each of these threads.</p><figure name="b399" id="b399" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Sr1sU01NyQxKU7vUumBS0A.png" data-width="1628" data-height="482" src="https://cdn-images-1.medium.com/max/800/1*Sr1sU01NyQxKU7vUumBS0A.png"></figure><p name="9241" id="9241" class="graf graf--p graf-after--figure">Compared with the situation that we only have 1 process when we can only run this process with the 4 inputs on a core one after one, we can run these 4 inputs simultaneously on the 4 cores. This is called parallelization can we can reduce our execution time of a program and finally achieves a speedup of performance.</p><ul class="postList"><li name="a220" id="a220" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Specialization for Hot Cache</strong></li></ul><p name="0030" id="0030" class="graf graf--p graf-after--li">The operations executed by each thread on the CPU comes from the fact that the performance is dependent on how many states exist in the processor cache. If each of these threads running on a different processor (core) has access to its own processor cache, then the cache will be <strong class="markup--strong markup--p-strong">hot</strong> and this will improve the performance by reducing the W/R operations from the memory.</p><ul class="postList"><li name="c08a" id="c08a" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Memory Efficiency</strong></li></ul><p name="4623" id="4623" class="graf graf--p graf-after--li">Some may have a question here. Why don’t we have 4 processes instead of 4 threads? The answer can be quite clear. If we have 4 threads of the same process, they can share the virtual address space (code, data, files). However, if we have 4 processes, each of them should have a virtual address space, which can be memory-consuming.</p><ul class="postList"><li name="b0ec" id="b0ec" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Improve Cross-Threads Communication</strong></li></ul><p name="c3a7" id="c3a7" class="graf graf--p graf-after--li">Passing data or synchronization among different processes requires interprocess communication mechanisms that are costly. However, because all the threads share the same virtual address space, then it will not be as costly as inter-process communication.</p><p name="c203" id="c203" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Context Switch for Threads</strong></p><p name="1911" id="1911" class="graf graf--p graf-after--p">Now, let’s think about the context switch for threads. We have talked that the context switch happens when we want to move from one process to another. The current CPU state will be stored in the CPU and then we have to reload the data from the memory to cache. The process of changing from the cold cache to the hot cache can be quite time-consuming and it reduces the general performance.</p><p name="dd7f" id="dd7f" class="graf graf--p graf-after--p">For the context switch between threads of the same process, because these threads share the same virtual space, we don’t need to redo the mapping page from the virtual address space to the physical address space. So the context switch between threads can be much faster than the context switch between processes.</p><p name="7f51" id="7f51" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Threads for a Single Processor</strong></p><p name="6b79" id="6b79" class="graf graf--p graf-after--p">Suppose we have only one processor, could we improve the general performance of the CPU by threads? The answer is <strong class="markup--strong markup--p-strong">yes</strong>. In reality, the processors are not running all the time because sometimes we have to wait for the disks or some other devices. The time that the CPU is not working because it is waiting for some other devices is called <strong class="markup--strong markup--p-strong">idle time</strong>.</p><p name="90f7" id="90f7" class="graf graf--p graf-after--p">If we can have an idle time longer than two times the context switch time from one thread to the other (because we have to switch from one thread and then switch back), then we can improve the performance by using multiple threads on one processor.</p><p name="cfd5" id="cfd5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Basic Thread Mechanisms</strong></p><p name="5546" id="5546" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Goals for Supporting Threads</strong></p><ul class="postList"><li name="2ea1" id="2ea1" class="graf graf--li graf-after--p">Design thread <strong class="markup--strong markup--li-strong">data structure</strong></li><li name="e3ed" id="e3ed" class="graf graf--li graf-after--li">Design mechanisms to <strong class="markup--strong markup--li-strong">create</strong> and <strong class="markup--strong markup--li-strong">manage</strong> threads</li><li name="8463" id="8463" class="graf graf--li graf-after--li">Design mechanisms to <strong class="markup--strong markup--li-strong">coordinate</strong> and run <strong class="markup--strong markup--li-strong">concurrently</strong></li></ul><p name="59c0" id="59c0" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Process Concurrency</strong></p><p name="e173" id="e173" class="graf graf--p graf-after--p">Is it possible for two processes to access the same physical address? The answer is <strong class="markup--strong markup--p-strong">no</strong>. We have known that different processes have independent virtual spaces mapping to different physical address spaces. Because the operating system controls the mapping and the virtual addresses of different processes do not allow to map to the same physical address. Thus, it is not possible for a process to access the physical location of another process.</p><p name="0159" id="0159" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Thread Concurrency</strong></p><p name="e898" id="e898" class="graf graf--p graf-after--p">Is it possible for two threads to access the same physical address? The answer is <strong class="markup--strong markup--p-strong">yes</strong> because we have known that different threads share the same virtual address space. So both of the threads of a process can concurrently be running as part of their address space and they can legally perform access to the same physical memory. However, if these threads can access a physical address at the same time, there will be some <strong class="markup--strong markup--p-strong">inconsistency problems (i.e. see the </strong><a href="https://en.wikipedia.org/wiki/Race_condition" data-href="https://en.wikipedia.org/wiki/Race_condition" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">data race</strong></a><strong class="markup--strong markup--p-strong"> problem)</strong>.</p><p name="a5d3" id="a5d3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Synchronization Mechanisms</strong></p><p name="af8c" id="af8c" class="graf graf--p graf-after--p">There are two kinds of synchronization mechanisms that serve different coordination purpose,</p><ul class="postList"><li name="ac79" id="ac79" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Concurrency Control Mechanism: Mutex</strong></li></ul><p name="5a72" id="5a72" class="graf graf--p graf-after--li">To deal with the concurrency problems of the threads, we need a mechanism for threads to execute in an exclusive manner, and this mechanism is called <strong class="markup--strong markup--p-strong">mutual exclusion (aka. Mutex)</strong>. Mutual exclusion is a mechanism that only one thread at a time is allowed to perform an operation. So if two threads want to make the same operation, one of them must wait for its turn.</p><figure name="eaf2" id="eaf2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ff3fVaw6dHQeluocBVgAqQ.png" data-width="1684" data-height="734" src="https://cdn-images-1.medium.com/max/800/1*Ff3fVaw6dHQeluocBVgAqQ.png"></figure><ul class="postList"><li name="3eef" id="3eef" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Coordination Mechanism: Conditional Variable</strong></li></ul><p name="ca59" id="ca59" class="graf graf--p graf-after--li">In addition, it is also useful for threads to have a mechanism to wait for one and another. Suppose we have a thread working, and we don’t want the other threads that wait for the result of this thread to repeatedly check the status of this thread because this can reduce the performance. The threads that are not working could send a specific condition at the beginning like “I will sleep until you wake me up.” and then when the working thread finishes (or meet some conditions), it sends another condition like “Wake up, I am finished.” before the proceeding. This mechanism is called a <strong class="markup--strong markup--p-strong">conditional variable</strong>.</p><figure name="df54" id="df54" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*k7b5JLjpR9AMaFaRZOS4AQ.png" data-width="1684" data-height="770" src="https://cdn-images-1.medium.com/max/800/1*k7b5JLjpR9AMaFaRZOS4AQ.png"></figure><p name="0c8b" id="0c8b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">3. Mutual Exclusion</strong></p><p name="9614" id="9614" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Thread Creation</strong></p><p name="1cf8" id="1cf8" class="graf graf--p graf-after--p">The following content is based on the <a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-birrell-paper.pdf" data-href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-birrell-paper.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">research paper</a> of <a href="https://cacm.acm.org/news/210689-in-memoriam-andrew-birrell-1951-2016/fulltext" data-href="https://cacm.acm.org/news/210689-in-memoriam-andrew-birrell-1951-2016/fulltext" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Andrew D. Birrell</a>, who is a computer scientist who devoted his life to operating and distributed systems. This is a fundamental level introduction of the theory of the threads.</p><p name="2611" id="2611" class="graf graf--p graf-after--p">In Birrell’s paper, we use a data structure called <strong class="markup--strong markup--p-strong">thread type</strong> to represent a thread and this data structure contains all information that specific to a thread including the thread ID, the register values, the program counter (PC), the stack pointer (SP), the stack, and some other attributes of that thread (for scheduling, debugging, etc.).</p><p name="fa7d" id="fa7d" class="graf graf--p graf-after--p">Birrell used a <code class="markup--code markup--p-code">fork</code> call to create a thread. We should note that this is different from the <code class="markup--code markup--p-code">fork</code> UNIX system call (for creating a copy of the current process and it can be used with the system call <code class="markup--code markup--p-code">exec</code>) that we have talked about in the previous sections. The synopsis of the <code class="markup--code markup--p-code">fork</code> should have two parameters, <code class="markup--code markup--p-code">proc</code> and <code class="markup--code markup--p-code">args</code> ,</p><pre name="615b" id="615b" class="graf graf--pre graf-after--p">t1 = fork(proc, args)</pre><p name="058d" id="058d" class="graf graf--p graf-after--pre">the <code class="markup--code markup--p-code">proc</code> parameter is the program counter that the new thread is going to start by and the <code class="markup--code markup--p-code">args</code> will be available on the stack of this newly created thread. <code class="markup--code markup--p-code">fork</code> also returns to its caller a handle (in this case, variable <code class="markup--code markup--p-code">t1</code>) on the newly created thread.</p><pre name="1346" id="1346" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">t1 Data Structure</strong><br>PC = proc<br>stack = args</pre><p name="df21" id="df21" class="graf graf--p graf-after--pre">After this operation completes, the process as a whole now has two threads, the original one <code class="markup--code markup--p-code">t0</code> (also called the parent thread) and the newly created one <code class="markup--code markup--p-code">t1</code>. Suppose we have a multiple-core system, then <code class="markup--code markup--p-code">t0</code> and <code class="markup--code markup--p-code">t1</code> can execute concurrently. Then, <code class="markup--code markup--p-code">t0</code> is going to continue the execution of the current code and <code class="markup--code markup--p-code">t1</code> will start to execute the code from <code class="markup--code markup--p-code">proc</code>.</p><p name="bb46" id="bb46" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Thread Termination</strong></p><p name="ba82" id="ba82" class="graf graf--p graf-after--p">Once <code class="markup--code markup--p-code">t1</code> is finished, <code class="markup--code markup--p-code">t1</code> can either return the computed results or it can have some status of the computation, like <code class="markup--code markup--p-code">success</code> or <code class="markup--code markup--p-code">error</code>. When <code class="markup--code markup--p-code">t1</code> finishes, some other threads (like <code class="markup--code markup--p-code">t0</code>) may want to get the returned result of <code class="markup--code markup--p-code">t1</code>. If we call <code class="markup--code markup--p-code">join</code> in the thread <code class="markup--code markup--p-code">t0</code>, then <code class="markup--code markup--p-code">t0</code> will be blocked until it gets the result of <code class="markup--code markup--p-code">t1</code>.</p><pre name="4615" id="4615" class="graf graf--pre graf-after--p">child_result = join(t1)</pre><p name="d068" id="d068" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(3) Thread Creation and Termination Example</strong></p><p name="0c66" id="0c66" class="graf graf--p graf-after--p">Suppose we have the following pseudocode and the operation <code class="markup--code markup--p-code">safe_insert</code> is a call that manipulates the shared list <code class="markup--code markup--p-code">list</code>.</p><pre name="e4a1" id="e4a1" class="graf graf--pre graf-after--p">Thread thread1;<br>Shared_list list;<br>thread1 = fork(safe_insert, 4);<br>safe_insert(6);<br>child_result = join(thread1);</pre><p name="b917" id="b917" class="graf graf--p graf-after--pre">the program above executes the call <code class="markup--code markup--p-code">safe_insert(4)</code> for <code class="markup--code markup--p-code">thread1</code> and <code class="markup--code markup--p-code">safe_insert(6)</code> for <code class="markup--code markup--p-code">thread0</code> in parallel, and it assigns the result of calling <code class="markup--code markup--p-code">safe_insert(4)</code> to the variable <code class="markup--code markup--p-code">child_result</code>. In this case, the <code class="markup--code markup--p-code">join</code> call is optional because the result of the <code class="markup--code markup--p-code">safe_insert(4)</code> will be stored in the shared link <code class="markup--code markup--p-code">link</code>. So we can also capture the child result without calling <code class="markup--code markup--p-code">join</code>.</p><p name="66c1" id="66c1" class="graf graf--p graf-after--p">Moreover, in reality, <code class="markup--code markup--p-code">join</code> is not called very much because most of the new threads we <code class="markup--code markup--p-code">fork</code> can have no results, or the result can be communicated in some other ways (like <code class="markup--code markup--p-code">list</code>). Thus, if a thread’s initial procedure has returned and there is no subsequent call of <code class="markup--code markup--p-code">join</code>, the thread quietly evaporates.</p><p name="fb21" id="fb21" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Recall: Linked List Data Structure</strong></p><p name="23c9" id="23c9" class="graf graf--p graf-after--p">Let’s see how does a shared linked list works. Almost each of the elements in a linked list consists of two parts, <code class="markup--code markup--p-code">value</code> (the value we want to store in this entry) and <code class="markup--code markup--p-code">p_next</code> (the pointer points to the next element). To insert a value <code class="markup--code markup--p-code">X</code> into the linked list <code class="markup--code markup--p-code">list</code>,</p><pre name="2b41" id="2b41" class="graf graf--pre graf-after--p">List_element e;<br>e.value = X;<br>e.p_next = list.p_next;<br>list.p_next = &amp;e;</pre><p name="9e58" id="9e58" class="graf graf--p graf-after--pre">In the beginning, when we create the linked list <code class="markup--code markup--p-code">Shared_list list;</code> , we are actually creating an element with <code class="markup--code markup--p-code">list.value = head;</code> and <code class="markup--code markup--p-code">list.p_next = NULL;</code>,</p><pre name="6064" id="6064" class="graf graf--pre graf-after--p">+-------+------+<br>|  head |     -+-&gt; NULL<br>+-------+------+</pre><p name="72c4" id="72c4" class="graf graf--p graf-after--pre">Then, suppose we insert a value <code class="markup--code markup--p-code">X</code>, our linked list should be,</p><pre name="f7d9" id="f7d9" class="graf graf--pre graf-after--p">+-------+------+   +-------+------+  <br>|  head |     -+-&gt; |   X   |     -+-&gt; NULL<br>+-------+------+   +-------+------+  </pre><p name="a3c2" id="a3c2" class="graf graf--p graf-after--pre">What’s more, suppose we then insert a value <code class="markup--code markup--p-code">Y</code>, our linked list should be,</p><pre name="3a0c" id="3a0c" class="graf graf--pre graf-after--p">+-------+------+   +-------+------+   +-------+------+<br>|  head |     -+-&gt; |   Y   |     -+-&gt; |   X   |     -+-&gt; NULL<br>+-------+------+   +-------+------+   +-------+------+</pre><p name="9ada" id="9ada" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(5) Concurrency Control Problem</strong></p><p name="c554" id="c554" class="graf graf--p graf-after--p">Now let’s look back to our last example. We have known that the program executes the call <code class="markup--code markup--p-code">safe_insert(4)</code> for <code class="markup--code markup--p-code">thread1</code> and <code class="markup--code markup--p-code">safe_insert(6)</code> for <code class="markup--code markup--p-code">thread0</code> in parallel and there is a problem if two threads are running on the CPU at the same time because their operations are randomly interleaved. If the thread <code class="markup--code markup--p-code">t1</code> and <code class="markup--code markup--p-code">t0</code> simultaneously set the pointer of the element to <code class="markup--code markup--p-code">NULL</code> and then take turns in setting the pointers of the <code class="markup--code markup--p-code">list</code>, the first inserted element will be abundant.</p><p name="5b00" id="5b00" class="graf graf--p graf-after--p">For example, after <code class="markup--code markup--p-code">e.value = 4; e.p_next = list.p_next; </code>and <code class="markup--code markup--p-code">e.value = 6; e.p_next = list.p_next;</code>, all the <code class="markup--code markup--p-code">p_next</code>s will be pointing to <code class="markup--code markup--p-code">NULL</code>,</p><pre name="0b73" id="0b73" class="graf graf--pre graf-after--p">+-------+------+<br>|   4   |     -+-&gt; NULL<br>+-------+------+<br>+-------+------+<br>|   6   |     -+-&gt; NULL<br>+-------+------+</pre><p name="d074" id="d074" class="graf graf--p graf-after--pre">Then if we first run <code class="markup--code markup--p-code">list.p_next = &amp;e;</code> for <code class="markup--code markup--p-code">t0</code> and then for <code class="markup--code markup--p-code">t1</code>, the result will be,</p><pre name="4000" id="4000" class="graf graf--pre graf-after--p">+-------+------+  +-------+------+<br>|  head |     -+-&gt;|   4   |     -+-&gt; NULL<br>+-------+------+  +-------+------+<br>+-------+------+<br>|   6   |     -+-&gt; NULL<br>+-------+------+</pre><p name="4d31" id="4d31" class="graf graf--p graf-after--pre">This is not what we want because the value <code class="markup--code markup--p-code">6</code> is not stored in the linked list.</p><p name="005e" id="005e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Concurrency Control By Mutual Exclusion</strong></p><p name="6707" id="6707" class="graf graf--p graf-after--p">We have noticed that we must deal with the concurrency problem if we want to implement a multi-thread system. In order to deal with that, the operating system (and the threading library) supports a mechanism called <strong class="markup--strong markup--p-strong">mutual exclusion (mutex)</strong>. A mutex is like a lock and it should be used whenever we want to restrict the shared data among the threads.</p><pre name="6f09" id="6f09" class="graf graf--pre graf-after--p">Mutex m;<br>lock(m) {<br>    // critical section<br>} // free lock</pre><p name="362c" id="362c" class="graf graf--p graf-after--pre">The data structure of mutex should at least contain information about the locked <strong class="markup--strong markup--p-strong">status</strong> (default unlocked), a list of all the <strong class="markup--strong markup--p-strong">threads that are blocked</strong> (no need to be in order), and the <strong class="markup--strong markup--p-strong">owner</strong> thread of this lock. When a thread locks a mutex, it has exclusive access to the shared resources and this means that the other threads can not be able to successfully access or lock the same mutex. If another thread attempts to lock the mutex when it is already locked, the second thread blocks until the mutex is unlocked. The portion of the shared code that has been locked by mutex is called the <strong class="markup--strong markup--p-strong">critical section</strong>.</p><p name="5e99" id="5e99" class="graf graf--p graf-after--p">It is also interesting to know that Birrell’s construct of mutex follows the structure of,</p><pre name="5d69" id="5d69" class="graf graf--pre graf-after--p">lock(m) {<br>    // critical section<br>} // free lock</pre><p name="e237" id="e237" class="graf graf--p graf-after--pre">and the code above implicitly frees the lock after the critical section. While most of the common APIs have two separate explicit calls <code class="markup--code markup--p-code">lock</code> and <code class="markup--code markup--p-code">unlock</code>. So the structure of some other interface can be,</p><pre name="203b" id="203b" class="graf graf--pre graf-after--p">lock(m);<br>// critical section<br>unlock(m);</pre><p name="80d9" id="80d9" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(7) Mutex: An Example</strong></p><p name="55a8" id="55a8" class="graf graf--p graf-after--p">Now, let’s see how to deal with the concurrency problem in the previous linked list example. Suppose we still have the following <code class="markup--code markup--p-code">main</code> program,</p><pre name="940e" id="940e" class="graf graf--pre graf-after--p">Thread thread1;<br>Shared_list list;<br>thread1 = fork(safe_insert, 4);<br>safe_insert(6);<br>child_result = join(thread1);</pre><p name="83da" id="83da" class="graf graf--p graf-after--pre">And we would like to build a <code class="markup--code markup--p-code">safe_insert</code> call that will not have any concurrency problems by using the mutex. This function could be,</p><pre name="4be7" id="4be7" class="graf graf--pre graf-after--p">Mutex m;<br>void <code class="markup--code markup--pre-code">safe_insert(int i) {<br>    lock(m) {<br>        </code>List_element e;<br>        e.value = i;<br>        e.p_next = list.p_next;<br>        list.p_next = &amp;e;<br>    <code class="markup--code markup--pre-code">}<br>}</code></pre><p name="22f2" id="22f2" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">4. Condition Variables</strong></p><p name="21da" id="21da" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Producer/Consumer Example</strong></p><p name="0eca" id="0eca" class="graf graf--p graf-after--p">Before we look at the mechanism of the condition variables, let’s first see a producer/consumer example. Suppose we create 10 threads (handled by <code class="markup--code markup--p-code">producers[0]</code> to <code class="markup--code markup--p-code">producers[9]</code>) used to write their thread IDs into a linked list by the call <code class="markup--code markup--p-code">safe_insert</code> until the linked list is full. Then, we create 1 thread that is going to print and then clear this linked list only when this list is full.</p><pre name="e0be" id="e0be" class="graf graf--pre graf-after--p">// main<br>for i = 0, ..., 9<br>    producers[i] = fork(safe_insert, NULL)   // create producers<br>consumer = fork(print_and_clear, list)       // create consumers</pre><p name="0228" id="0228" class="graf graf--p graf-after--pre">Suppose we only use the mutex, we should have the <code class="markup--code markup--p-code">safe_insert</code> call as</p><pre name="72bd" id="72bd" class="graf graf--pre graf-after--p">// producers<br>Mutex m;<br>lock(m) {<br>    list.insert(producers[i].tid);<br>}</pre><p name="b332" id="b332" class="graf graf--p graf-after--pre">Then the consumer call should be,</p><pre name="a74e" id="a74e" class="graf graf--pre graf-after--p">// consumer<br>Mutex m;<br>lock(m) {<br>    if list.full {<br>        list.print;<br>        list.clear;<br>    } <br>}</pre><p name="2c52" id="2c52" class="graf graf--p graf-after--pre">However, the behavior of the consumer can be wasteful because it has to conduct &lt;lock, not full, unlock&gt; pairs every time, and what we want is that this thread can wait for the other threads and do nothing until the other threads tell it that the list is full.</p><p name="2780" id="2780" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Condition Variable</strong></p><p name="6524" id="6524" class="graf graf--p graf-after--p">Birrell recognizes that this can be a common situation in the multi-thread system, so he designed a new construct called the <strong class="markup--strong markup--p-strong">condition variable</strong>. This construct should be used in conjunction with mutexes in order to control the threads.</p><p name="0347" id="0347" class="graf graf--p graf-after--p">So the producer program can check for the full condition of every execution and send a single of <code class="markup--code markup--p-code">list_full</code> if the list is full. The <code class="markup--code markup--p-code">Signal</code> call can be used to send the signal.</p><pre name="311f" id="311f" class="graf graf--pre graf-after--p">// producers<br>Mutex m;<br>lock(m) {<br>    list.insert(producers[i].tid);<br>    if list.full Signal(list_full);<br>}</pre><p name="8240" id="8240" class="graf graf--p graf-after--pre">For the consumer thread, it will wait for the signal <code class="markup--code markup--p-code">list_full</code> before it continues the program. We are using the <code class="markup--code markup--p-code">Wait(m, list_full)</code> call to wait for the signal,</p><pre name="1ba0" id="1ba0" class="graf graf--pre graf-after--p">// consumer<br>Mutex m;<br>lock(m) {<br>    While (!list.full) Wait(m, list_full);<br>    list.print;<br>    list.clear;<br>}</pre><p name="65fb" id="65fb" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(3) Condition Variable API</strong></p><p name="17b6" id="17b6" class="graf graf--p graf-after--p">For the condition variable, we commonly have the following APIs,</p><ul class="postList"><li name="84f1" id="84f1" class="graf graf--li graf-after--p">Condition type: used to set the data type of the signal,</li></ul><pre name="9276" id="9276" class="graf graf--pre graf-after--li">Condition cond;</pre><ul class="postList"><li name="ea0b" id="ea0b" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">Wait</code>: for waiting for the signal. We should know that the mutex is automatically released and re-acquired if we have to wait for this condition <code class="markup--code markup--li-code">cond</code> to occur.</li></ul><pre name="3615" id="3615" class="graf graf--pre graf-after--li">Wait(mutex, cond);<br>/* automatically release mutex<br>** go on waiting queue<br>**<br>** wait ... wait ... wait<br>**<br>** remove from waiting queue<br>** re-acquire mutex<br>** exit the wait operation */</pre><ul class="postList"><li name="d6ec" id="d6ec" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">Signal</code>: be able to weak up one thread that is waiting for <code class="markup--code markup--li-code">cond</code> at a time.</li></ul><pre name="190c" id="190c" class="graf graf--pre graf-after--li">Signal(cond);</pre><ul class="postList"><li name="4d41" id="4d41" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">Broadcast</code>: be able to weak up all threads that are waiting for <code class="markup--code markup--li-code">cond</code>.</li></ul><pre name="4ebc" id="4ebc" class="graf graf--pre graf-after--li graf--trailing">Broadcast(cond);</pre></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/abafa6c6bd61"><time class="dt-published" datetime="2021-01-30T18:25:48.436Z">January 30, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/operating-system-4-thread-part-1-abafa6c6bd61" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>