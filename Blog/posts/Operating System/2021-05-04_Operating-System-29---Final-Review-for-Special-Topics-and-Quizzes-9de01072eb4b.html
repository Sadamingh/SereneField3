<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Operating System 29 | Final Review for Special Topics and Quizzes</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Operating System 29 | Final Review for Special Topics and Quizzes</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Operating System
</section>
<section data-field="body" class="e-content">
<section name="8ccc" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="171f" id="171f" class="graf graf--h3 graf--leading graf--title">Operating System 29 | Final Review for Special Topics and Quizzes</h3><figure name="b3b1" id="b3b1" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*V5Ry-WA3tPR_owCd.png" data-width="1508" data-height="794" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*V5Ry-WA3tPR_owCd.png"></figure><ol class="postList"><li name="cda7" id="cda7" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">OS Scheduling</strong></li></ol><p name="0777" id="0777" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Linux O(1) Scheduler</strong></p><p name="4260" id="4260" class="graf graf--p graf-after--p">0 being the highest (ts = 100 ms) and 139 (ts = 10 ms) is the lowest. The default level is 120. The pointer of the two arrays should be swapped.</p><ul class="postList"><li name="e46a" id="e46a" class="graf graf--li graf-after--p">O(1) Vs. CFS: CFS is better for interactive operations.</li></ul><p name="a0b2" id="a0b2" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Fedorova’s Paper</strong></p><p name="cd09" id="cd09" class="graf graf--p graf-after--p">Conclusion: workloads of mixed CPI result in better performance</p><p name="d8f7" id="d8f7" class="graf graf--p graf-after--p">Problem: real workload CPIs have little difference</p><p name="8733" id="8733" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 1-1. Round-Robin Scheduler</strong></p><p name="49e5" id="49e5" class="graf graf--p graf-after--p">On a single CPU system, consider the following workload and conditions:</p><ul class="postList"><li name="a7a8" id="a7a8" class="graf graf--li graf-after--p">10 I/O-bound tasks and 1 CPU-bound task</li><li name="c5a8" id="c5a8" class="graf graf--li graf-after--li">I/O-bound tasks issue an I/O operation once every 1 ms of CPU computing</li><li name="a4f2" id="a4f2" class="graf graf--li graf-after--li">I/O operations always take 10 ms to complete</li><li name="e277" id="e277" class="graf graf--li graf-after--li">Context switching overhead is 0.1 ms</li><li name="6d20" id="6d20" class="graf graf--li graf-after--li">I/O device(s) have an infinite capacity to handle concurrent I/O requests</li><li name="97e0" id="97e0" class="graf graf--li graf-after--li">All tasks are long-running</li></ul><p name="c378" id="c378" class="graf graf--p graf-after--li">Now, answer the following questions (for each question, round to the nearest percent):</p><p name="586f" id="586f" class="graf graf--p graf-after--p">a. What is the <strong class="markup--strong markup--p-strong">CPU utilization</strong> (%) for a round-robin scheduler where the timeslice is 1 ms?</p><p name="3c73" id="3c73" class="graf graf--p graf-after--p">b. What is the <strong class="markup--strong markup--p-strong">CPU utilization</strong> (%) for a round-robin scheduler where the timeslice is 10 ms?</p><p name="e9a5" id="e9a5" class="graf graf--p graf-after--p">c. What is the <strong class="markup--strong markup--p-strong">CPU utilization</strong> (%) for a round-robin scheduler where the timeslice is 20 ms?</p><p name="766a" id="766a" class="graf graf--p graf-after--p">d.What is the <strong class="markup--strong markup--p-strong">I/O utilization</strong> (%) for a round-robin scheduler where the timeslice is 1 ms?</p><p name="c52a" id="c52a" class="graf graf--p graf-after--p">e.What is the <strong class="markup--strong markup--p-strong">I/O utilization</strong> (%) for a round-robin scheduler where the timeslice is 20 ms?</p><p name="6717" id="6717" class="graf graf--p graf-after--p">Solution:</p><p name="b945" id="b945" class="graf graf--p graf-after--p">Workload Summary:</p><p name="b295" id="b295" class="graf graf--p graf-after--p">For 10 I/O-bound tasks,</p><ul class="postList"><li name="9f0e" id="9f0e" class="graf graf--li graf-after--p">being issued every 1 ms</li><li name="7144" id="7144" class="graf graf--li graf-after--li">completing in 10 ms</li><li name="b166" id="b166" class="graf graf--li graf-after--li">infinite capacity</li></ul><p name="981b" id="981b" class="graf graf--p graf-after--li">For 1 CPU-bound task,</p><ul class="postList"><li name="6083" id="6083" class="graf graf--li graf-after--p">fully occupy the CPU</li></ul><p name="1197" id="1197" class="graf graf--p graf-after--li">For context switching,</p><ul class="postList"><li name="081d" id="081d" class="graf graf--li graf-after--p">1 ms each</li></ul><p name="9704" id="9704" class="graf graf--p graf-after--li">a.</p><p name="0386" id="0386" class="graf graf--p graf-after--p">With RR scheduler with 1 ms time slice and all the tasks are long-run (means they will looply execute and never stops),</p><figure name="4f18" id="4f18" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*USMglZg8kUERWnqffZoNxQ.png" data-width="1518" data-height="340" src="https://cdn-images-1.medium.com/max/800/1*USMglZg8kUERWnqffZoNxQ.png"></figure><p name="f52d" id="f52d" class="graf graf--p graf-after--figure">The CPU utilization should be,</p><pre name="bdaa" id="bdaa" class="graf graf--pre graf-after--p">task CPU Time = 1 ms<br>Context Switch = 0.1 ms<br>CPU utilization = task CPU Time / (Context Switch + task CPU Time) = 91%</pre><p name="b83a" id="b83a" class="graf graf--p graf-after--pre">b.</p><p name="cf95" id="cf95" class="graf graf--p graf-after--p">With RR scheduler with 10 ms time slice and all the tasks are long-run, only the CPU-bound task can fully utilize the full-time slice of 10 ms,</p><figure name="7c25" id="7c25" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1pqGw8ejdmn96CNedvkEwg.png" data-width="1518" data-height="340" src="https://cdn-images-1.medium.com/max/800/1*1pqGw8ejdmn96CNedvkEwg.png"></figure><p name="a1bf" id="a1bf" class="graf graf--p graf-after--figure">so the CPU utilization should be,</p><pre name="721f" id="721f" class="graf graf--pre graf-after--p">I/O CPU Time = 1 ms<br>Comp CPU Time = 10 ms<br>Context Switch = 0.1 ms<br>CPU utilization = (10 * 1 ms + 1 * 10 ms) / (10 * 1 ms + 1 * 10 ms + 0.1 ms * 11) = 20/21.1 = 95%</pre><p name="d442" id="d442" class="graf graf--p graf-after--pre">c.</p><p name="ca86" id="ca86" class="graf graf--p graf-after--p">With RR scheduler with 20 ms time slice and all the tasks are long-run, only the CPU-bound task can fully utilize the full-time slice of 20 ms. The CPU utilization should be,</p><pre name="2c22" id="2c22" class="graf graf--pre graf-after--p">I/O CPU Time = 1 ms<br>Comp CPU Time = 20 ms<br>Context Switch = 0.1 ms<br>CPU utilization = (10 * 1 ms + 1 * 20 ms) / (10 * 1 ms + 1 * 20 ms + 0.1 ms * 11) = 30/31.1 = 96%</pre><p name="6de1" id="6de1" class="graf graf--p graf-after--pre">d.</p><p name="5d83" id="5d83" class="graf graf--p graf-after--p">For I/O utilization,</p><ul class="postList"><li name="bf57" id="bf57" class="graf graf--li graf-after--p">the first I/O request is issued at 1 ms time</li><li name="cadd" id="cadd" class="graf graf--li graf-after--li">the last I/O request is issued at,</li></ul><pre name="928e" id="928e" class="graf graf--pre graf-after--li">10 * 1ms + 9 * 0.1 = 10.9 ms</pre><ul class="postList"><li name="2b29" id="2b29" class="graf graf--li graf-after--pre">the last I/O request is finished at,</li></ul><pre name="f1b0" id="f1b0" class="graf graf--pre graf-after--li">10.9 ms + 10 ms = 20.9 ms</pre><ul class="postList"><li name="f6d2" id="f6d2" class="graf graf--li graf-after--pre">Total time from I/O processing should be,</li></ul><pre name="9ec6" id="9ec6" class="graf graf--pre graf-after--li">20.9 ms - 1 ms = 19.9 ms</pre><ul class="postList"><li name="5516" id="5516" class="graf graf--li graf-after--pre">the CPU bound task will complete at,</li></ul><pre name="dd17" id="dd17" class="graf graf--pre graf-after--li">10 * 1 ms + 1 * 1 ms + 11 * 0.1 = 12.1 ms</pre><ul class="postList"><li name="dc8a" id="dc8a" class="graf graf--li graf-after--pre">Because all the I/O bound tasks will not finish when the CPU bound task, so we will have an I/O utilization of,</li></ul><pre name="39e2" id="39e2" class="graf graf--pre graf-after--li">100%</pre><p name="dbcc" id="dbcc" class="graf graf--p graf-after--pre">e.</p><p name="6eb3" id="6eb3" class="graf graf--p graf-after--p">For I/O utilization,</p><ul class="postList"><li name="f271" id="f271" class="graf graf--li graf-after--p">the first I/O request is issued at 1 ms time</li><li name="c7ac" id="c7ac" class="graf graf--li graf-after--li">the last I/O request is issued at,</li></ul><pre name="d37b" id="d37b" class="graf graf--pre graf-after--li">10 * 1ms + 9 * 0.1 = 10.9 ms</pre><ul class="postList"><li name="a469" id="a469" class="graf graf--li graf-after--pre">the last I/O request is finished at,</li></ul><pre name="2649" id="2649" class="graf graf--pre graf-after--li">10.9 ms + 10 ms = 20.9 ms</pre><ul class="postList"><li name="6908" id="6908" class="graf graf--li graf-after--pre">Total time from I/O processing should be,</li></ul><pre name="5e5b" id="5e5b" class="graf graf--pre graf-after--li">20.9 ms - 1 ms = 19.9 ms</pre><ul class="postList"><li name="1000" id="1000" class="graf graf--li graf-after--pre">the CPU bound task will complete at,</li></ul><pre name="c1d8" id="c1d8" class="graf graf--pre graf-after--li">10 * 1 ms + 1 * 20 ms + 11 * 0.1 = 31.1 ms</pre><ul class="postList"><li name="63f8" id="63f8" class="graf graf--li graf-after--pre">Because all the I/O bound tasks finish before CPU bound task completes, so we will have an I/O utilization of,</li></ul><pre name="b104" id="b104" class="graf graf--pre graf-after--li">I/O utilization = 19.9 / 31.1 = 64%</pre><p name="e904" id="e904" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Example 1-2. Linux O(1) Scheduler</strong></p><p name="c4ed" id="c4ed" class="graf graf--p graf-after--p">For the next four questions, consider a Linux system with the following assumptions:</p><ul class="postList"><li name="83d6" id="83d6" class="graf graf--li graf-after--p">uses the O(1) scheduling algorithm for time-sharing threads</li><li name="f0a5" id="f0a5" class="graf graf--li graf-after--li">must assign a time quantum for thread T1 with priority 110</li><li name="2734" id="2734" class="graf graf--li graf-after--li">must assign a time quantum for thread T2 with priority 135</li></ul><p name="6238" id="6238" class="graf graf--p graf-after--li">Provide answers to the following:</p><p name="bb4b" id="bb4b" class="graf graf--p graf-after--p">a. Which thread has a <strong class="markup--strong markup--p-strong">“higher”</strong> priority (will be serviced first)?</p><p name="2219" id="2219" class="graf graf--p graf-after--p">b. Which thread is assigned a <strong class="markup--strong markup--p-strong">longer time quantum</strong>?</p><p name="6030" id="6030" class="graf graf--p graf-after--p">c. Assume T2 has used its time quantum without blocking. What will happen to the value that represents its priority level when T2 gets scheduled again (lower/decrease, higher/increase, same) ___________.</p><p name="8c3c" id="8c3c" class="graf graf--p graf-after--p">d. Assume now that T2 blocks for I/O before its time quantum expired. What will happen to the value that represents its priority level when T2 gets scheduled again (lower/decrease, higher/increase, same) ___________.</p><p name="c391" id="c391" class="graf graf--p graf-after--p">Solution:</p><p name="601e" id="601e" class="graf graf--p graf-after--p">Lower level value has higher priority and longer time slice (quantum), so</p><p name="7f7e" id="7f7e" class="graf graf--p graf-after--p">a. thread T1</p><p name="a7d5" id="a7d5" class="graf graf--p graf-after--p">b. thread T1</p><p name="e339" id="e339" class="graf graf--p graf-after--p">c. higher/increase. Because without blocking means that we have a longer time quantum, so in the next schedule, the time quantum of it will be reduced by increasing the level value</p><p name="f1fe" id="f1fe" class="graf graf--p graf-after--p">d. lower/decrease. expire means that we have a shorter time quantum, so in the next schedule, the time quantum of it will be increased by reducing the level value</p><p name="461a" id="461a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 1–3. Hardware Counter</strong></p><p name="0cbe" id="0cbe" class="graf graf--p graf-after--p">Consider a quad-core machine with a single memory module connected to the CPUs via a shared “bus”. On this machine, a CPU instruction takes 1 cycle, and a memory instruction takes 4 cycles.</p><p name="ea65" id="ea65" class="graf graf--p graf-after--p">The machine has two hardware counters:</p><ul class="postList"><li name="831f" id="831f" class="graf graf--li graf-after--p">counter that measures IPC</li><li name="a9bd" id="a9bd" class="graf graf--li graf-after--li">counter that measures CPI</li></ul><p name="7a2e" id="7a2e" class="graf graf--p graf-after--li">Answer the following:</p><p name="a2fa" id="a2fa" class="graf graf--p graf-after--p">a. What does IPC stand for in this context? ____________</p><p name="fe0c" id="fe0c" class="graf graf--p graf-after--p">b. What does CPI stand for in this context? ____________</p><p name="9134" id="9134" class="graf graf--p graf-after--p">c. What is the highest IPC on this machine? ____________</p><p name="a6bd" id="a6bd" class="graf graf--p graf-after--p">d. What is the highest CPI on this machine? ____________</p><p name="453c" id="453c" class="graf graf--p graf-after--p">e. Suppose we have the following long-running workloads of 8 memory instructions and 8 CPU instructions. Which one will perform better? Select all that’s apply. ____________</p><ul class="postList"><li name="49a4" id="49a4" class="graf graf--li graf-after--p">A.</li></ul><pre name="1368" id="1368" class="graf graf--pre graf-after--li">Core 1: 4 memory instructions <br>Core 2: 4 memory instructions<br>Core 3: 4 CPU  instructions<br>Core 4: 4 CPU  instructions</pre><ul class="postList"><li name="31bc" id="31bc" class="graf graf--li graf-after--pre">B.</li></ul><pre name="cddf" id="cddf" class="graf graf--pre graf-after--li">Core 1: 4 memory instructions <br>Core 2: 4 CPU  instructions<br>Core 3: 4 memory instructions<br>Core 4: 4 CPU  instructions</pre><ul class="postList"><li name="f2cb" id="f2cb" class="graf graf--li graf-after--pre">C.</li></ul><pre name="e3e7" id="e3e7" class="graf graf--pre graf-after--li">Core 1: 1 memory instructions and 3 CPU instructions<br>Core 2: 3 memory instructions and 1 CPU instructions<br>Core 3: 4 memory instructions<br>Core 4: 4 CPU  instructions</pre><ul class="postList"><li name="01e3" id="01e3" class="graf graf--li graf-after--pre">D.</li></ul><pre name="3e31" id="3e31" class="graf graf--pre graf-after--li">Core 1: 1 memory instructions and 3 CPU instructions<br>Core 2: 3 memory instructions and 1 CPU instructions<br>Core 3: 2 memory instructions and 2 CPU instructions<br>Core 4: 4 CPU  instructions</pre><ul class="postList"><li name="f96b" id="f96b" class="graf graf--li graf-after--pre">E.</li></ul><pre name="0e63" id="0e63" class="graf graf--pre graf-after--li">Core 1: 2 memory instructions and 2 CPU instructions<br>Core 2: 2 memory instructions and 2 CPU instructions<br>Core 3: 2 memory instructions and 2 CPU instructions<br>Core 4: 2 memory instructions and 2 CPU instructions</pre><ul class="postList"><li name="757f" id="757f" class="graf graf--li graf-after--pre">F.</li></ul><pre name="1e2e" id="1e2e" class="graf graf--pre graf-after--li">Core 1: 1 memory instructions and 3 CPU instructions<br>Core 2: 3 memory instructions and 1 CPU instructions<br>Core 3: 2 memory instructions and 2 CPU instructions<br>Core 4: 2 memory instructions and 2 CPU instructions</pre><p name="2633" id="2633" class="graf graf--p graf-after--pre">Solution:</p><p name="270c" id="270c" class="graf graf--p graf-after--p">a. Instruction per cycle</p><p name="4ce7" id="4ce7" class="graf graf--p graf-after--p">b. Cycles per instruction</p><p name="bc5d" id="bc5d" class="graf graf--p graf-after--p">c. 4. In the extreme case, if we only have CPU instructions on each core, we will have an IPC of 4.</p><p name="7559" id="7559" class="graf graf--p graf-after--p">d. 16. In the extreme case, of we have only memory instructions on each core, these instructions will contend on the shared bus and the shared memory controller. So in the worst case, they can take up to 4*4 = 16 cycles for one instruction to complete.</p><p name="2870" id="2870" class="graf graf--p graf-after--p">e. EF. If we have a mixed CPI on each core, we can have better performance.</p><p name="7df4" id="7df4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 1-4. Multilevel Feed Back Queue</strong></p><p name="6795" id="6795" class="graf graf--p graf-after--p">Consider a system that has a CPU-bound process, which requires a burst time of 40 seconds. The multilevel feedback queue scheduling algorithm is used and the queue time quantum 2 seconds and in each level it is incremented by 5 seconds. Then how many times the process will be interrupted and on which queue the process will terminate the execution?</p><p name="fb32" id="fb32" class="graf graf--p graf-after--p">Solution:</p><p name="3705" id="3705" class="graf graf--p graf-after--p">The process is interrupted 4 times and completes on queue 5.</p><ul class="postList"><li name="cfc8" id="cfc8" class="graf graf--li graf-after--p">At Queue 1 it is executed for 2 seconds and then interrupted and shifted to queue 2.</li><li name="7894" id="7894" class="graf graf--li graf-after--li">At Queue 2 it is executed for 7 seconds and then interrupted and shifted to queue 3.</li><li name="03f8" id="03f8" class="graf graf--li graf-after--li">At Queue 3 it is executed for 12 seconds and then interrupted and shifted to queue 4.</li><li name="c0a3" id="c0a3" class="graf graf--li graf-after--li">At Queue 4 it is executed for 17 seconds and then interrupted and shifted to queue 5.</li><li name="59be" id="59be" class="graf graf--li graf-after--li">At Queue 5 it executes for 2 seconds and then it completes.</li></ul><p name="0e2b" id="0e2b" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Example 1-5. CPU Scheduling</strong></p><p name="2ad7" id="2ad7" class="graf graf--p graf-after--p">Consider a scenario with the following 3-task workload:</p><ul class="postList"><li name="438a" id="438a" class="graf graf--li graf-after--p">T1 is I/O-bound but needs 3ms of CPU time</li><li name="3a2e" id="3a2e" class="graf graf--li graf-after--li">T2 and T3 are CPU-bound and need 4ms and 5ms of CPU time, respectively.</li><li name="04e4" id="04e4" class="graf graf--li graf-after--li">T1 issues an I/O request after 1ms of CPU time.</li><li name="93d6" id="93d6" class="graf graf--li graf-after--li">I/O requests take 2 ms to complete.</li><li name="012b" id="012b" class="graf graf--li graf-after--li">The scheduling algorithm runs in 0.2ms</li><li name="b824" id="b824" class="graf graf--li graf-after--li">A context switch costs 0.3ms</li><li name="43e3" id="43e3" class="graf graf--li graf-after--li">Tasks arrive in the order 1, 2, 3.</li></ul><p name="b3ae" id="b3ae" class="graf graf--p graf-after--li">Suppose we have a round-robin scheduler with 2ms timeslices. Answer:</p><p name="423f" id="423f" class="graf graf--p graf-after--p">a. What are the basic steps involved in scheduling a thread on the CPU?</p><p name="fe32" id="fe32" class="graf graf--p graf-after--p">b. What are the basic data structures involved in scheduling a thread on the CPU?</p><p name="69ea" id="69ea" class="graf graf--p graf-after--p">c. Calculate the throughput of this workload.</p><p name="853c" id="853c" class="graf graf--p graf-after--p">d. Calculate the average completion time of this workload.</p><p name="3915" id="3915" class="graf graf--p graf-after--p">e. Calculate the average waiting time of this workload.</p><p name="89ba" id="89ba" class="graf graf--p graf-after--p">f. Calculate the CPU utilization of this workload.</p><p name="8df2" id="8df2" class="graf graf--p graf-after--p">g. Calculate the I/O utilization of this workload before it finishes.</p><p name="6224" id="6224" class="graf graf--p graf-after--p">h. What if the I/O requests take 5 ms to complete? Will the result change? Explain your answer.</p><p name="ecf2" id="ecf2" class="graf graf--p graf-after--p">Solution:</p><p name="9af7" id="9af7" class="graf graf--p graf-after--p">a.</p><ul class="postList"><li name="447a" id="447a" class="graf graf--li graf-after--p">idling: when the CPU becomes idle, we can start scheduling</li><li name="6362" id="6362" class="graf graf--li graf-after--li">preemption: kernel sends a signal to interrupt the current task</li><li name="e6ed" id="e6ed" class="graf graf--li graf-after--li">prioritization: kernel prioritizes a new task based on the priority</li><li name="3379" id="3379" class="graf graf--li graf-after--li">timeslice expiration: once the timeslice expires, the scheduler needs to determine who is allotted the next timeslice.</li></ul><p name="118f" id="118f" class="graf graf--p graf-after--li">b. The CPU scheduling relies on,</p><ul class="postList"><li name="5be0" id="5be0" class="graf graf--li graf-after--p">Scheduling policies: for example, FCFS, SJF, complex first, etc.</li><li name="6ff5" id="6ff5" class="graf graf--li graf-after--li">Runqueue: store the tasks and enable specific decision-making</li></ul><p name="b91f" id="b91f" class="graf graf--p graf-after--li">c.</p><figure name="bbfd" id="bbfd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_xSBcBcX9_q5_3mKpPeAyQ.png" data-width="1400" data-height="221" src="https://cdn-images-1.medium.com/max/800/1*_xSBcBcX9_q5_3mKpPeAyQ.png"></figure><pre name="71e0" id="71e0" class="graf graf--pre graf-after--figure">total CPU time = 3 + 4 + 5 = 12 s<br># of ctx switches = 5<br># of schedulings = 6<br>total time = total CPU time + # of ctx switches * 0.3 + # of schedulings * 0.2<br>= 12 + 5 * 0.3 + 6 * 0.2<br>= 12 + 1.5 + 1.2<br>= 14.7 s<br>Therefore, the throughput is,<br>throughput = 3/14.7 = 0.204 tasks/sec</pre><p name="4039" id="4039" class="graf graf--p graf-after--pre">d.</p><pre name="1697" id="1697" class="graf graf--pre graf-after--p">T1 starts at 0s<br>T2 starts at 0 + 1 + 0.2 + 0.3 = 1.5 s<br>T3 starts at 1.5 + 2 + 0.2 + 0.3 = 4 s</pre><pre name="02fd" id="02fd" class="graf graf--pre graf-after--pre">T1 finishes at 4 + 2 + 0.2 + 0.3 + 2 = 8.5 s<br>T2 finishes at 8.5 + 0.2 + 0.3 + 2 = 11 s<br>T3 finishes at the end = 14.7 s</pre><pre name="e971" id="e971" class="graf graf--pre graf-after--pre">Therefore, the avg. comp time = (8.5 + 11 + 14.7) / 3<br>= 11.4 s</pre><p name="1744" id="1744" class="graf graf--p graf-after--pre">e.</p><pre name="4eeb" id="4eeb" class="graf graf--pre graf-after--p">Similarly, the avg. waiting time = (0 + 1.5 + 4) / 3<br>= 5.5 / 3 = 1.83 s</pre><p name="9d7c" id="9d7c" class="graf graf--p graf-after--pre">f.</p><pre name="bb03" id="bb03" class="graf graf--pre graf-after--p">CPU utilization = total CPU time / total time<br>= 12/14.7<br>= 81.6%</pre><p name="a916" id="a916" class="graf graf--p graf-after--pre">g.</p><pre name="38c0" id="38c0" class="graf graf--pre graf-after--p">I/O utilization = I/O time / total time<br>= 2/14.7<br>= 13.6%</pre><p name="e690" id="e690" class="graf graf--p graf-after--pre">h.</p><p name="861f" id="861f" class="graf graf--p graf-after--p graf--trailing">No. The result will not change because the second schedule of T1 is at the time 6.5 s, and T1 starts I/O at 1 s. The time interval is 5.5 s and it is larger than the I/O time of 5 s.</p></div></div></section><section name="9a39" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="eec0" id="eec0" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong">2. Synchronization</strong></p><p name="d7d4" id="d7d4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 2-1. Spinning and Blocking</strong></p><p name="ae9d" id="ae9d" class="graf graf--p graf-after--p">In a multiprocessor system, a thread is trying to acquire a locked mutex.</p><p name="31bd" id="31bd" class="graf graf--p graf-after--p">a. What is the main difference between spinning and blocking.</p><p name="85e9" id="85e9" class="graf graf--p graf-after--p">b. Should the thread spin or block until the mutex is released?</p><p name="cbca" id="cbca" class="graf graf--p graf-after--p">c. Why might it be better to spin in some instances?</p><p name="43dd" id="43dd" class="graf graf--p graf-after--p">d. What if this were a uniprocessor system?</p><p name="ab28" id="ab28" class="graf graf--p graf-after--p">Solution:</p><p name="5d41" id="5d41" class="graf graf--p graf-after--p">a. When spinning, the current thread will continue occupy the lock, while for blocking, the current thread will be context switched to some other threads.</p><p name="aa79" id="aa79" class="graf graf--p graf-after--p">b. When the current thread requires resources that is occupied by another core, we should spin. When we currect thread requires resources that is occupied by the current core, we should block.</p><p name="2a80" id="2a80" class="graf graf--p graf-after--p">c. Because the owner of the lock may release it in a time smaller than the cost of context switching twice.</p><p name="cb08" id="cb08" class="graf graf--p graf-after--p">d. We can not use spinlocks for a uniprocessor system or the only CPU will be occupied with no exits.</p><p name="7c4b" id="7c4b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 2-2. Spinlock Implementations Performance</strong></p><p name="f9e9" id="f9e9" class="graf graf--p graf-after--p">For the following question, consider a multi-processor with write-invalidated cache coherence.</p><p name="1a08" id="1a08" class="graf graf--p graf-after--p">Determine whether the use of a dynamic (exponential backoff) delay has the same, better, or worse performance than a test-and-test-and-set (“spin on read”) lock. Then, explain why by making a performance comparison using each of the following metrics:</p><ul class="postList"><li name="063c" id="063c" class="graf graf--li graf-after--p">Latency</li><li name="b88c" id="b88c" class="graf graf--li graf-after--li">Delay</li><li name="3edd" id="3edd" class="graf graf--li graf-after--li">Contention</li></ul><p name="eec3" id="eec3" class="graf graf--p graf-after--li">Solution:</p><p name="e174" id="e174" class="graf graf--p graf-after--p">a. Better when we have less cores and lower loads. This is because the test-and-test-and-set lock is simple and the latency and delay is better. We don’t have to worry about contention when we have lower loads. However, because the delay time for dynamic delay lock is frequently unnecessary, the performance will be worse.</p><p name="9a62" id="9a62" class="graf graf--p graf-after--p">b. Worse when we have more cores and higher loads. This is because the test-and-test-and-set lock O(N²) high contention and dynamic delay lock is better.</p><p name="3443" id="3443" class="graf graf--p graf-after--p graf--trailing">c. Same when we have medium cores and medium loads. It is possible that at a specific load, those different approaches will result in the same performance.</p></div></div></section><section name="7bd9" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="7771" id="7771" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong">3. Memory, Storage, and I/O Devices</strong></p><p name="1a55" id="1a55" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 3-1. Page Table Size</strong></p><p name="6084" id="6084" class="graf graf--p graf-after--p">Consider a 32-bit (x86) platform running Linux that uses a single-level page table. What are the <strong class="markup--strong markup--p-strong">maximum number of page table entries</strong> when the following page sizes are used?</p><p name="07ca" id="07ca" class="graf graf--p graf-after--p">a. regular (4 kB) pages?</p><p name="355e" id="355e" class="graf graf--p graf-after--p">b. large (2 MB) pages?</p><p name="2e10" id="2e10" class="graf graf--p graf-after--p">Solution:</p><p name="9f2e" id="9f2e" class="graf graf--p graf-after--p">a.</p><pre name="76f3" id="76f3" class="graf graf--pre graf-after--p">Page size = 4 kB      =&gt;      log2(4*1024) = 12 bit page offsets<br>32 bit platform       =&gt;      32 - 12 = 20 bits of tag<br>Therefore, we can have 2^20 tags<br>So the maximum number of page table entries is 2^20.</pre><p name="d15b" id="d15b" class="graf graf--p graf-after--pre">b. Similarly,</p><pre name="fcaa" id="fcaa" class="graf graf--pre graf-after--p">Page size = 2 MB      =&gt;     log2(2*1024*1024) = 21 bit page offsets<br>32 bit platform       =&gt;     32 - 21 = 11 bits of tag<br>Therefore, we can have 2^11 tags<br>So the maximum number of page table entries is 2^11.</pre><p name="e155" id="e155" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Example 3-2. PIO and DMA</strong></p><p name="1cdb" id="1cdb" class="graf graf--p graf-after--p">Answer the following questions about PIO:</p><p name="90ea" id="90ea" class="graf graf--p graf-after--p">a. Considering I/O devices, what does PIO stand for?</p><p name="78bc" id="78bc" class="graf graf--p graf-after--p">b. List the steps performed by the OS or process running on the CPU when sending a network packet using PIO.</p><p name="f127" id="f127" class="graf graf--p graf-after--p">c. Considering I/O devices, what does DMA stand for?</p><p name="edba" id="edba" class="graf graf--p graf-after--p">d. Explain why DMA can improve the performance of PIO?</p><p name="a12b" id="a12b" class="graf graf--p graf-after--p">Solution:</p><p name="2685" id="2685" class="graf graf--p graf-after--p">a. Programmed I/O: CPU directly write command and data to device registers</p><p name="7ef7" id="7ef7" class="graf graf--p graf-after--p">b. The steps are,</p><ul class="postList"><li name="a3c5" id="a3c5" class="graf graf--li graf-after--p">write ‘command’ to the device to send a packet of appropriate size</li><li name="2e7f" id="2e7f" class="graf graf--li graf-after--li">copy the header and data in a contiguous memory location</li><li name="137f" id="137f" class="graf graf--li graf-after--li">copy data to NIC/device data transfer registers</li><li name="c03e" id="c03e" class="graf graf--li graf-after--li">read status register to confirm that copied data was successfully transmitted</li><li name="0dd4" id="0dd4" class="graf graf--li graf-after--li">repeat the last two steps as many times as needed to transmit the full packet, until the end-of-packet is reached.</li></ul><p name="c828" id="c828" class="graf graf--p graf-after--li">c. Direct Memory Access: CPU directly write command to device registers but data is written from the memory based on the DMA controller.</p><p name="918c" id="918c" class="graf graf--p graf-after--p">d. DMA will be used to transfer data. So we perform 1 store instruction and 1 DMA configuration instead of many times CPU accesses based on the size of the transferred data.</p><p name="fe11" id="fe11" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 3–3. Inode</strong></p><p name="d6f0" id="d6f0" class="graf graf--p graf-after--p">Assume an inode has the following structure:</p><figure name="3b59" id="3b59" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*u0TSj9UrHePnHnkAw8xj3A.png" data-width="1298" data-height="580" src="https://cdn-images-1.medium.com/max/800/1*u0TSj9UrHePnHnkAw8xj3A.png"></figure><p name="c603" id="c603" class="graf graf--p graf-after--figure">Also, assume that each block pointer element is 4 bytes.</p><p name="8ca9" id="8ca9" class="graf graf--p graf-after--p">If a block on the disk is 4 kB, then what is the maximum file size that can be supported by this inode structure?</p><p name="d5aa" id="d5aa" class="graf graf--p graf-after--p">Solution:</p><pre name="d3ae" id="d3ae" class="graf graf--pre graf-after--p"><br>Block size = 4 kB<br>Pointers in a block = 4 kB / 4 B = 1024 pointers</pre><pre name="68c9" id="68c9" class="graf graf--pre graf-after--pre">// For the single indirect pointer layers,<br>Size1 = Pointers in a block * Block size = 4 * 1024 kB</pre><pre name="2af8" id="2af8" class="graf graf--pre graf-after--pre">// For the double indirect pointer layers,<br>Size2 = (Pointers in a block)^2 * Block size = 4 * (1024)^2 kB</pre><pre name="2174" id="2174" class="graf graf--pre graf-after--pre">// For the triple indirect pointer layers,<br>Size3 = (Pointers in a block)^3 * Block size = 4 * (1024)^3 kB</pre><pre name="f211" id="f211" class="graf graf--pre graf-after--pre graf--trailing">// In total, approximately<br>Size = Size1 + Size2 + Size3 = 4 * (1024)^3 kB <br>= 4 * (1024)^2 MB <br>= 4 * 1024 GB<br>= 4 TB</pre></div></div></section><section name="2b2a" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="13b6" id="13b6" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong">4. RPC and DFS</strong></p><p name="f194" id="f194" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Consistency Models</strong></p><ul class="postList"><li name="1324" id="1324" class="graf graf--li graf-after--p">Strict: All writes are in strict order</li><li name="062c" id="062c" class="graf graf--li graf-after--li">Sequential: All writes are in the same order</li><li name="0c0b" id="0c0b" class="graf graf--li graf-after--li">Causal: Some writes are in the order implied by some reads</li><li name="6fb6" id="6fb6" class="graf graf--li graf-after--li">Weak: No specific order or writes</li></ul><p name="1798" id="1798" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Example 4-1. XDR Complier</strong></p><p name="f7a6" id="f7a6" class="graf graf--p graf-after--p">An RPC routine <code class="markup--code markup--p-code">get_coordinates()</code> returns the N-dimensional coordinates of a data point, where each coordinate is an integer. Write the elements of the C data structure that corresponds to the 3D coordinates of a data point.</p><p name="5cc8" id="5cc8" class="graf graf--p graf-after--p">Solution:</p><p name="e520" id="e520" class="graf graf--p graf-after--p">Because N is not specified, we have to use the variable-length array data type in the XDR file, and it should be defined by,</p><pre name="4bb1" id="4bb1" class="graf graf--pre graf-after--p">int coordinate&lt;M&gt;;</pre><p name="2ec3" id="2ec3" class="graf graf--p graf-after--pre">where <code class="markup--code markup--p-code">M</code> is the maximum value of N (means N ≤ M).</p><p name="7c89" id="7c89" class="graf graf--p graf-after--p">When this is compiled in C, this will translate into a data structure that has two fields, an integer <code class="markup--code markup--p-code">len</code> that corresponds to the actual size of this array and a pointer <code class="markup--code markup--p-code">val</code> that is the address of where the data in this array is actually stored.</p><pre name="2d4c" id="2d4c" class="graf graf--pre graf-after--p">struct coordinate {<br>    int len;<br>    int *val;<br>};</pre><p name="5ba6" id="5ba6" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Example 4-2. DFS Semantics</strong></p><p name="7009" id="7009" class="graf graf--p graf-after--p">Consider the following timeline where ‘f’ is distributed shared file and P1 and P2 are processes:</p><figure name="e77e" id="e77e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HgNFua0NL442d7X6Q3HVqA.png" data-width="1069" data-height="194" src="https://cdn-images-1.medium.com/max/800/1*HgNFua0NL442d7X6Q3HVqA.png"></figure><p name="6aae" id="6aae" class="graf graf--p graf-after--figure">Assume the original content of file <code class="markup--code markup--p-code">f</code> was <code class="markup--code markup--p-code">&#39;a&#39;</code>. For each of the following DFS semantics, what will be read by P2 when t = 4s?</p><ul class="postList"><li name="8403" id="8403" class="graf graf--li graf-after--p">UNIX semantics</li><li name="e57e" id="e57e" class="graf graf--li graf-after--li">NFS semantics</li><li name="ef49" id="ef49" class="graf graf--li graf-after--li">Sprite semantics</li></ul><p name="282c" id="282c" class="graf graf--p graf-after--li">Solution:</p><p name="d8ee" id="d8ee" class="graf graf--p graf-after--p">Unix works by updating immediately to the server. the second something happens in P1 or P2, everyone knows about it. So for UNIX semantics, we will read <code class="markup--code markup--p-code">&#39;ab&#39;</code>.</p><p name="fe78" id="fe78" class="graf graf--p graf-after--p">For NFS (session semantics + periodic update), it works is that it updates the server once the writer for P1 is closed and when we open a file, we will directly read from the memory. So we will read <code class="markup--code markup--p-code">&#39;a&#39;</code>.</p><p name="2bdb" id="2bdb" class="graf graf--p graf-after--p">For Sprite semantics, we will open the file in the process that is currently writing. Therefore, it will read <code class="markup--code markup--p-code">&#39;ab&#39;</code>.</p><p name="eec2" id="eec2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 4-3. Causal Consistency</strong></p><p name="ef29" id="ef29" class="graf graf--p graf-after--p">Consider the following sequence of operations by processors P1, P2, and P3 which occurred in a distributed shared memory system:</p><figure name="6a3a" id="6a3a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0DeRP-FXlB89ZYGn4Ve36g.png" data-width="1069" data-height="194" src="https://cdn-images-1.medium.com/max/800/1*0DeRP-FXlB89ZYGn4Ve36g.png"></figure><p name="3b92" id="3b92" class="graf graf--p graf-after--figure">Answer the following questions:</p><p name="cd80" id="cd80" class="graf graf--p graf-after--p">a. Name all processors (P1, P2, or P3) that observe causally consistent reads.</p><p name="b766" id="b766" class="graf graf--p graf-after--p">b. Is this execution causally consistent?</p><p name="d7a3" id="d7a3" class="graf graf--p graf-after--p">Solution:</p><p name="7e26" id="7e26" class="graf graf--p graf-after--p">a. P1 and P2. W_m1(3)@P1 and W_m1(2)@ P2 are causally related (via R_m1(2) @P1), so these two writes cannot be seen in reverse order, as currently seen by P3. P3 doesn’t observe causally consistent reads.</p><p name="27f7" id="27f7" class="graf graf--p graf-after--p">b. No. P3 doesn’t follow causal consistency. We can not read 2 from m1 after we read 3 from it because of the causal consistency between P1 and p2.</p><p name="4258" id="4258" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 4-4. Distributed Applications</strong></p><p name="30c5" id="30c5" class="graf graf--p graf-after--p">You are designing the new image datastore for an application that stores users’ images (like <a href="http://picasa.google.com/" data-href="http://picasa.google.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Picasa</a>). The new design must consider the following scale:</p><ul class="postList"><li name="71ac" id="71ac" class="graf graf--li graf-after--p">The current application has 30 million users</li><li name="d3d0" id="d3d0" class="graf graf--li graf-after--li">Each user has on average 2,000 photos</li><li name="c348" id="c348" class="graf graf--li graf-after--li">Each photo is on average 500 kB</li><li name="7013" id="7013" class="graf graf--li graf-after--li">Requests are evenly distributed across all images</li></ul><p name="6bf6" id="6bf6" class="graf graf--p graf-after--li">Answer the following:</p><p name="5751" id="5751" class="graf graf--p graf-after--p">a. Would you use replication or partitioning as a mechanism to ensure high responsiveness of the image store?</p><p name="23bb" id="23bb" class="graf graf--p graf-after--p">b. If you have 10 server machines at your disposal, and one of them crashes, what’s the percentage of requests that the image store will not be able to respond to, if any?</p><p name="01e9" id="01e9" class="graf graf--p graf-after--p">Solution:</p><p name="7711" id="7711" class="graf graf--p graf-after--p">a. Partitioning. The current data set is ~30 PB, so cannot store it on one machine</p><p name="4463" id="4463" class="graf graf--p graf-after--p">b. 10% since requests are evenly distributed on 10 server machines.</p><p name="b810" id="b810" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 4-5. Weak Consistency</strong></p><p name="691c" id="691c" class="graf graf--p graf-after--p">Let’s suppose we have the following sequence of operations by processors P1, P2, and P3,</p><figure name="f3f4" id="f3f4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CxSvB2BYW7Ey_COH4mrYtQ.png" data-width="1452" data-height="214" src="https://cdn-images-1.medium.com/max/800/1*CxSvB2BYW7Ey_COH4mrYtQ.png"></figure><p name="82db" id="82db" class="graf graf--p graf-after--figure">Then answer the following questions,</p><p name="11df" id="11df" class="graf graf--p graf-after--p">a. Consider the following sequence of operations. Is this execution weakly consistent? (Yes/No) ______________</p><p name="7e50" id="7e50" class="graf graf--p graf-after--p">b. If you ignore the sync operations, is this execution causal consistency? (Yes/No) ______________</p><p name="4051" id="4051" class="graf graf--p graf-after--p">Solution:</p><p name="015e" id="015e" class="graf graf--p graf-after--p">a. Yes. This is weakly consistent.</p><p name="5a5a" id="5a5a" class="graf graf--p graf-after--p">b. No. There will not be any causal consistency because only one process has the operation order.</p><p name="372d" id="372d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 4-6. Consistency Models</strong></p><p name="9e91" id="9e91" class="graf graf--p graf-after--p">Choose the correct consistency model that defines the following conditions:</p><p name="2b8a" id="2b8a" class="graf graf--p graf-after--p">I. All Writes are propagated to other processes, and all Writes done elsewhere are brought locally, at a sync instruction.</p><p name="c754" id="c754" class="graf graf--p graf-after--p">II. Accesses to sync variables are sequentially consistent</p><p name="b6e0" id="b6e0" class="graf graf--p graf-after--p">III. Access to sync variable is not permitted unless all Writes elsewhere have completed</p><p name="4a2e" id="4a2e" class="graf graf--p graf-after--p">IV. No data access is allowed until all previous synchronization variable accesses have been performed</p><ul class="postList"><li name="14d2" id="14d2" class="graf graf--li graf-after--p">A. Weak consistency</li><li name="72bc" id="72bc" class="graf graf--li graf-after--li">B. Causal consistency</li><li name="5285" id="5285" class="graf graf--li graf-after--li">C. Sequential consistency</li><li name="5177" id="5177" class="graf graf--li graf-after--li">D. Strict consistency</li></ul><p name="6ce1" id="6ce1" class="graf graf--p graf-after--li">Solution: A. Weak consistency</p><p name="d6d8" id="d6d8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 4-7. DFS Caching</strong></p><p name="5ce5" id="5ce5" class="graf graf--p graf-after--p">Caching is an important optimization technique. Describe the three potential locations for caching in a distributed file system. Briefly discuss the advantages and disadvantages of each location.</p><p name="037c" id="037c" class="graf graf--p graf-after--p">Solution:</p><ul class="postList"><li name="3404" id="3404" class="graf graf--li graf-after--p">Client memory/client buffer cache: for the regular file system, fast but small</li><li name="ee17" id="ee17" class="graf graf--li graf-after--li">Client storage device (HDD/SSD): slow but large</li><li name="7d6e" id="7d6e" class="graf graf--li graf-after--li">Buffer cache in memory on server: the hit rate depends on how many clients are accessing the server and how many of them are interleaved</li></ul><p name="54c0" id="54c0" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Example 4-8. Stated Server Vs. Stateless Server</strong></p><p name="0747" id="0747" class="graf graf--p graf-after--p">Answer the following questions:</p><p name="7583" id="7583" class="graf graf--p graf-after--p">a. What are the advantages of a stateless distributed file server?</p><p name="b5c7" id="b5c7" class="graf graf--p graf-after--p">b. What are the advantages of a stated distributed file server?</p><p name="df51" id="df51" class="graf graf--p graf-after--p">c. What are the disadvantages of a stateless distributed file server?</p><p name="6a4f" id="6a4f" class="graf graf--p graf-after--p">d. What are the disadvantages of a stated distributed file server?</p><p name="617e" id="617e" class="graf graf--p graf-after--p">Solution:</p><pre name="7553" id="7553" class="graf graf--pre graf-after--p">a.<br>• Stateless server is more robust than stateful file server, hence they provide fault tolerance. <br>• Lost connections can&#39;t leave a file in an invalid state. <br>• Rebooting the server does not lose state information. <br>• Rebooting the client does not confuse a stateless server.</pre><pre name="b2ca" id="b2ca" class="graf graf--pre graf-after--pre">b. <br>• It is simple. <br>• Fewer disk accesses are needed at server side. <br>• Server knows if the file was opened for sequential access, and hence can read ahead the next blocks. <br>• It reduces the network traffic as client need not send state information with every call to server.</pre><pre name="d1eb" id="d1eb" class="graf graf--pre graf-after--pre">c. <br>• As the server doesn&#39;t maintain any state information about file processing activity, it cannot detect and discard duplicate requests from client. <br>• To ensure consistency of files, the requests made by clients must be idempotent. <br>• The stateless file server incurs performance penalty due to: <br>    • The file server opens the file at every file operation. <br>    • When client performs write operation, the changes are needed to be reflected in disk copy of the file. <br>    • Stateless file server cannot employ file caching or disk caching.</pre><pre name="fad3" id="fad3" class="graf graf--pre graf-after--pre">d.<br>• When the client crashes, the file processing activity should be abandoned, and the file state should have to be restored to its previous consistent state, so that the client can restart its file processing activity. <br>• The resources allocated to aborted file processing should have to be released. <br>• When the server crashes, the state information stored in file server metadata is lost, hence the file processing activity should be terminated. Thus client knows about server crashes.</pre><p name="efb1" id="efb1" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Example 4–9. RPC Details</strong></p><p name="4192" id="4192" class="graf graf--p graf-after--p">Answer the following questions:</p><p name="077d" id="077d" class="graf graf--p graf-after--p">a. If the caller machine (client) in an RPC crashes while waiting for a reply, we say that the computation which was initiated on the callee machine is an “orphan”. Properly terminating orphan computations is called the orphan elimination problem. What sort of issues makes orphan elimination difficult in RPC systems?</p><p name="3cf3" id="3cf3" class="graf graf--p graf-after--p">b. Under what circumstances might an RPC-based program be less efficient than a carefully optimized message-based program?</p><p name="fbbb" id="fbbb" class="graf graf--p graf-after--p">Solution:</p><p name="f444" id="f444" class="graf graf--p graf-after--p">a. When we have an orphan, the RPC runtime will not be able to provide some better understandings of what’s going on. And they will provide a new type of error notification that tries to capture what goes wrong with an RPC request without claiming to provide the exact detail. Thus, we have to manually look into the log files or binary files to find out what goes wrong. This process of locating error details will be costly and this will also make orphan elimination difficult.</p><p name="4fee" id="4fee" class="graf graf--p graf-after--p graf--trailing">b. Message-based program allows sending all data in a single payload and able to finish in one call. So there is no need to wait. In this situation, it is better.</p></div></div></section><section name="31ba" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="3654" id="3654" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong">5. Virtualization</strong></p><p name="e5c3" id="e5c3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 5-1. Virtualization Examples</strong></p><p name="9a11" id="9a11" class="graf graf--p graf-after--p">Fill in the following blanks with which virtualization model they are by bare-metal virtualization (B, aka. Type I hypervisor) or hosted virtualization (H, aka. Type II hypervisor).</p><ul class="postList"><li name="81cf" id="81cf" class="graf graf--li graf-after--p">Microsoft Hyper-V ____________</li><li name="2516" id="2516" class="graf graf--li graf-after--li">Citrix Xen ______________</li><li name="32d2" id="32d2" class="graf graf--li graf-after--li">VMware ESX ____________</li><li name="0a0c" id="0a0c" class="graf graf--li graf-after--li">Kernel-based VM (KVM) _____________</li><li name="d69a" id="d69a" class="graf graf--li graf-after--li">Oracle VM for x86 ____________</li><li name="dd28" id="dd28" class="graf graf--li graf-after--li">VMware Fusion ____________</li><li name="a16d" id="a16d" class="graf graf--li graf-after--li">Oracle Virtual Box ______________</li></ul><p name="faa9" id="faa9" class="graf graf--p graf-after--li">Solution:</p><ul class="postList"><li name="3812" id="3812" class="graf graf--li graf-after--p">B</li><li name="5a2d" id="5a2d" class="graf graf--li graf-after--li">B</li><li name="57b2" id="57b2" class="graf graf--li graf-after--li">B</li><li name="645e" id="645e" class="graf graf--li graf-after--li">H</li><li name="256f" id="256f" class="graf graf--li graf-after--li">H</li><li name="1320" id="1320" class="graf graf--li graf-after--li">H</li><li name="14c8" id="14c8" class="graf graf--li graf-after--li">H</li></ul><p name="7775" id="7775" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Example 5–2. VMware’s x86 VMM</strong></p><p name="5744" id="5744" class="graf graf--p graf-after--p">Which of the following statements about VMware’s x86 virtual machine monitor are true or false? Explain.</p><ul class="postList"><li name="1b03" id="1b03" class="graf graf--li graf-after--p">The x86 architecture originally contained instructions that were non-virtualizable using trap-and-emulate virtualization.</li><li name="3cb7" id="3cb7" class="graf graf--li graf-after--li">Shadow page tables store the mappings from guest physical to host physical memory addresses.</li><li name="0c17" id="0c17" class="graf graf--li graf-after--li">VMware’s VMM prevents the guest from accessing the memory used for the VMM’s internal state by unmapping the corresponding entries in the guest’s page table.</li><li name="e0e1" id="e0e1" class="graf graf--li graf-after--li">VMware’s binary translation-based VMM translates guest kernel-mode instructions but not guest user-mode instructions.</li><li name="95f5" id="95f5" class="graf graf--li graf-after--li">A monitor using hardware-assisted virtualization (Intel’s VT-x or AMD-V) is always faster than one using binary translation.</li></ul><p name="e708" id="e708" class="graf graf--p graf-after--li">Solution:</p><ul class="postList"><li name="a217" id="a217" class="graf graf--li graf-after--p">True. For example, the <code class="markup--code markup--li-code">popf</code> instruction in x86 was originally non-virtualizable using trap-and-emulate.</li><li name="0f46" id="0f46" class="graf graf--li graf-after--li">False. Shadow page tables store the mappings from guest virtual memory to host physical memory addresses.</li><li name="21d9" id="21d9" class="graf graf--li graf-after--li">False. Permission bits in page tables do not allow for execute-only mappings, which are needed for the Translation Cache. The VMM’s own data must be accessible to translated code but not the original guest instructions, even though both run at the same privilege level. Thus, memory segmentation is used to prevent the guest from accessing the VMM’s internal state.</li><li name="92cb" id="92cb" class="graf graf--li graf-after--li">True. Why we use binary translation-based VMM is because we want to trap to the kernel, and only the kernel instructions need to conduct operations like that.</li><li name="00d5" id="00d5" class="graf graf--li graf-after--li graf--trailing">False. Binary translation can also have some optimizations that will result in better performance.</li></ul></div></div></section><section name="4af9" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="2220" id="2220" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong">6. IPC</strong></p><p name="1cae" id="1cae" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 6-1. Shared-Memory IPC</strong></p><p name="5f31" id="5f31" class="graf graf--p graf-after--p">Answer the following questions,</p><p name="d6c9" id="d6c9" class="graf graf--p graf-after--p">a. What’s the difference between a shared memory id (<code class="markup--code markup--p-code">shmid</code>) and a shared memory key?</p><p name="9494" id="9494" class="graf graf--p graf-after--p">b. How does the kernel actually implement shared memory segments?</p><p name="4094" id="4094" class="graf graf--p graf-after--p">Solution:</p><p name="6016" id="6016" class="graf graf--p graf-after--p">a.</p><p name="06ba" id="06ba" class="graf graf--p graf-after--p">If provided that certain limits of the segments are met, then it assigns to it a <code class="markup--code markup--p-code">shmid</code>. This key is used to uniquely identify the segment within the OS, and any other process can refer to this particular segment using this shared memory id.</p><p name="0bef" id="0bef" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">key</code> refers to the value which is given in reference to IPC resources like shared memory, message queues, and semaphores.</p><p name="4a2a" id="4a2a" class="graf graf--p graf-after--p">b.</p><p name="8ff7" id="8ff7" class="graf graf--p graf-after--p graf--trailing">The kernel supports segments of the shared memory that don’t need to be contiguous physical pages. These segments are system-wide and there is a limit for the total number of these segments. In today’s Linux system, this is not an issue because we can use up to 4,000 segments. However, in the past, we can only use 6 segments. When a process requests that a shared memory segment is created, the OS allocates the required amount of physical memory.</p></div></div></section><section name="2e58" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="4c4d" id="4c4d" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong">Final Example. Potpourri</strong></p><p name="d068" id="d068" class="graf graf--p graf-after--p">a. True or False</p><ul class="postList"><li name="e00a" id="e00a" class="graf graf--li graf-after--p">Test-and-set instruction used for synchronization needs to be performed in the kernel mode.</li><li name="a4a8" id="a4a8" class="graf graf--li graf-after--li">Marshaling is the process by which Byzantine Generals are forced into making good decisions.</li><li name="9880" id="9880" class="graf graf--li graf-after--li">A Remote Procedure Call (RPC) can be used to call a procedure in another process on the same machine.</li><li name="d869" id="d869" class="graf graf--li graf-after--li">With the NFS distributed file system, it is possible for one client to write a value into a file that is not seen by another client when reading that file.</li></ul><pre name="a2c2" id="a2c2" class="graf graf--pre graf-after--li graf--trailing">a. <br>F           // user mode<br>F           // the process of packaging data items<br>T<br>T</pre></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/9de01072eb4b"><time class="dt-published" datetime="2021-05-04T17:28:06.055Z">May 4, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/operating-system-29-final-review-for-special-topics-and-quizzes-9de01072eb4b" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>