<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Operating System 14 | Event-Driven Model with the Flash Server Example, Experiment Design, Metrics…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Operating System 14 | Event-Driven Model with the Flash Server Example, Experiment Design, Metrics…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Operating System
</section>
<section data-field="body" class="e-content">
<section name="5677" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="fb31" id="fb31" class="graf graf--h3 graf--leading graf--title">Operating System 14 | Event-Driven Model with the Flash Server Example, Experiment Design, Metrics, and Results</h3><figure name="9262" id="9262" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*vJ7mEO5goNEX50b2.png" data-width="1508" data-height="794" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*vJ7mEO5goNEX50b2.png"></figure><ol class="postList"><li name="8eb0" id="8eb0" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Recall: Boss-Worker Model Vs. Pipeline Model Performance</strong></li></ol><p name="7410" id="7410" class="graf graf--p graf-after--li">Let’s recall the <strong class="markup--strong markup--p-strong">boss-worker model</strong> and the <strong class="markup--strong markup--p-strong">pipeline model</strong> for multithreading systems. Suppose we have 11 tasks and 6 working threads for each model. A worker in the boss-worker model takes 120ms to complete a task, while each stage in the pipeline mode takes 20ms.</p><p name="e28c" id="e28c" class="graf graf--p graf-after--p">Let’s compare the performance of these two models.</p><p name="8d7c" id="8d7c" class="graf graf--p graf-after--p">For the boss-worker model, we have 1 boss thread and 5 worker threads. In order to handle these 11 tasks, we need at least <code class="markup--code markup--p-code">([11/5]+1) * 120 = 360ms</code>.</p><p name="8ace" id="8ace" class="graf graf--p graf-after--p">For the pipeline model, because we have to fill the pipeline at the beginning, <code class="markup--code markup--p-code">6*20 = 120 ms</code> is taken for completing the first task. Then each of the other tasks will take only 20 ms more for completion. So in conclusion, we need <code class="markup--code markup--p-code">120 + 10*20 = 320 ms</code> for finishing all the tasks.</p><p name="1361" id="1361" class="graf graf--p graf-after--p">Then the <strong class="markup--strong markup--p-strong">average execution time</strong> for these two models would be,</p><ul class="postList"><li name="14c7" id="14c7" class="graf graf--li graf-after--p">For the boss-worker model, the avg. execution time should be <code class="markup--code markup--li-code">360/11 = 32.72 ms</code></li><li name="08e3" id="08e3" class="graf graf--li graf-after--li">For the pipeline model, the avg. execution time should be <code class="markup--code markup--li-code">320/11 = 29.09 ms</code></li></ul><p name="96e9" id="96e9" class="graf graf--p graf-after--li">Now, let’s consider another performance metric, the <strong class="markup--strong markup--p-strong">completion time</strong> for each task. The completion time for a thread is defined as the time it takes from the start of the program and the completion of this task. Take the pipeline model as an example, the first task completes at 120 ms, the second task completes at 140 ms, etc.</p><p name="f9b1" id="f9b1" class="graf graf--p graf-after--p">Then the <strong class="markup--strong markup--p-strong">average completion time</strong> for these two models would be,</p><ul class="postList"><li name="4e38" id="4e38" class="graf graf--li graf-after--p">For the boss-worker model, the avg. execution time should be <code class="markup--code markup--li-code">(5*120 + 5*240 + 360)/11 = 196 ms</code></li><li name="4f9c" id="4f9c" class="graf graf--li graf-after--li">For the pipeline model, the avg. execution time should be <code class="markup--code markup--li-code">(120 + 140 + 160 + ... + 320)/11 = 220 ms</code></li></ul><p name="9655" id="9655" class="graf graf--p graf-after--li">So, when we are thinking about comparing the performance, we should specify the metrics used to evaluate the performance. In the example above, if we consider the average execution time, we should pick the pipeline model. However, if we consider the average execution time, we should pick the boss-worker model.</p><p name="aed0" id="aed0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Performance Metrics</strong></p><p name="c701" id="c701" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Reasons for Using Threads</strong></p><p name="1e0d" id="1e0d" class="graf graf--p graf-after--p">We have known that the threads are quite useful for us, and here are some reasons why these threads can be useful.</p><ul class="postList"><li name="02a5" id="02a5" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Parallelization</strong>: the threads can be used to parallelize problems so that we can gain some speedups</li><li name="cbca" id="cbca" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Hot Cache</strong>: the threads allow us to benefit from a hot cache because we can specialize what a particular thread is doing on a given CPU</li><li name="d51a" id="d51a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Efficiency</strong>: because different threads can share the same address space, the <strong class="markup--strong markup--li-strong">memory footprint</strong> for a multi-threading system is reduced. What’s more, different threads are <strong class="markup--strong markup--li-strong">easier to synchronize</strong> with each other compared with communication between the processes. The threads are useful even on the signal CPU because it allows hiding<strong class="markup--strong markup--li-strong"> the latency of the I/O</strong> operations by context switch to other threads.</li></ul><p name="be35" id="be35" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) The Definition of the Performance Metrics</strong></p><p name="763f" id="763f" class="graf graf--p graf-after--p">Ideally, metrics should be represented with values that we can measure and quantify. The definition of the term <strong class="markup--strong markup--p-strong">metrics</strong> according to Webster’s, for instance, is that it&#39;s a <strong class="markup--strong markup--p-strong">measurement standard</strong>. In our analysis of systems, a metric should be measurable, it should allow us to quantify a property of a system so that we can evaluate the system’s behavior or at least compare it to other systems.</p><p name="df15" id="df15" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) The Features of the Performance Metrics</strong></p><p name="7bde" id="7bde" class="graf graf--p graf-after--p">To check whether or not it is a metric, we have to consider the following features.</p><ul class="postList"><li name="21d6" id="21d6" class="graf graf--li graf-after--p">the metrics should be <strong class="markup--strong markup--li-strong">measurable </strong>(i.e. <strong class="markup--strong markup--li-strong">execution time</strong>)</li><li name="8f52" id="8f52" class="graf graf--li graf-after--li">if the metrics can not be measurable, they should have a <strong class="markup--strong markup--li-strong">quantifiable property</strong></li><li name="ff82" id="ff82" class="graf graf--li graf-after--li">the metrics are associated with the <strong class="markup--strong markup--li-strong">system </strong>(i.e. <strong class="markup--strong markup--li-strong">implementation</strong> of a particular problem)</li><li name="effd" id="effd" class="graf graf--li graf-after--li">the metrics can be used to<strong class="markup--strong markup--li-strong"> evaluate the system behavior</strong> (i.e. the <strong class="markup--strong markup--li-strong">utilization</strong> <strong class="markup--strong markup--li-strong">improvement</strong> of the hardware compared with the others)</li></ul><p name="d4d5" id="d4d5" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Examples of Performance Metrics</strong></p><p name="06f5" id="06f5" class="graf graf--p graf-after--p">So far in the lesson, we mentioned several useful metrics. For instance,</p><ul class="postList"><li name="54ae" id="54ae" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Execution time</strong></li><li name="c265" id="c265" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Throughput</strong></li><li name="ba14" id="ba14" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Request rate</strong></li><li name="98c2" id="98c2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">CPU utilization</strong></li></ul><p name="7fa8" id="7fa8" class="graf graf--p graf-after--li">But there are also many other useful metrics to consider,</p><ul class="postList"><li name="ffbf" id="ffbf" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Wait time</strong>: the user may not just care when they will get an answer, but they may also care when their job will actually start being executed</li><li name="9530" id="9530" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Platform efficiency</strong>: although throughput helps evaluate the utility of a platform, if we are the owners of a data center, we also care bout the platform efficiency. The reason for this is that we have to spend money to run the machines, to buy more servers, so it&#39;s important to have a good ratio, so the platform efficiently will capture that.</li><li name="2995" id="2995" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Performance per dollar</strong>: if we are considering buying the next greatest hardware platform, then we can think about whether the cost that we will pay extra for that new piece of hardware will be compensated with some impact on the performance that we are seeing</li><li name="10b5" id="10b5" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Performance per watt</strong>: we are also concerned about the amount of power (watts) that can be delivered to a particular platform or the energy that will be consumed during the execution. This is because we have to pay for the power of running the machine</li><li name="09c8" id="09c8" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Percentage of SLA</strong> (<strong class="markup--strong markup--li-strong">Service Level Agreement</strong>) <strong class="markup--strong markup--li-strong">violations</strong>: because enterprise applications will give typically SLAs to their customers. This is an agreement like “you will get a response within 3 seconds”. Sometimes, the SLA for the enterprise can be more subtle than that. For example, a service like Expedia has an SLA with its customers (i.e. Delta Airlines, American Airlines) that it will provide the most accurate quote for 95% of the flights that are being returned to customers. Thus, a metric like the percentage of SLA violations would capture that information.</li><li name="4bcd" id="4bcd" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Slack in the application</strong>: for instance, humans cannot perceive more than 30 fps (frames per second), so making sure that there are at least 30 fps so that the users don’t start seeing some random buffering pauses during the video</li><li name="8867" id="8867" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Client-perceived Performance</strong>: except for the request rate and the wait time, a metric that is really concerned about is the client-perceived performance (the service is performing well or not).</li><li name="15f3" id="15f3" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Aggregate performance</strong>: the aggregate performance metric tries to average the execution time for all tasks or average the wait time for all tasks, or maybe even this would be a weighted average based on the priorities of the tasks</li><li name="438c" id="438c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Resource usage </strong>or <strong class="markup--strong markup--li-strong">average resource usage</strong>: similar to the CPU utilization, there are a number of other resources that we may be concerned about, memory, file system, so the storage subsystem</li></ul><p name="2cdb" id="2cdb" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(5) Obtain Measurable Quantity of the Metrics</strong></p><p name="cf53" id="cf53" class="graf graf--p graf-after--p">Ideally, we will obtain these metrics or gather these measurements by <strong class="markup--strong markup--p-strong">running experiments using real software deployments on the real machines using real workloads</strong>.</p><p name="dc1b" id="dc1b" class="graf graf--p graf-after--p">However, sometimes that really not an option, we cannot wait to actually deploy the software before we start measuring something about it or analyzing its behavior. In those cases, we have to <strong class="markup--strong markup--p-strong">resort to experimentation with some representative configurations that in some way mimic as much as possible </strong>(i.e. on the workload, the machine type, the aspects of the real system). This can be realized by using some <strong class="markup--strong markup--p-strong">simulation</strong> software.</p><p name="d9ff" id="d9ff" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) The Definition of Testbed</strong></p><p name="4407" id="4407" class="graf graf--p graf-after--p">If we choose to simulate the metrics, it requires viable settings where one can evaluate a system and gather some performance metrics about it. We refer to these experimental settings as a <strong class="markup--strong markup--p-strong">testbed</strong>. So the testbed tells us where were the experiments carried out and what were the relevant metrics that were measured.</p><p name="ab80" id="ab80" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) More Thoughts on the Reasons for Using Threads</strong></p><p name="85e2" id="85e2" class="graf graf--p graf-after--p">So now, if we go back now to our question about the reasons for using threads, the answer can not be that simple at this time. When we are considering whether or not threads are useful, we have to think about the metrics we took into consideration. Whether or not using threads may depend on,</p><ul class="postList"><li name="ff78" id="ff78" class="graf graf--li graf-after--p">metrics</li><li name="1ee5" id="1ee5" class="graf graf--li graf-after--li">workload (different number of tasks)</li><li name="590a" id="590a" class="graf graf--li graf-after--li">algorithms we use (for graph processing)</li><li name="c452" id="c452" class="graf graf--li graf-after--li">file systems (for handling file patterns)</li><li name="a586" id="a586" class="graf graf--li graf-after--li">etc.</li></ul><p name="054f" id="054f" class="graf graf--p graf-after--li">Thus, whether or not threads are useful really depends on the context we made.</p><p name="0c34" id="0c34" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Example: Flash Web Server Implementation</strong></p><p name="0ef4" id="0ef4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Methods for Providing Concurrency</strong></p><p name="c902" id="c902" class="graf graf--p graf-after--p">To understand when threads are useful, let’s start by thinking about what are the different ways to provide concurrency. So far we have discussed two methods to provide the concurrency operations,</p><ul class="postList"><li name="48e7" id="48e7" class="graf graf--li graf-after--p">multiple threads (MT)</li><li name="b23c" id="b23c" class="graf graf--li graf-after--li">multiple processes (MP)</li></ul><p name="3afd" id="3afd" class="graf graf--p graf-after--li">In addition, for this part, we will introduce a new method for providing concurrency,</p><ul class="postList"><li name="f205" id="f205" class="graf graf--li graf-after--p">event-driven model (EDM)</li></ul><p name="90fd" id="90fd" class="graf graf--p graf-after--li">Our discussion will be based on the implementation of a simple webserver Flash and it is a must for us to refer to the paper <a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" data-href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Flash: An Efficient and Portable Web Server</em></a> by Vivek. We use the example of a web server because it is important to be able to concurrently process client requests.</p><p name="45c4" id="45c4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Working Routine of A Simple Web Server</strong></p><p name="b2b1" id="b2b1" class="graf graf--p graf-after--p">Before we continue, let’s talk about the steps involved in the implementation of a simple HTTP web server. Remember this is very similar to the GETFILE server that we have implemented for the project 1. So the steps in a simple web server are,</p><ul class="postList"><li name="463d" id="463d" class="graf graf--li graf-after--p">Web server accepts connection</li><li name="f5e0" id="f5e0" class="graf graf--li graf-after--li">Client/Browser sends request</li><li name="33ab" id="33ab" class="graf graf--li graf-after--li">Web server reads request</li><li name="374d" id="374d" class="graf graf--li graf-after--li">Web server parse request</li><li name="f28d" id="f28d" class="graf graf--li graf-after--li">Web server find the file requested</li><li name="ca27" id="ca27" class="graf graf--li graf-after--li">Web server computes respond header</li><li name="2765" id="2765" class="graf graf--li graf-after--li">Web server sends header</li><li name="3277" id="3277" class="graf graf--li graf-after--li">Web server reads file and sends data repeatedly until data completely transfered</li><li name="5770" id="5770" class="graf graf--li graf-after--li">End of service</li></ul><p name="1c22" id="1c22" class="graf graf--p graf-after--li">The basic sequential steps for serving a request for static content are also illustrated and described by <strong class="markup--strong markup--p-strong">Figure 1</strong> in the Flash paper by Vivek Pai,</p><figure name="7392" id="7392" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Oe-AIOW3U3YDfzQT88SSgg.png" data-width="1236" data-height="185" src="https://cdn-images-1.medium.com/max/800/1*Oe-AIOW3U3YDfzQT88SSgg.png"><figcaption class="imageCaption"><a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" data-href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank"><em class="markup--em markup--figure-em">Flash: An Efficient and Portable Web Server</em></a><em class="markup--em markup--figure-em"> Figure 1</em></figcaption></figure><p name="540b" id="540b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Multi-Process (MP) Web Server</strong></p><p name="3217" id="3217" class="graf graf--p graf-after--p">One easy way to achieve concurrency is to have multiple instances of the same process and that way we have a <strong class="markup--strong markup--p-strong">multi-process (MP)<em class="markup--em markup--p-em"> </em></strong>implementation. This implementation is also illustrated by Vivek Pai’s paper,</p><figure name="4e92" id="4e92" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gqOhvsPSY9PNcBKysFDbrw.png" data-width="1318" data-height="500" src="https://cdn-images-1.medium.com/max/800/1*gqOhvsPSY9PNcBKysFDbrw.png"><figcaption class="imageCaption"><a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" data-href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank"><em class="markup--em markup--figure-em">Flash: An Efficient and Portable Web Server</em></a><em class="markup--em markup--figure-em"> Figure 2</em></figcaption></figure><p name="242b" id="242b" class="graf graf--p graf-after--figure">The benefit of this approach is that it is <strong class="markup--strong markup--p-strong">simple</strong> to implement because we can directly fork the initial process to spawn more processes once we developed the sequential steps for one process.</p><p name="0c30" id="0c30" class="graf graf--p graf-after--p">However, the downside of this method is that we have to allocate memory for every one of the processes, and this will ultimately put a <strong class="markup--strong markup--p-strong">high load on the memory</strong> subsystem and will <strong class="markup--strong markup--p-strong">hurt performance</strong>. Also, there will be a <strong class="markup--strong markup--p-strong">high cost of context switching</strong> among processes. In addition, it can be rather <strong class="markup--strong markup--p-strong">expensive to maintain a shared state</strong> across processes because of the communication mechanisms and the synchronization mechanisms that are available across processes.</p><p name="eac4" id="eac4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Multi-Threaded (MT) Web Server</strong></p><p name="5556" id="5556" class="graf graf--p graf-after--p">An alternative to the multi-process model is to develop the web server as a <strong class="markup--strong markup--p-strong">multi-threaded (MT)</strong> application. Again, the following illustration is taken from Figure 3 of Pai’s Flash paper,</p><figure name="d596" id="d596" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Z3E15canDlw32SuybWzSYQ.png" data-width="1318" data-height="362" src="https://cdn-images-1.medium.com/max/800/1*Z3E15canDlw32SuybWzSYQ.png"><figcaption class="imageCaption"><a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" data-href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank"><em class="markup--em markup--figure-em">Flash: An Efficient and Portable Web Server</em></a><em class="markup--em markup--figure-em"> Figure 3</em></figcaption></figure><p name="0a50" id="0a50" class="graf graf--p graf-after--figure">Basically, we have two approaches to implement this MT web server. One is to treat all the threads <strong class="markup--strong markup--p-strong">equally as workers</strong>, the other is to use a <strong class="markup--strong markup--p-strong">boss-worker model</strong>.</p><p name="059d" id="059d" class="graf graf--p graf-after--p">As it is described in the paper, every single one of the threads executes all the steps, starting from the accept connection call to completely sending the file. Thus, we will then have <strong class="markup--strong markup--p-strong">multiple execution contexts</strong> for multiple threads within the <strong class="markup--strong markup--p-strong">same address space</strong>, so every single one of the threads is processing a request.</p><p name="f87b" id="f87b" class="graf graf--p graf-after--p">Another approach to implementing this multi-threaded web server is by a <strong class="markup--strong markup--p-strong">boss-worker model</strong> where a single boss thread performs the accept connection and every single one of the worker threads performs the remaining operations, from reading the request to completely sending the file.</p><p name="8083" id="8083" class="graf graf--p graf-after--p">The benefits of this method are,</p><ul class="postList"><li name="0f88" id="0f88" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">memory efficiency</strong>: because threads share the same address space, the memory requirement will be much lower compared with the MP method</li><li name="1431" id="1431" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">shared states</strong>: threads don’t have to perform system calls in order to coordinate with other threads as the MP method does</li><li name="39db" id="39db" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">cheap context switch</strong>: because the context switching can be done at the multi-threading library level, it is relatively cheap for us</li></ul><p name="f441" id="f441" class="graf graf--p graf-after--li">However, there are also some downsides for this method,</p><ul class="postList"><li name="f16b" id="f16b" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">complicated for implementation</strong>: the downside of the approach is that it is not so simple and straightforward to implement the MT program</li><li name="5932" id="5932" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">needs synchronization</strong>: also, synchronization tools (i.e. mutex and condition variables) are needed when different threads are accessing/updating the shared address space or the shared states</li><li name="d75f" id="d75f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">needs OS’s underlying supports on threads</strong>: although all of the modern OS are regularly multi-threaded, it is not the case back to the time of the Flash paper (i.e. FreeBSD 2.2.6 at that time only supports ULTs without KLTs)</li></ul><p name="4b23" id="4b23" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(5) Single-Process Event-Driven (SPED) Model Web Server</strong></p><p name="e2a4" id="e2a4" class="graf graf--p graf-after--p">Now let’s talk about an alternative model for structuring server applications that perform concurrent processing. The model we’ll talk about is called the <strong class="markup--strong markup--p-strong">event-driven model</strong>. The application is implemented in a <strong class="markup--strong markup--p-strong">single address space</strong>, there is basically only a <strong class="markup--strong markup--p-strong">single process</strong> and a <strong class="markup--strong markup--p-strong">single thread</strong> of control. Then it is called a <strong class="markup--strong markup--p-strong">single-process event-driven model (SPED)</strong>. Here is the illustration of this model and this is taken from Vivek Pai’s Flash paper as well,</p><figure name="010a" id="010a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*V3YtKXOs3or8pdly8JwsEQ.png" data-width="1318" data-height="350" src="https://cdn-images-1.medium.com/max/800/1*V3YtKXOs3or8pdly8JwsEQ.png"><figcaption class="imageCaption"><a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" data-href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank"><em class="markup--em markup--figure-em">Flash: An Efficient and Portable Web Server</em></a><em class="markup--em markup--figure-em"> Figure 4</em></figcaption></figure><p name="2182" id="2182" class="graf graf--p graf-after--figure">Now, let’s then analyze some of the main components of the SPEDM,</p><ul class="postList"><li name="a53d" id="a53d" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Event Dispatcher</strong></li></ul><p name="dbfb" id="dbfb" class="graf graf--p graf-after--li">The main part of the process is an <strong class="markup--strong markup--p-strong">event dispatcher</strong> that loopingly looks for the incoming events. Then based on those events, the dispatcher invokes one or more event handlers. The events including things like the receipt of a request, the completion of send, the completion of a disk read operation, and etc.</p><p name="32e1" id="32e1" class="graf graf--p graf-after--p">The dispatcher has the ability to accept any of these types of <strong class="markup--strong markup--p-strong">notifications</strong>. And then based on the notification type, invoke the <strong class="markup--strong markup--p-strong">appropriate handler</strong>. So in that sense, it operates very much like a state machine.</p><ul class="postList"><li name="8b3e" id="8b3e" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Event Handlers</strong></li></ul><p name="fb8d" id="fb8d" class="graf graf--p graf-after--li">The event handlers are called after the dispatcher receives a particular notification. Invoking a handler for a single-threaded process simply means to jump to the appropriate location in the process’ address space where the corresponding handler is implemented. At that point, the handler execution can start.</p><figure name="de3b" id="de3b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YuYvjmFZgk5GETg4KpJ-Zw.png" data-width="1318" data-height="196" src="https://cdn-images-1.medium.com/max/800/1*YuYvjmFZgk5GETg4KpJ-Zw.png"></figure><p name="66b8" id="66b8" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(6) Concurrency For SPED</strong></p><p name="61e0" id="61e0" class="graf graf--p graf-after--p">In the MP models and MT models, to achieve concurrency, we would simply add multiple execution contexts. If we have fewer CPUs than context, then we would have to <strong class="markup--strong markup--p-strong">context switch</strong> among them.</p><p name="b7f3" id="b7f3" class="graf graf--p graf-after--p">The way that EDM achieves concurrency is by <strong class="markup--strong markup--p-strong">interleaving</strong> the processing of multiple requests <strong class="markup--strong markup--p-strong">within the same execution context</strong>.</p><p name="8804" id="8804" class="graf graf--p graf-after--p">Now let’s see an example. Suppose we have a request for client C1. After that,</p><ul class="postList"><li name="78cb" id="78cb" class="graf graf--li graf-after--p">Step 1: the connection event handler is called by the dispatcher and the accept operation gets processed.</li><li name="db06" id="db06" class="graf graf--li graf-after--li">Step 2: the actual request message gets processed and the message gets parsed</li><li name="de7e" id="de7e" class="graf graf--li graf-after--li">Step 3: the files are extracted so now we actually need to read the file. We initiate I/O from the reading-file handler</li></ul><p name="22ea" id="22ea" class="graf graf--p graf-after--li">At this moment, the request for client C1 is <strong class="markup--strong markup--p-strong">waiting on disk I/O</strong> to complete because it takes time for memory accessing.</p><figure name="af5c" id="af5c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*aNyTLsBLZNpbsgVdAlzSfw.png" data-width="1376" data-height="338" src="https://cdn-images-1.medium.com/max/800/1*aNyTLsBLZNpbsgVdAlzSfw.png"></figure><p name="8187" id="8187" class="graf graf--p graf-after--figure">Meanwhile, suppose that two more requests have come in, so client C2 and C3 have sent requests for connections. Let’s say the C2 request was picked up first, the connection was accepted, and now for the processing of C2, we need to wait for the actual HTTP message to be received. Assume the processing of C2 is waiting on the request message to be received.</p><figure name="4f20" id="4f20" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Bnq2bj3AQXUmBbAkGPYp3g.png" data-width="1376" data-height="338" src="https://cdn-images-1.medium.com/max/800/1*Bnq2bj3AQXUmBbAkGPYp3g.png"></figure><p name="2c08" id="2c08" class="graf graf--p graf-after--figure">And let’s say C3, its request has been accepted and is currently being handled, so the C3 request is in the accept-connection handler.</p><figure name="2fae" id="2fae" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*QK4jaz9D4qd1pYUqTlMVuA.png" data-width="1376" data-height="338" src="https://cdn-images-1.medium.com/max/800/1*QK4jaz9D4qd1pYUqTlMVuA.png"></figure><p name="3ff2" id="3ff2" class="graf graf--p graf-after--figure">Some amount of time later the processing of all of these three requests has moved a little bit further along. Then we may find out that the execution context turns out to be,</p><figure name="3e75" id="3e75" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vYAY2fNaCn5jyFU-IeH7Rg.png" data-width="1376" data-height="338" src="https://cdn-images-1.medium.com/max/800/1*vYAY2fNaCn5jyFU-IeH7Rg.png"></figure><p name="01c1" id="01c1" class="graf graf--p graf-after--figure">Thus, although we have only one execution context with only one thread, we can also have <strong class="markup--strong markup--p-strong">concurrent execution of multiple client requests</strong> that happen to be <strong class="markup--strong markup--p-strong">interleaved</strong>. So even if we have a SPED model, there can be multiple client requests being handled at the same time.</p><p name="6337" id="6337" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Why SPED Works?</strong></p><p name="89cc" id="89cc" class="graf graf--p graf-after--p">In the SPED, a request will be processed in the context of a single thread as long as it doesn’t have to wait. Whenever a wait needs to happen then the execution thread will switch to servicing another request. As a result, the latency for waiting is hidden.</p><figure name="a717" id="a717" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rRs2ddl3bbaNqS7vNfhljw.png" data-width="1830" data-height="478" src="https://cdn-images-1.medium.com/max/800/1*rRs2ddl3bbaNqS7vNfhljw.png"></figure><p name="214c" id="214c" class="graf graf--p graf-after--figure">If we have multiple CPUs, the event-driven model will still make sense, especially when we need to <strong class="markup--strong markup--p-strong">handle more</strong> requests than the number of CPUs.</p><p name="6648" id="6648" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) SPED Implementation</strong></p><p name="ea3e" id="ea3e" class="graf graf--p graf-after--p">We have known that both the sockets (for support network) and the files (for supporting disk I/O) are some sorts of OS abstractions, and they are actually identified by the identical data structure called <strong class="markup--strong markup--p-strong">file descriptors (FD)</strong>. Thus, any of the events in the execution context of this web server is actually an input on any of the FDs that are associated with.</p><p name="8c93" id="8c93" class="graf graf--p graf-after--p">To determine which FD has an input, the Flash paper use the <code class="markup--code markup--p-code u-paddingRight0 u-marginRight0"><a href="https://man7.org/linux/man-pages/man2/select.2.html" data-href="https://man7.org/linux/man-pages/man2/select.2.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">select</a>()</code> system call. This system call allows a program to monitor multiple file descriptors, waiting until one or more of the file descriptors become “ready” (or “input possible”) for some class of I/O operation. A file descriptor is considered ready if it is possible to perform a corresponding I/O operation without blocking.</p><p name="088a" id="088a" class="graf graf--p graf-after--p">However, there are also some other system calls that can be used to monitor multiple file descriptors, like <code class="markup--code markup--p-code u-paddingRight0 u-marginRight0"><a href="https://man7.org/linux/man-pages/man2/poll.2.html" data-href="https://man7.org/linux/man-pages/man2/poll.2.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">poll</a>()</code> or <code class="markup--code markup--p-code u-paddingRight0 u-marginRight0"><a href="https://man7.org/linux/man-pages/man7/epoll.7.html" data-href="https://man7.org/linux/man-pages/man7/epoll.7.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">epoll</a>()</code>. The problem with both the <code class="markup--code markup--p-code">select</code> and <code class="markup--code markup--p-code">poll</code> system call is that they really have to scan through a potentially long list of FDs before they can determine which FD has an input. However, commonly, very few FDs will actually have an input, so a lot of that search time will be wasted. The <code class="markup--code markup--p-code">epoll</code> system call is provided for eliminating some of the problems of <code class="markup--code markup--p-code">select</code> and <code class="markup--code markup--p-code">poll</code> , because a lot of the high-performance servers that require high data rates and low latency would use this kind of mechanism today.</p><p name="b455" id="b455" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) SPED Evaluation</strong></p><p name="126e" id="126e" class="graf graf--p graf-after--p">The benefits of the event-driven model really come from its design including,</p><ul class="postList"><li name="4aa0" id="4aa0" class="graf graf--li graf-after--p">single address space: saves the memory resource</li><li name="a237" id="a237" class="graf graf--li graf-after--li">single flow of control: this makes it easy for us to implement</li><li name="5d48" id="5d48" class="graf graf--li graf-after--li">lower overheads: no need for context switching.</li><li name="d3b6" id="d3b6" class="graf graf--li graf-after--li">no synchronization: no need for shared access to variables</li></ul><p name="80a7" id="80a7" class="graf graf--p graf-after--li">However, there’s still a problem with the SPED model. If one of the handlers issues a blocking I/O call to read data from the network or from disk, the entire event-driven process can be blocked. Let’s see how to deal with this problem.</p><p name="9ccf" id="9ccf" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) Blocking Solution 1: Asynchronous I/O Operations</strong></p><p name="a7f9" id="a7f9" class="graf graf--p graf-after--p">One way to circumvent this problem is to use asynchronous I/O operations.</p><p name="d45a" id="d45a" class="graf graf--p graf-after--p">The asynchronous I/O operations have two properties,</p><ul class="postList"><li name="f4f8" id="f4f8" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">decide how to collect and return data</strong>: when the I/O system call is made, the kernel captures enough information about the I/O operation, and where and how the data should be returned once it becomes available</li><li name="4801" id="4801" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">proceed to execute and come back later</strong>: asynchronous I/O operations calls also provide the caller with an opportunity to proceed to execute something and then come back at a later time to check if the results of the asynchronous operation are already available. For instance, the processor to a thread can come back later to check if a file has already been read and the data is available in a buffer in memory or in the buffer.</li></ul><p name="ef75" id="ef75" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(11) Asynchronous I/O Operations Requirements</strong></p><p name="7f72" id="7f72" class="graf graf--p graf-after--p">To implement synchronous I/O operations, one <strong class="markup--strong markup--p-strong">necessary</strong> requirement is that the OS <strong class="markup--strong markup--p-strong">kernel should be multi-threaded</strong>. So while the caller thread continues execution, another kernel thread does all the necessary work and all the waiting that&#39;s needed to perform the I/O operation, to get the I/O data, and then to also make sure that the results become available to the appropriate UL context.</p><p name="eb5f" id="eb5f" class="graf graf--p graf-after--p">Also, asynchronous operations also require supports from the <strong class="markup--strong markup--p-strong">actual I/O devices</strong>. For instance, the caller thread can simply pass some request data structure to the device itself, and then the device performs the operation and the thread, at a later time, can come and check to see whether the device has completed the operation.</p><p name="0a58" id="0a58" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(12) Asynchronous I/O Operations Problem</strong></p><p name="1aeb" id="1aeb" class="graf graf--p graf-after--p">The problem with asynchronous I/O calls is that they weren’t ubiquitously available in the past (especially at the time for the Flash paper), and even today they may not be available for all types of devices.</p><p name="c8a3" id="c8a3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(13) Blocking Solution 2: AMPED Architecture</strong></p><p name="e1e3" id="e1e3" class="graf graf--p graf-after--p">To deal with the blocking problem, Vivek Pai’s paper proposed the use of <strong class="markup--strong markup--p-strong">helpers</strong>. The following illustration is Figure 5 taken from the paper.</p><figure name="e3c2" id="e3c2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ridg4-P7I5y1qV11F3nEDg.png" data-width="1358" data-height="430" src="https://cdn-images-1.medium.com/max/800/1*ridg4-P7I5y1qV11F3nEDg.png"><figcaption class="imageCaption"><a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" data-href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank"><em class="markup--em markup--figure-em">Flash: An Efficient and Portable Web Server</em></a><em class="markup--em markup--figure-em"> Figure 5</em></figcaption></figure><p name="8f69" id="8f69" class="graf graf--p graf-after--figure">Instead of using a SPED model, we add some helper <strong class="markup--strong markup--p-strong">processes (or threads)</strong> that help us handle the disk operations. At the time of the writing of the paper, another limitation was that not all kernels were multi-threaded, so basically, not all kernels supported the one-to-one model that we talked about. In order to deal with this limitation, the decision in the paper was to make these helper entities processes. Then, because this model now has multiple processes, it is now called an <strong class="markup--strong markup--p-strong">asymmetric multi-process event-driven (AMPED)</strong> architecture. It is called “asymmetric” because the helper threads execute all the I/O operations while the main thread performs everything else.</p><p name="077b" id="077b" class="graf graf--p graf-after--p">When a disk operation is necessary, the event dispatcher instructs a helper process via. a <strong class="markup--strong markup--p-strong">socket-based interface</strong> or an <strong class="markup--strong markup--p-strong">inter-process communication (IPC) channel</strong> called a <strong class="markup--strong markup--p-strong">pipe</strong>. When a blocking happens for an I/O operation, the helper will be blocked, and the main event dispatcher can still be executing.</p><p name="e5a3" id="e5a3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(14) AMPED Evaluation</strong></p><p name="afea" id="afea" class="graf graf--p graf-after--p">The key benefits of the asymmetric model that we described are,</p><ul class="postList"><li name="3f76" id="3f76" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">deal with the potential blocking problem</strong> of the SPED model</li><li name="a20b" id="a20b" class="graf graf--li graf-after--li">concurrency with a <strong class="markup--strong markup--li-strong">smaller memory requirement</strong>: we can finally achieve concurrency by AMPED instead of the normal MP or MT models, where each process/thread has to perform the whole request. So the memory requirement will be much less significant.</li></ul><p name="9889" id="9889" class="graf graf--p graf-after--li">The downside are that,</p><ul class="postList"><li name="c247" id="c247" class="graf graf--li graf-after--p">The AMPED model can not be arbitrarily used for many applications (except for the server applications)</li><li name="157e" id="157e" class="graf graf--li graf-after--li">there are also some complexities with the routing of events in multi-CPU systems</li></ul><p name="3466" id="3466" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(15) Features for the Flash Server</strong></p><p name="9c36" id="9c36" class="graf graf--p graf-after--p">We will now talk about the features of the Flash paper,</p><ul class="postList"><li name="22d7" id="22d7" class="graf graf--li graf-after--p">an event-driven web server that follows the <strong class="markup--strong markup--li-strong">AMPED model</strong></li><li name="ecd5" id="ecd5" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">helper processes</strong> are used for disk reads</li><li name="a3e9" id="a3e9" class="graf graf--li graf-after--li">just returns <strong class="markup--strong markup--li-strong">static files</strong> (because this is the old fashioned web 1.0 technology)</li><li name="7cb4" id="7cb4" class="graf graf--li graf-after--li">helper-dispatcher communication via <strong class="markup--strong markup--li-strong">pipes</strong></li><li name="f917" id="f917" class="graf graf--li graf-after--li">dispatcher checks if a file in the main memory by <code class="markup--code markup--li-code">mincore</code></li><li name="1409" id="1409" class="graf graf--li graf-after--li">helper reads files from the memory by <code class="markup--code markup--li-code">mmap</code> call</li></ul><p name="6b6e" id="6b6e" class="graf graf--p graf-after--li">Although the dispatcher’s check by <code class="markup--code markup--p-code">mincore</code> seems unnecessary, it actually results in <strong class="markup--strong markup--p-strong">big savings</strong> because it prevents the full process from being blocked if it turns out that a blocking I/O operation is necessary.</p><p name="d53b" id="d53b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(16) Additional Optimization for Flash</strong></p><ul class="postList"><li name="e5de" id="e5de" class="graf graf--li graf-after--p">application-level caching: the files don’t need to be repeatedly looked up whenever we need to find the file. We also cache the header and the file path.</li><li name="a2bc" id="a2bc" class="graf graf--li graf-after--li">scatter-gather support: the header and the actual data don’t have to be aligned, one next to the other in memory, they can be sent from different memory locations, so there is a copy avoided</li></ul><p name="66d3" id="66d3" class="graf graf--p graf-after--li">At the time the paper was written, these optimizations were pretty novel and in fact, some of the systems they compared against did not have some of these things included.</p><p name="61ee" id="61ee" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(17) Example of the Apache Web Server</strong></p><p name="ed83" id="ed83" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">apache web server</strong> is a popular modern open-source webserver and now let’s give a brief introduction of this server.</p><figure name="c9ee" id="c9ee" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dof02w_DKzOuRAjuVsov2w.png" data-width="1040" data-height="328" src="https://cdn-images-1.medium.com/max/800/1*dof02w_DKzOuRAjuVsov2w.png"></figure><ul class="postList"><li name="89c5" id="89c5" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Apache Core</strong></li></ul><p name="a35c" id="a35c" class="graf graf--p graf-after--li">The Apache core component provides basic server-like operations like accepting connections and managing concurrency.</p><ul class="postList"><li name="462c" id="462c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Modules</strong></li></ul><p name="f891" id="f891" class="graf graf--p graf-after--li">The various modules correspond to different types of functionality that are executed on each request. Specific Apache deployment can be configured to include different types of modules. For instance, the server can have certain security features, some management of dynamic content, or etc.</p><ul class="postList"><li name="6f6c" id="6f6c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Logical Flow of Control</strong></li></ul><p name="b3b1" id="b3b1" class="graf graf--p graf-after--li">The flow of control is sort of similar to the EDM that we saw, for instance, each request ultimately passes through all the handlers.</p><ul class="postList"><li name="d7a5" id="d7a5" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">MP + MT Model</strong></li></ul><p name="35d1" id="35d1" class="graf graf--p graf-after--li">Apache is a combination of an MP and an MT model. Each process is internally a multi-threaded boss-worker threading model. In addition, the number of processes can also be dynamically adjusted.</p><p name="fdaf" id="fdaf" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Experimental Methodology</strong></p><p name="65df" id="65df" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Comparing Criteria</strong></p><p name="062f" id="062f" class="graf graf--p graf-after--p">In the Flash paper, the experiments are designed so that they can make stronger arguments about the contributions that the authors claim about Flash. Before we achieve a good experimental design, we have to ask ourselves about what we want to compare. So before we start, let’s think about the following questions,</p><ul class="postList"><li name="4a53" id="4a53" class="graf graf--li graf-after--p">Define <strong class="markup--strong markup--li-strong">Comparision Points</strong>: what is the system we want to compare?</li><li name="8fb4" id="8fb4" class="graf graf--li graf-after--li">Define <strong class="markup--strong markup--li-strong">Inputs</strong>: what workload will be used?</li><li name="7018" id="7018" class="graf graf--li graf-after--li">Define <strong class="markup--strong markup--li-strong">Metrics</strong>: how will we measure performance?</li></ul><p name="51ad" id="51ad" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Comparision Points For the Flash Paper</strong></p><p name="a531" id="a531" class="graf graf--p graf-after--p">Now, let’s see what we were comparing in the Flash paper,</p><ul class="postList"><li name="e159" id="e159" class="graf graf--li graf-after--p">an <strong class="markup--strong markup--li-strong">MP version </strong>of the same kind of Flash processing (1 thread per process)</li><li name="6129" id="6129" class="graf graf--li graf-after--li">an <strong class="markup--strong markup--li-strong">MT version</strong> that follows the Boss-Worker model</li><li name="2ced" id="2ced" class="graf graf--li graf-after--li">a <strong class="markup--strong markup--li-strong">SPED model</strong> that we discussed first</li></ul><p name="210a" id="210a" class="graf graf--p graf-after--li">In addition, we will also compare with two existing web server implementations,</p><ul class="postList"><li name="b02e" id="b02e" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Zeus</strong> (SPED model with 2 processes)</li><li name="5524" id="5524" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Apache</strong> (at the time Apache v1.3.1 was an MP configuration)</li></ul><p name="3be6" id="3be6" class="graf graf--p graf-after--li">However, in the end, we would like to compare every single one of all these implementations with Flash,</p><ul class="postList"><li name="179e" id="179e" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Flash</strong> (AMPED model)</li></ul><p name="ac71" id="ac71" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Workloads/Inputs For the Flash Paper</strong></p><ul class="postList"><li name="6ab5" id="6ab5" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Realistic Request Workload</strong></li></ul><p name="c7e9" id="c7e9" class="graf graf--p graf-after--li">To define useful inputs, they wanted workloads that represent a realistic sequence of requests because that&#39;s what will capture the real distribution of web page accesses over time</p><ul class="postList"><li name="67f8" id="67f8" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Controlled Workload for Reproduction</strong>: Trace-based approach</li></ul><p name="0e86" id="0e86" class="graf graf--p graf-after--li">To repeat the experiment with this same pattern of accesses, they also used a <strong class="markup--strong markup--p-strong">trace-based approach</strong> where they gathered traces from real web servers, and then they replayed those traces so as to be able to repeat the experiment with the different implementations. Therefore, different implementations will be evaluated against the <strong class="markup--strong markup--p-strong">same trace</strong>.</p><p name="1f93" id="1f93" class="graf graf--p graf-after--p">In the paper, the traces used were gathered at Rice University where the authors of this paper are from. These traces are called,</p><ul class="postList"><li name="296f" id="296f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">CS Web trace</strong>: (LARGE) represents the Rice University web server for the Computer Science Department and it includes a large number of files and doesn’t really fit in memory</li><li name="43fd" id="43fd" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Owlnet trace</strong>: (SMALL) a web server that hosted a number of student web pages and it was much smaller, so it would typically fit in the memory of a common server</li></ul><p name="68fe" id="68fe" class="graf graf--p graf-after--li">In addition to these two traces, they also used,</p><ul class="postList"><li name="2fcc" id="2fcc" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Synthetic workload</strong>: this is generated by a synthetic workload generator. With the synthetic workload generator, as opposed to replaying these traces of real-world page access distributions, they would perform some best or worst type of analysis or run some <strong class="markup--strong markup--li-strong">what-if questions</strong>, like what-if the distribution of the web page accesses had a certain pattern, would something change about their observations.</li></ul><p name="675d" id="675d" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Metrics For the Flash Paper</strong></p><p name="3c8f" id="3c8f" class="graf graf--p graf-after--p">Finally, let’s look at what are the relevant metrics that the authors picked in order to perform their comparisons.</p><ul class="postList"><li name="12c0" id="12c0" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Bandwidth</strong>: means the total amount of useful bytes, the unit of this metric is <strong class="markup--strong markup--li-strong">bytes per second</strong> or <strong class="markup--strong markup--li-strong">megabytes per second</strong> or similar</li><li name="852b" id="852b" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Connection Rate</strong>: measures the ability of concurrent processing. This metrics is defined as the <strong class="markup--strong markup--li-strong">total number of client connections that service over a period of time</strong>.</li></ul><p name="2684" id="2684" class="graf graf--p graf-after--li">Both of these metrics were evaluated with respect to the file size. For a <strong class="markup--strong markup--p-strong">larger file size</strong>, we can obtain <strong class="markup--strong markup--p-strong">higher bandwidth</strong> because the connection cost can be amortized. However, the more work that the server will have to do for each connection because of a larger file, the more <strong class="markup--strong markup--p-strong">negative impacts on the connection rate</strong>.</p><p name="17c4" id="17c4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Experimental Results For Best Case Numbers with Synthetic load</strong></p><ul class="postList"><li name="c7ba" id="c7ba" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Input</strong>: <strong class="markup--strong markup--li-strong">Synthetic load</strong>, N requests for the <strong class="markup--strong markup--li-strong">same file</strong>. This is the <strong class="markup--strong markup--li-strong">best case</strong> because the clients will not be asking for the same file every time. Because the file will be in cache, we don’t have to pay too much for disk I/O.</li></ul><p name="4a2f" id="4a2f" class="graf graf--p graf-after--li">Now, let’s examine the result.</p><figure name="b24a" id="b24a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vn9EMJRS--kQDyd1bUoYWA.png" data-width="1527" data-height="451" src="https://cdn-images-1.medium.com/max/800/1*vn9EMJRS--kQDyd1bUoYWA.png"><figcaption class="imageCaption"><a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" data-href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank"><em class="markup--em markup--figure-em">Flash: An Efficient and Portable Web Server</em></a><em class="markup--em markup--figure-em"> Figure 6</em></figcaption></figure><p name="54f1" id="54f1" class="graf graf--p graf-after--figure">So our observations are,</p><ul class="postList"><li name="3e3f" id="3e3f" class="graf graf--li graf-after--p">For all of the curves, initially, when the file size is small, bandwidth is low, and as the file size increases the bandwidth increases. So all of the implementations have very <strong class="markup--strong markup--li-strong">similar results</strong>.</li><li name="85b9" id="85b9" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">SPED is really the best model.</strong> That’s expected because it doesn’t have any threads or processes among which it needs to context switch.</li><li name="ac3f" id="ac3f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Flash is similar</strong> but it performs that <strong class="markup--strong markup--li-strong">extra check</strong> for the memory presence. In this case, because we are requesting the same file, there’s no need for blocking I/O, so <strong class="markup--strong markup--li-strong">none of the helper processes will be invoked</strong>. That’s why we see a little bit lower performance for Flash.</li><li name="7b80" id="7b80" class="graf graf--li graf-after--li">Zeus has an anomaly, its performance drops a little bit after 100 bytes. This has to do with some misalignment for some of the <strong class="markup--strong markup--li-strong">DMA operations</strong>.</li><li name="d853" id="d853" class="graf graf--li graf-after--li">For the MT and the MP models, the performance is slower because of the <strong class="markup--strong markup--li-strong">context switching</strong> and <strong class="markup--strong markup--li-strong">extra synchronization</strong>.</li><li name="9cef" id="9cef" class="graf graf--li graf-after--li">Apache is the worst because it <strong class="markup--strong markup--li-strong">doesn’t have any optimizations</strong> that the others implement.</li></ul><p name="52b1" id="52b1" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(6) Experimental Results For Realistic Workloads</strong></p><p name="ca8c" id="ca8c" class="graf graf--p graf-after--p">Since real clients don’t behave like this synthetic workload, we also need to have a look at what happens with some of the realistic traces, the Owlnet and the CS trace.</p><figure name="0e84" id="0e84" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ALW_QoSIv_uAAnXaZqfNvw.png" data-width="1758" data-height="444" src="https://cdn-images-1.medium.com/max/800/1*ALW_QoSIv_uAAnXaZqfNvw.png"><figcaption class="imageCaption"><a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" data-href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank"><em class="markup--em markup--figure-em">Flash: An Efficient and Portable Web Server</em></a><em class="markup--em markup--figure-em"> Figure 8</em></figcaption></figure><p name="86b1" id="86b1" class="graf graf--p graf-after--figure">So our observations are,</p><ul class="postList"><li name="46b4" id="46b4" class="graf graf--li graf-after--p">Owlnet Trace</li></ul><p name="efe1" id="efe1" class="graf graf--p graf-after--li">Owlnet trace is very similar to the best-case performance with SPED and Flash being the best, and then MT, MP, and Apache dropping down. This is because that the Owlnet trace is a <strong class="markup--strong markup--p-strong">small trace</strong>, and most of it will <strong class="markup--strong markup--p-strong">fit in the cache</strong>. So we’ll have a similar behavior like what we had in the “best” case.</p><p name="99e5" id="99e5" class="graf graf--p graf-after--p">The reason why Flash surpasses the SPED is that, sometimes, the blocking I/O is required. Given the blocking I/O possibility SPED will occasionally block, whereas in Flash the helper processes will resolve the problem, we will therefore achieve a better result for the Flash.</p><ul class="postList"><li name="ee21" id="ee21" class="graf graf--li graf-after--p">CS Web Trace</li></ul><p name="d38b" id="d38b" class="graf graf--p graf-after--li">Because the CS trace is a <strong class="markup--strong markup--p-strong">larger trace</strong>, most requests will require a disk I/O operation and it is <strong class="markup--strong markup--p-strong">not going to fit the cache</strong>.</p><p name="3938" id="3938" class="graf graf--p graf-after--p">Since the SPED does not support asynchronous I/O operations, the performance of it drops significantly.</p><p name="d632" id="d632" class="graf graf--p graf-after--p">What’s more, we see that the <strong class="markup--strong markup--p-strong">MT model is better than the MP model</strong>. The main reason for that is that the MT implementation has a <strong class="markup--strong markup--p-strong">smaller memory footprint</strong>, which leads to more memories for caching files. It is also because the context switching between threads is much cheaper than processes.</p><p name="17a2" id="17a2" class="graf graf--p graf-after--p">In this case, we can also see that the Flash server performs best because,</p><ul class="postList"><li name="c4af" id="c4af" class="graf graf--li graf-after--p">it has <strong class="markup--strong markup--li-strong">the smallest memory footprint</strong> with MP and MT</li><li name="153e" id="153e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">no need for explicit synchronization</strong></li><li name="4ae6" id="4ae6" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">helper</strong> threads for handling blocking I/O operation</li></ul><p name="49a8" id="49a8" class="graf graf--p graf-after--li">In both of those cases, Apache performed worst.</p><p name="70e9" id="70e9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Experimental Results For Optimization</strong></p><p name="f5a8" id="f5a8" class="graf graf--p graf-after--p">Let’s try to understand if there’s really an impact of the optimizations performed in Flash.</p><figure name="dba6" id="dba6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*GrK5cFTsEXN143VbTlE0EA.png" data-width="1178" data-height="542" src="https://cdn-images-1.medium.com/max/800/1*GrK5cFTsEXN143VbTlE0EA.png"><figcaption class="imageCaption"><a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" data-href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-pai-paper.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank"><em class="markup--em markup--figure-em">Flash: An Efficient and Portable Web Server</em></a><em class="markup--em markup--figure-em"> Figure 11</em></figcaption></figure><p name="c6ec" id="c6ec" class="graf graf--p graf-after--figure">The performance that is gathered with Flash without any optimizations is the bottom line. While the final line at the top includes all of the optimizations. Thus, in conclusion, we can know that these optimizations for caching are indeed very important.</p><p name="3a8e" id="3a8e" class="graf graf--p graf-after--p">In both of the previous results, Apache performed worst. This is because Apache would have been impacted if it had integrated some of these same optimizations as the other implementations.</p><p name="9177" id="9177" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Result Summary</strong></p><p name="fec0" id="fec0" class="graf graf--p graf-after--p">Here’s a summary of the results above.</p><ul class="postList"><li name="9514" id="9514" class="graf graf--li graf-after--p">When the data is in cache: SPED &gt; AMPED Flash &gt;&gt; MT/MP</li><li name="7b4f" id="7b4f" class="graf graf--li graf-after--li graf--trailing">With the disk-bound workload: AMPED Flash &gt;&gt; MT/MP &gt; SPED</li></ul></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/f8b6213907f5"><time class="dt-published" datetime="2021-03-02T19:25:25.903Z">March 2, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/operating-system-14-event-driven-model-with-the-flash-server-example-experiment-design-metrics-f8b6213907f5" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>