<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Distributed Computing 2 | Introduction to Spark and Basic Spark Examples</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Distributed Computing 2 | Introduction to Spark and Basic Spark Examples</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Distributed Computing
</section>
<section data-field="body" class="e-content">
<section name="76f0" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="be33" id="be33" class="graf graf--h3 graf--leading graf--title">Distributed Computing 2 | Introduction to Spark and Basic SparkÂ Examples</h3><figure name="78e2" id="78e2" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*EtFPINBOSWOLQde0.png" data-width="1144" data-height="634" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*EtFPINBOSWOLQde0.png"></figure><ol class="postList"><li name="df0c" id="df0c" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Introduction to Spark</strong></li></ol><p name="be81" id="be81" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) The Introduction to Spark</strong></p><p name="4504" id="4504" class="graf graf--p graf-after--p">Apache Spark is a fast and general-purpose cluster computing system and it is intended to handle large-scale data. Spark is built on top of the Scala language.</p><p name="308a" id="308a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Spark Vs. Hadoop</strong></p><p name="d69a" id="d69a" class="graf graf--p graf-after--p">Hadoop is entirely dependent on MapReduce, which is designed to solve the issues of,</p><ul class="postList"><li name="32bc" id="32bc" class="graf graf--li graf-after--p">Distribution: Distribute the data.</li><li name="fd1c" id="fd1c" class="graf graf--li graf-after--li">Parallelism: Perform subsets of the computation simultaneously.</li><li name="7d91" id="7d91" class="graf graf--li graf-after--li">Fault Tolerance: Handle component failure.</li></ul><p name="9caf" id="9caf" class="graf graf--p graf-after--li">However, it can be very slow because MapReduce needs to store results in HDFS (i.e. Hadoop file system) before they can be used by another job. The HDFS is related to the disk, and it can be really slow on file I/O.</p><p name="af91" id="af91" class="graf graf--p graf-after--p">Spark is faster than Hadoop because Spark uses only the memory for file I/O. This means Spark can also hide the low-level interfaces and languages which can be difficult for the users.</p><figure name="3da9" id="3da9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*84hKlJT9CsKH1TAZIf5gsw.png" data-width="1306" data-height="306" src="https://cdn-images-1.medium.com/max/800/1*84hKlJT9CsKH1TAZIf5gsw.png"></figure><p name="cf18" id="cf18" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Benefits of Using Spark</strong></p><ul class="postList"><li name="c813" id="c813" class="graf graf--li graf-after--p">Fast Speed: Spark has an advanced directed acyclic graph (i.e. DAC) execution engine that supports cyclic data flow and in-memory computing.</li><li name="d429" id="d429" class="graf graf--li graf-after--li">Ease of Use: We can quickly write Spark applications in Java, Scala, Python, R.</li><li name="af3b" id="af3b" class="graf graf--li graf-after--li">Generality: Spark powers a stack of libraries including SQL, streaming, and complex analytic tools.</li><li name="2a1a" id="2a1a" class="graf graf--li graf-after--li">Compatibility: Spark can run on different platforms like Hadoop, Mesos, standalone, or in the cloud.</li><li name="b30a" id="b30a" class="graf graf--li graf-after--li">Accessibility: Spark can access diverse data sources including HDFS, Cassandra, HBase, and S3.</li></ul><p name="5267" id="5267" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) The Structure of Spark</strong></p><p name="59b6" id="59b6" class="graf graf--p graf-after--p">The structure of Spack is called a Spark stack, which has three layers,</p><ul class="postList"><li name="2754" id="2754" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Applications Layer</strong>: Including Spark SQL (used to treat the data with the data frame based on the RDD), Spark MLlib (the Spark machine learning library and it will not be covered in this series), Spark Streaming, and Spark GraphX</li><li name="c6c6" id="c6c6" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Processing Engine Layer</strong>: This layer refers to the Spark core which manages to distribute the job, store fundamental functions, and manage RDD objects.</li><li name="9801" id="9801" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Cluster Resource Management Layer</strong>: This can be either the Standalone Scheduler cluster manager, the Hadoop YARN cluster manager, or the Apache Mesos cluster manager. In this series, we will only talk about the Standalone Scheduler cluster manager.</li></ul><figure name="01dc" id="01dc" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*YWAAfQIha68zy4BznlVxrw.png" data-width="1678" data-height="410" src="https://cdn-images-1.medium.com/max/800/1*YWAAfQIha68zy4BznlVxrw.png"></figure><p name="5cdb" id="5cdb" class="graf graf--p graf-after--figure">There is also another layer of the distributed file system we need to support the whole distributed system, which can be,</p><ul class="postList"><li name="b81b" id="b81b" class="graf graf--li graf-after--p">Hadoop Distributed File System (HDFS)</li><li name="ceec" id="ceec" class="graf graf--li graf-after--li">Cassandra</li><li name="7ba1" id="7ba1" class="graf graf--li graf-after--li">Amazon S3</li><li name="9f82" id="9f82" class="graf graf--li graf-after--li">HBase</li><li name="79ea" id="79ea" class="graf graf--li graf-after--li">OpenStack Swift</li><li name="70df" id="70df" class="graf graf--li graf-after--li">etc.</li></ul><p name="a349" id="a349" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(5) The Definition of Resilient Distributed Dataset (RDD)</strong></p><p name="6dc9" id="6dc9" class="graf graf--p graf-after--p">The RDDs are abstractions of distributed collections of items with operations and transformations applicable to the dataset. RDD is a new object type supported by Spark and it is not a list type.</p><p name="1cf2" id="1cf2" class="graf graf--p graf-after--p">It is called <strong class="markup--strong markup--p-strong">resilient</strong> because if it failed, it is going to rebuild itself instead of recover from the latest version, so we are actually not going to store the data somewhere in the disk and copy them back after failures. Because of this reason, RDD is <strong class="markup--strong markup--p-strong">immutable</strong>, and we can only store the result after conducting some operations on the RDD to some new RDDs.</p><p name="7410" id="7410" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Spark Core</strong></p><p name="744a" id="744a" class="graf graf--p graf-after--p">Spark core contains Spark functionalities required for running jobs and needed by other components. It contains</p><ul class="postList"><li name="f8db" id="f8db" class="graf graf--li graf-after--p">All the RDDs</li><li name="6f8c" id="6f8c" class="graf graf--li graf-after--li">Fundamental functions: file I/O, scheduling, shuffling, security, networking, etc.</li></ul><p name="aa9a" id="aa9a" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(7) Spark SQL</strong></p><p name="c085" id="c085" class="graf graf--p graf-after--p">The Spark SQL provides functions for manipulating large sets of distributed, structured data using an SQL subset. It transforms operations on DataFrames to operations on RDDs so that it can support a variety of data sources,</p><ul class="postList"><li name="13f7" id="13f7" class="graf graf--li graf-after--p">Hive</li><li name="3c60" id="3c60" class="graf graf--li graf-after--li">JSON</li><li name="1262" id="1262" class="graf graf--li graf-after--li">Relational databases</li><li name="7cf3" id="7cf3" class="graf graf--li graf-after--li">NoSQL databases</li><li name="bf80" id="bf80" class="graf graf--li graf-after--li">Parquet files</li></ul><p name="af97" id="af97" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(8) Spark MLlib</strong></p><p name="55f2" id="55f2" class="graf graf--p graf-after--p">Spark MLlib is the library of machine learning algorithms. It includes logistic regression, naÃ¯ve Bayes, support vector machine, decision trees, random forests, linear regression, and k-mean clustering. The API of this library is based on the DataFrames and the RDD-based API is now in maintenance mode, which means it will probably not be supported in the future. You can find some reasons why MLlib switches to the DataFrame-based API <a href="https://spark.apache.org/docs/latest/ml-guide.html" data-href="https://spark.apache.org/docs/latest/ml-guide.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><p name="a6b5" id="a6b5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) Spark Streaming</strong></p><p name="8eec" id="8eec" class="graf graf--p graf-after--p">Spark Streaming is used to ingest real-time streaming data from various sources including HDFS, Kafka, Flume, Twitter, ZeroMQ, and etc.</p><p name="1474" id="1474" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) Spark GraphX</strong></p><p name="f104" id="f104" class="graf graf--p graf-after--p">Spark GraphX provides functions for building graphs, represented as graph RDDsÂ : <code class="markup--code markup--p-code">EdgeRDD</code> and <code class="markup--code markup--p-code">VertexRDD</code>. It also contains important algorithms of graph theory such as page rank, connected components, shortest paths, SVD++.</p><p name="66c4" id="66c4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(11) The Applications of Spark</strong></p><p name="ac72" id="ac72" class="graf graf--p graf-after--p">There are applications of Spark and here is a list of them,</p><ul class="postList"><li name="0aa3" id="0aa3" class="graf graf--li graf-after--p">Extract-Transformation-Load (i.e. ETL) operations and processing: building the data pipeline</li><li name="ca8f" id="ca8f" class="graf graf--li graf-after--li">Predictive analytics</li><li name="d28a" id="d28a" class="graf graf--li graf-after--li">Machine learning</li><li name="9edb" id="9edb" class="graf graf--li graf-after--li">Data access operation (SQL queries and visualizations)</li><li name="7e15" id="7e15" class="graf graf--li graf-after--li">Text mining and text processing</li><li name="0ade" id="0ade" class="graf graf--li graf-after--li">Real-time event processing</li><li name="3aad" id="3aad" class="graf graf--li graf-after--li">Graph applications</li><li name="91eb" id="91eb" class="graf graf--li graf-after--li">Pattern recognition</li><li name="3e5d" id="3e5d" class="graf graf--li graf-after--li">Recommendation engines</li><li name="c64b" id="c64b" class="graf graf--li graf-after--li">etc.</li></ul><p name="0b8f" id="0b8f" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Spark Components</strong></p><p name="6382" id="6382" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of Spark Context</strong></p><p name="b956" id="b956" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Spark Context</strong> is the entry gate of Apache Spark functionality. The most important step of any Spark driver application is to generate <code class="markup--code markup--p-code">SparkContext</code>. It allows your Spark application to access Spark clusters with the help of the resource manager (Standalone/YARN/Mesos). Spark applications run as independent sets of processes on a cluster, coordinated by the <code class="markup--code markup--p-code">SparkContext</code> object in your main program (called the <strong class="markup--strong markup--p-strong">driver program</strong>).</p><figure name="29b2" id="29b2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*c-RwPYSAwQB4_CX0aIHOgw.png" data-width="1678" data-height="430" src="https://cdn-images-1.medium.com/max/800/1*c-RwPYSAwQB4_CX0aIHOgw.png"></figure><ul class="postList"><li name="12b9" id="12b9" class="graf graf--li graf-after--figure">Step 1: SparkContext connect to cluster manager</li><li name="eb03" id="eb03" class="graf graf--li graf-after--li">Step 2: Spark acquires executors on nodes in the cluster</li><li name="158c" id="158c" class="graf graf--li graf-after--li">Step 3: Spark sends the application code to the executors</li><li name="eeb3" id="eeb3" class="graf graf--li graf-after--li">Step 4: SparkContext sends tasks to the executors to run</li></ul><p name="7c30" id="7c30" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Example of Spark Context</strong></p><p name="aff5" id="aff5" class="graf graf--p graf-after--p">To try the <code class="markup--code markup--p-code">SparkContext</code>Â , letâs first write a program for testing.</p><ul class="postList"><li name="42ee" id="42ee" class="graf graf--li graf-after--p">Letâs first import the Spark,</li></ul><pre name="2350" id="2350" class="graf graf--pre graf-after--li">import pyspark</pre><ul class="postList"><li name="df3a" id="df3a" class="graf graf--li graf-after--pre">To initialize a <code class="markup--code markup--li-code">SparkContext</code> object with <code class="markup--code markup--li-code">appName</code> of <code class="markup--code markup--li-code">Test</code>Â , we should call</li></ul><pre name="fa2e" id="fa2e" class="graf graf--pre graf-after--li">sc = pyspark.SparkContext(appName=&#39;Test&#39;).getOrCreate()</pre><p name="bb0a" id="bb0a" class="graf graf--p graf-after--pre">Now we have a Spark Context assigned to the variable <code class="markup--code markup--p-code">sc</code> and the current program used to generate this Spark Context is called a driver program.</p><ul class="postList"><li name="eed1" id="eed1" class="graf graf--li graf-after--p">If we want to stop running a Spark Context, we should call,</li></ul><pre name="a5ac" id="a5ac" class="graf graf--pre graf-after--li">sc.stop()</pre><ul class="postList"><li name="df56" id="df56" class="graf graf--li graf-after--pre">To show the general Spark Context information, we can call,</li></ul><pre name="daf8" id="daf8" class="graf graf--pre graf-after--li">sc.getConf().getAll()</pre><p name="452e" id="452e" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(3) Spark UI</strong></p><p name="f561" id="f561" class="graf graf--p graf-after--p">Spark has a UI that we can check states of jobs, storage, and executors. To assign a typical port for UI, we have to call <code class="markup--code markup--p-code">SparkConf()</code> before we call <code class="markup--code markup--p-code">getOrCreate()</code>.</p><ul class="postList"><li name="efc6" id="efc6" class="graf graf--li graf-after--p">Set APP name</li></ul><pre name="7ad7" id="7ad7" class="graf graf--pre graf-after--li">conf = pyspark.SparkConf().setAppName(&quot;Test&quot;)</pre><ul class="postList"><li name="c815" id="c815" class="graf graf--li graf-after--pre">Set IP to <code class="markup--code markup--li-code">localhost</code></li></ul><pre name="6282" id="6282" class="graf graf--pre graf-after--li">conf = pyspark.SparkConf().set(&quot;spark.driver.host&quot;, &quot;localhost&quot;)</pre><ul class="postList"><li name="e95c" id="e95c" class="graf graf--li graf-after--pre">Set port to <code class="markup--code markup--li-code">4050</code></li></ul><pre name="cb02" id="cb02" class="graf graf--pre graf-after--li">conf = pyspark.SparkConf().set(&quot;spark.ui.port&quot;, &quot;4050&quot;)</pre><ul class="postList"><li name="89c2" id="89c2" class="graf graf--li graf-after--pre">Assign the configurations <code class="markup--code markup--li-code">conf</code> to the <code class="markup--code markup--li-code">SparkContext</code></li></ul><pre name="beec" id="beec" class="graf graf--pre graf-after--li">sc = pyspark.SparkContext(conf=conf)</pre><ul class="postList"><li name="8b90" id="8b90" class="graf graf--li graf-after--pre">Open the Spark UI</li></ul><pre name="126c" id="126c" class="graf graf--pre graf-after--li"># GOTO: <br>http://localhost:4050</pre><p name="6f5a" id="6f5a" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(4) Spark I/O</strong></p><p name="fdf1" id="fdf1" class="graf graf--p graf-after--p">Now, letâs use Spark for reading a local file.</p><ul class="postList"><li name="9964" id="9964" class="graf graf--li graf-after--p">Create a <code class="markup--code markup--li-code">CSV</code> file named <code class="markup--code markup--li-code">Test.csv</code></li></ul><pre name="a182" id="a182" class="graf graf--pre graf-after--li">file = open(&quot;Test.csv&quot;, &quot;w&quot;)<br>for i in range(20):<br>    file.write(str(i) + &#39;,&#39; + str(i+1) + &quot;\n&quot;)<br>file.write(str(0) + &#39;,&#39; + str(1))<br>file.close()</pre><ul class="postList"><li name="1914" id="1914" class="graf graf--li graf-after--pre">Read from this <code class="markup--code markup--li-code">Test.csv</code> file by Spark</li></ul><pre name="67be" id="67be" class="graf graf--pre graf-after--li">rdd = sc.textFile(&quot;Test.csv&quot;, minPartitions=8)</pre><ul class="postList"><li name="ab32" id="ab32" class="graf graf--li graf-after--pre">Or we can read the data from a <code class="markup--code markup--li-code">list</code> or an <code class="markup--code markup--li-code">np.array</code></li></ul><pre name="b213" id="b213" class="graf graf--pre graf-after--li">rdd = sc<code class="markup--code markup--pre-code">.parallelize(data)</code></pre><ul class="postList"><li name="84b5" id="84b5" class="graf graf--li graf-after--pre">Print all the data in RDD</li></ul><pre name="5f8e" id="5f8e" class="graf graf--pre graf-after--li">print(rdd.collect())</pre><p name="710b" id="710b" class="graf graf--p graf-after--pre">The output should be,</p><pre name="5d8b" id="5d8b" class="graf graf--pre graf-after--p">[&#39;0,1&#39;, &#39;1,2&#39;, &#39;2,3&#39;, &#39;3,4&#39;, &#39;4,5&#39;, &#39;5,6&#39;, &#39;6,7&#39;, &#39;7,8&#39;, &#39;8,9&#39;, &#39;9,10&#39;, &#39;10,11&#39;, &#39;11,12&#39;, &#39;12,13&#39;, &#39;13,14&#39;, &#39;14,15&#39;, &#39;15,16&#39;, &#39;16,17&#39;, &#39;17,18&#39;, &#39;18,19&#39;, &#39;19,20&#39;, &#39;0,1&#39;]</pre><ul class="postList"><li name="6cd7" id="6cd7" class="graf graf--li graf-after--pre">Print the data in each partition of that RDD</li></ul><pre name="b3dd" id="b3dd" class="graf graf--pre graf-after--li">nulls = [print(i) for i in rdd.glom().collect()]</pre><p name="5310" id="5310" class="graf graf--p graf-after--pre">The output should be,</p><pre name="0bc9" id="0bc9" class="graf graf--pre graf-after--p">[&#39;0,1&#39;, &#39;1,2&#39;, &#39;2,3&#39;, &#39;3,4&#39;]<br>[&#39;4,5&#39;, &#39;5,6&#39;, &#39;6,7&#39;]<br>[&#39;7,8&#39;, &#39;8,9&#39;, &#39;9,10&#39;]<br>[&#39;10,11&#39;, &#39;11,12&#39;]<br>[&#39;12,13&#39;, &#39;13,14&#39;, &#39;14,15&#39;]<br>[&#39;15,16&#39;, &#39;16,17&#39;]<br>[&#39;17,18&#39;, &#39;18,19&#39;]<br>[&#39;19,20&#39;, &#39;0,1&#39;]</pre><p name="a377" id="a377" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(5) Spark Data Processing</strong></p><ul class="postList"><li name="8c18" id="8c18" class="graf graf--li graf-after--p">Split the data (we have to assign the result to a new RDD because it is immutable)</li></ul><pre name="53f0" id="53f0" class="graf graf--pre graf-after--li">rdd_split = rdd.map(lambda x: [int(x.split(&quot;,&quot;)[0]), int(x.split(&quot;,&quot;)[1])])<br>print(rdd_split.collect())</pre><p name="200b" id="200b" class="graf graf--p graf-after--pre">The result should be,</p><pre name="6bcd" id="6bcd" class="graf graf--pre graf-after--p">[[0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11], [11, 12], [12, 13], [13, 14], [14, 15], [15, 16], [16, 17], [17, 18], [18, 19], [19, 20], [0, 1]]</pre><ul class="postList"><li name="e89d" id="e89d" class="graf graf--li graf-after--pre">Filtering the data</li></ul><pre name="7122" id="7122" class="graf graf--pre graf-after--li">rdd_reduced = rdd_split.filter(lambda x: x[0] &lt; 5)<br>nulls = [print(i) for i in rdd_reduced.glom().collect()]</pre><p name="b57a" id="b57a" class="graf graf--p graf-after--pre">The result should be,</p><pre name="ff1e" id="ff1e" class="graf graf--pre graf-after--p">[[0, 1], [1, 2], [2, 3], [3, 4]]<br>[[4, 5]]<br>[]<br>[]<br>[]<br>[]<br>[]<br>[[0, 1]]</pre><ul class="postList"><li name="0055" id="0055" class="graf graf--li graf-after--pre">Flat Map the list result</li></ul><pre name="17c0" id="17c0" class="graf graf--pre graf-after--li">rdd_flat = rdd_reduced.flatMap(lambda x: x)<br>nulls = [print(i) for i in rdd_flat.glom().collect()]</pre><p name="ef67" id="ef67" class="graf graf--p graf-after--pre">The result should be,</p><pre name="80ca" id="80ca" class="graf graf--pre graf-after--p">[0, 1, 1, 2, 2, 3, 3, 4]<br>[4, 5]<br>[]<br>[]<br>[]<br>[]<br>[]<br>[0, 1]</pre><ul class="postList"><li name="7f38" id="7f38" class="graf graf--li graf-after--pre">Get the unique result in all the numbers</li></ul><pre name="6313" id="6313" class="graf graf--pre graf-after--li">rdd_0 = rdd_flat.distinct()<br>nulls = [print(i) for i in rdd_0.glom().collect()]</pre><p name="da5b" id="da5b" class="graf graf--p graf-after--pre">The result should be,</p><pre name="c628" id="c628" class="graf graf--pre graf-after--p">[0]<br>[1]<br>[2]<br>[3]<br>[4]<br>[5]<br>[]<br>[]</pre><ul class="postList"><li name="8823" id="8823" class="graf graf--li graf-after--pre">Sort the result by descending</li></ul><pre name="4778" id="4778" class="graf graf--pre graf-after--li">rdd_sort = rdd_split.sortBy(lambda x: x[0], ascending=False)<br>nulls = [print(i) for i in rdd_split.glom().collect()]<br>print(&quot;\n========================= After Sorting ========================&quot;)<br>nulls = [print(i) for i in rdd_sort.glom().collect()]</pre><p name="c8f2" id="c8f2" class="graf graf--p graf-after--pre">The result should be,</p><pre name="da0f" id="da0f" class="graf graf--pre graf-after--p graf--trailing">[[0, 1], [1, 2], [2, 3], [3, 4]]<br>[[4, 5], [5, 6], [6, 7]]<br>[[7, 8], [8, 9], [9, 10]]<br>[[10, 11], [11, 12]]<br>[[12, 13], [13, 14], [14, 15]]<br>[[15, 16], [16, 17]]<br>[[17, 18], [18, 19]]<br>[[19, 20], [0, 1]]<br><br>========================= After Sorting ========================<br>[[19, 20], [18, 19]]<br>[[17, 18], [16, 17], [15, 16]]<br>[[14, 15], [13, 14]]<br>[[12, 13], [11, 12], [10, 11]]<br>[[9, 10], [8, 9], [7, 8]]<br>[[6, 7], [5, 6]]<br>[[4, 5], [3, 4], [2, 3]]<br>[[1, 2], [0, 1], [0, 1]]</pre></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/e559e74de3d2"><time class="dt-published" datetime="2021-11-05T16:06:37.066Z">November 5, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/distributed-computing-2-introduction-to-spark-and-basic-spark-examples-e559e74de3d2" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>