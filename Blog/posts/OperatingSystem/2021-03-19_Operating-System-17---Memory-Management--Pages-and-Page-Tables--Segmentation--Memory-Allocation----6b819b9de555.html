<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Operating System 17 | Memory Management, Pages and Page Tables, Segmentation, Memory Allocation, …</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Operating System 17 | Memory Management, Pages and Page Tables, Segmentation, Memory Allocation, …</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Operating System
</section>
<section data-field="body" class="e-content">
<section name="1f76" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="37cb" id="37cb" class="graf graf--h3 graf--leading graf--title">Operating System 17 | <strong class="markup--strong markup--h3-strong">Memory Management, Pages and Page Tables, Segmentation, Memory Allocation, Copy-On-Write, and Checkpointing</strong></h3><figure name="a2a8" id="a2a8" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*tAXayKXgqSnCbJY_.png" data-width="1508" data-height="794" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*tAXayKXgqSnCbJY_.png"></figure><ol class="postList"><li name="89c9" id="89c9" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Introduction to Memory Management</strong></li></ol><p name="4db4" id="4db4" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Virtual Addresses Vs. Physical Addresses</strong></p><p name="68c5" id="68c5" class="graf graf--p graf-after--p">We have discussed that almost every application uses <strong class="markup--strong markup--p-strong">virtual addresses (VA)</strong> and these virtual addresses are translated to actual <strong class="markup--strong markup--p-strong">physical addresses (PA)</strong> in the <strong class="markup--strong markup--p-strong">physical memory (DRAM)</strong> where the particular state is stored. In fact, the virtual address space can be much larger than the physical address space.</p><p name="9100" id="9100" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Goal of Memory Management</strong></p><p name="330e" id="330e" class="graf graf--p graf-after--p">In order to manage the physical memory, the OS must then be able to <strong class="markup--strong markup--p-strong">allocate</strong> physical memory and also <strong class="markup--strong markup--p-strong">arbitrate</strong> how it’s being accessed.</p><ul class="postList"><li name="5f48" id="5f48" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Allocation</strong>: requires that the OS tracks how physical memory is used and also which frame is free among the physical memory. In addition, OS must have mechanisms to decide how to replace the contents that are currently in physical memory with the needed content that’s on some temporary storage like the disk.</li><li name="f654" id="f654" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Arbitration</strong>: requires that the OS is quickly able to interpret and verify memory access. That means that when looking at a virtual address, the OS should be able to translate the virtual address into a physical address rapidly and verify that it is indeed legal access. To implement this, the OS relies on a combination of hardware and software supports.</li></ul><p name="289d" id="289d" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Pages Vs. Page Frames</strong></p><p name="c5ff" id="c5ff" class="graf graf--p graf-after--p">The virtual address space is subdivided into fixed-size segments that we call <strong class="markup--strong markup--p-strong">pages</strong>. The physical memory is divided into <strong class="markup--strong markup--p-strong">page frames</strong> of the same size. The role of the OS is to map pages from the virtual memory into page frames of the physical memory.</p><figure name="dc13" id="dc13" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*x1iDyv1XWaYmWjZB.png" data-width="1660" data-height="650" src="https://cdn-images-1.medium.com/max/800/0*x1iDyv1XWaYmWjZB.png"></figure><p name="58b0" id="58b0" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Segment-Based Memory Management</strong></p><p name="4641" id="4641" class="graf graf--p graf-after--p">Paging is not the only way to decouple the virtual and the physical memory. Another approach is segmentation so which would be a segment-based memory management approach.</p><p name="e9a7" id="e9a7" class="graf graf--p graf-after--p">With segmentation, the allocation process doesn’t use fixed-sized pages. Instead, it uses more <strong class="markup--strong markup--p-strong">flexibly sized segments</strong> that can be mapped to some regions in physical memory. Arbitration of accesses in order to either translate or validate the appropriate access uses <strong class="markup--strong markup--p-strong">segment registers</strong> that are typically supported on modern hardware.</p><p name="9a4f" id="9a4f" class="graf graf--p graf-after--p">However, paging is the dominant method used in the current OSs and we will mainly focus our discussion on page-based memory management in this section.</p><p name="e1e9" id="e1e9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Hardware Supports for Memory Management</strong></p><p name="e45c" id="e45c" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">memory management unit</strong> (aka. <strong class="markup--strong markup--p-strong">MMU</strong>) in the hardware can be used to support memory management.</p><ul class="postList"><li name="a996" id="a996" class="graf graf--li graf-after--p">First, the CPU <strong class="markup--strong markup--li-strong">issue</strong>s virtual addresses to the MMU</li><li name="c683" id="c683" class="graf graf--li graf-after--li">Second, the MMU <strong class="markup--strong markup--li-strong">translate</strong>s VAs into the corresponding PAs</li><li name="357d" id="357d" class="graf graf--li graf-after--li">Or potentially, the MMU can generate a <strong class="markup--strong markup--li-strong">fault</strong>. The faults are an exception or a signal that is generated by the MMU that can indicate illegal access (e.g. access memory not allocated), inadequate permissions (e.g. no writing permissions), or non-existing pages (e.g. file not found in memory).</li></ul><figure name="fbe9" id="fbe9" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*JxK0KnbrvU_18sb7nfrfKw.png" data-width="1998" data-height="774" src="https://cdn-images-1.medium.com/max/800/1*JxK0KnbrvU_18sb7nfrfKw.png"></figure><p name="8825" id="8825" class="graf graf--p graf-after--figure">Since the memory address translation happens frequently, most MMUsd would integrate a small cache of virtual to physical (V2P) mappings and this is called the <strong class="markup--strong markup--p-strong">translation lookaside buffer</strong> (aka. <strong class="markup--strong markup--p-strong">TLB</strong>). The presence of TLB will boost the V2P translation because we don&#39;t need to translate by page tables.</p><p name="cba4" id="cba4" class="graf graf--p graf-after--p">Although the actual V2P translation is done by the <strong class="markup--strong markup--p-strong">hardware</strong>, the OS maintains data structures such as the page tables. Thus, the hardware implies,</p><ul class="postList"><li name="3521" id="3521" class="graf graf--li graf-after--p">what type of memory management modes are supported</li><li name="d0d0" id="d0d0" class="graf graf--li graf-after--li">what kinds of pages can there be</li></ul><p name="38c7" id="38c7" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(6) Software Supports for Memory Management</strong></p><p name="4ecb" id="4ecb" class="graf graf--p graf-after--p">There are other flexible aspects of memory management since they’re performed in software. For instance, the <strong class="markup--strong markup--p-strong">actual allocation</strong> determining which portions of the main memory will be used by which process, or the <strong class="markup--strong markup--p-strong">replacement policies</strong> that determine which portions of the state will be in main memory vs on disk.</p><p name="a00f" id="a00f" class="graf graf--p graf-after--p">We will focus our discussion on those software aspects of memory management since that’s more relevant from an OS’s perspective.</p><p name="617e" id="617e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Page Tables</strong></p><p name="61b4" id="61b4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Page Tables</strong></p><p name="8bb7" id="8bb7" class="graf graf--p graf-after--p">In the page-based memory management system, the arbitration of the accesses is done by <strong class="markup--strong markup--p-strong">page tables</strong>. Page Tables are like a map that can be used to translate virtual memory addresses into physical memory addresses.</p><p name="c1f1" id="c1f1" class="graf graf--p graf-after--p">A virtual address consists of two parts, the virtual page number, and the page offset. The <strong class="markup--strong markup--p-strong">virtual page number</strong> is used as the index to the page table, and then it will be translated to a <strong class="markup--strong markup--p-strong">physical frame number</strong> based on the mappings in the page tables. The <strong class="markup--strong markup--p-strong">offset</strong> will be added to the frame number to create the physical address.</p><figure name="5999" id="5999" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*5I-fGHQ_GMM70PEu.png" data-width="1836" data-height="712" src="https://cdn-images-1.medium.com/max/800/0*5I-fGHQ_GMM70PEu.png"></figure><p name="4852" id="4852" class="graf graf--p graf-after--figure">The physical memory for the virtual address space (or virtual address array) is only allocated when the process during its first initialization routine <code class="markup--code markup--p-code">init_array(&amp;array_addr)</code>. We refer to this as allocation on the <strong class="markup--strong markup--p-strong">first touch</strong>. The reason for this is that we want to make sure that physical memory is allocated only when it’s really needed because some of the virtual addresses are not going to be used by us.</p><p name="8b17" id="8b17" class="graf graf--p graf-after--p">The page table also maintains a <strong class="markup--strong markup--p-strong">valid bit</strong> (aka. <strong class="markup--strong markup--p-strong">present flag</strong>) that tells whether we have a corresponding physical memory in the memory. If the valid bit is set to 0, then the MMU can know that the page we request is not in the memory. So it will raise a fault.</p><p name="c8ca" id="c8ca" class="graf graf--p graf-after--p">Finally, the OS creates <strong class="markup--strong markup--p-strong">a page table for every process</strong> that it runs. That means that whenever a context switch is performed, the OS has to make sure that it switches to the page table of the new context switched process. We said that hardware assists with page table access by maintaining a <strong class="markup--strong markup--p-strong">register</strong> (e.g. CR3 for x86 system) that <strong class="markup--strong markup--p-strong">points to the active page table</strong>.</p><figure name="8a34" id="8a34" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RzQi-2npF2jMz0yPYI5Gjw.png" data-width="1328" data-height="380" src="https://cdn-images-1.medium.com/max/800/1*RzQi-2npF2jMz0yPYI5Gjw.png"></figure><p name="fe3c" id="fe3c" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Page Table Entries (PTE) Flags</strong></p><p name="92f3" id="92f3" class="graf graf--p graf-after--p">Now, let’s focus on the page table and see how does its entry look like. We have known that the page table is used for mapping the virtual memory to the physical memory. What we don’t know is that the page table also stores the information for each of the pages for hardware and software interpretations. Thus, except for the <strong class="markup--strong markup--p-strong">page frame number (PFN)</strong>, there are also many other useful <strong class="markup--strong markup--p-strong">flags</strong> in each entry. The most common flags are,</p><ul class="postList"><li name="6b85" id="6b85" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Present flag (P)</strong>: we have already discussed this flag as the valid bit and it is used to indicate whether the content is actually <strong class="markup--strong markup--li-strong">present in physical</strong> <strong class="markup--strong markup--li-strong">memory or not</strong></li><li name="8a11" id="8a11" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Dirty flag (D)</strong>: this is used for file systems where files are cached in the memory, then we can use this bit to detect which file has been <strong class="markup--strong markup--li-strong">written to</strong> so that we can update the disk in the future</li><li name="0db0" id="0db0" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Access flag (A)</strong>: this can be used to track whether the page has been accessed<strong class="markup--strong markup--li-strong"> for read or for write</strong></li><li name="77f3" id="77f3" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Protection Bits (R/W/X)</strong>: the protection bits can be used to show the <strong class="markup--strong markup--li-strong">permission</strong> of the page (e.g. write-only, read-only, etc.)</li></ul><figure name="b05a" id="b05a" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*OPqyA2xIoejov8UNbnqoiA.png" data-width="1446" data-height="460" src="https://cdn-images-1.medium.com/max/800/1*OPqyA2xIoejov8UNbnqoiA.png"></figure><p name="c409" id="c409" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Page Table: An Example</strong></p><p name="a35e" id="a35e" class="graf graf--p graf-after--p">Now, let’s see a real Pentium x86 page table entry,</p><figure name="d325" id="d325" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BENFijlao8jBa_tm7NLgAw.png" data-width="1590" data-height="162" src="https://cdn-images-1.medium.com/max/800/1*BENFijlao8jBa_tm7NLgAw.png"></figure><p name="7faa" id="7faa" class="graf graf--p graf-after--figure">its flags are,</p><ul class="postList"><li name="609e" id="609e" class="graf graf--li graf-after--p">P: which is exactly the same as the present flag</li><li name="3c27" id="3c27" class="graf graf--li graf-after--li">D: which is exactly the same as the dirty flag</li><li name="8e37" id="8e37" class="graf graf--li graf-after--li">A: which is exactly the same as the access flag</li><li name="73a8" id="73a8" class="graf graf--li graf-after--li">R/W: permission bit. 0 means read-only and 1 means read/write</li><li name="067a" id="067a" class="graf graf--li graf-after--li">U/S: permission bit. 0 means user mode and 1 means supervisor mode only</li><li name="fee4" id="fee4" class="graf graf--li graf-after--li">Other flags: for caching-related info (write-through/write-back, caching disabled, etc.)</li><li name="f3d2" id="f3d2" class="graf graf--li graf-after--li">Unused bits: for future use</li></ul><p name="df7e" id="df7e" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Page Fault</strong></p><p name="24d6" id="24d6" class="graf graf--p graf-after--p">The MMU uses the page table entries not only for the V2P translation but also relies on some bits of each entry to detect the validity of access. The <strong class="markup--strong markup--p-strong">page fault</strong> means that the MMU find out that the VA can not translate to PA. After that, the CPU will place an error code on the stack of the kernel and then it will generate a trap into the OS kernel. Then based on the error code, the kernel will call a page fault handler that determines what is the following actions need to be taken.</p><p name="baf5" id="baf5" class="graf graf--p graf-after--p">Key pieces of information in this error code will include whether or not the fault was caused because of,</p><ul class="postList"><li name="10de" id="10de" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">the non-existing page</strong>: need to load from the disk</li><li name="8917" id="8917" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">protection error</strong>: means there is a permission problem (i.e. SIGSEGV)</li></ul><p name="bb65" id="bb65" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(5) Page Fault: An Example</strong></p><p name="56a0" id="56a0" class="graf graf--p graf-after--p">On x86 platforms, the error code information is generated from some of the flags (e.g. PTE flags). The faulting address that is stored in a register CR2 is also needed during the page fault handler.</p><p name="f433" id="f433" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Recall: Page Table Size</strong></p><p name="2c77" id="2c77" class="graf graf--p graf-after--p">Now, let’s see how we can calculate the size of a page table. We will do the calculation for both the 32-bit system and the 64-bit system. Because we have only a 1-level page table, then this is called a flat page table. We have done this calculation in the following article and now we will do it again.</p><div name="98e6" id="98e6" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/adamedelwiess/high-performance-computer-architecture-23-virtual-memory-virtual-to-physical-translation-page-aa7648477f70" data-href="https://medium.com/adamedelwiess/high-performance-computer-architecture-23-virtual-memory-virtual-to-physical-translation-page-aa7648477f70" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/high-performance-computer-architecture-23-virtual-memory-virtual-to-physical-translation-page-aa7648477f70"><strong class="markup--strong markup--mixtapeEmbed-strong">High-Performance Computer Architecture 23 | Virtual Memory, Virtual to Physical Translation, Page…</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: High-Performance Computer Architecture</em>medium.com</a><a href="https://medium.com/adamedelwiess/high-performance-computer-architecture-23-virtual-memory-virtual-to-physical-translation-page-aa7648477f70" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="e960534a0d5a3f6e01f9497679223f05" data-thumbnail-img-id="0*m9KH39yOJkq80bAM.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*m9KH39yOJkq80bAM.png);"></a></div><p name="628b" id="628b" class="graf graf--p graf-after--mixtapeEmbed">As we have known, the size of a flat page table can be calculated as,</p><figure name="6eb2" id="6eb2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*eJ7v8bIgLdMh0-rj.png" data-width="1422" data-height="106" src="https://cdn-images-1.medium.com/max/800/0*eJ7v8bIgLdMh0-rj.png"></figure><p name="ff64" id="ff64" class="graf graf--p graf-after--figure">Thus, suppose we have a 32-bit system, which means that each virtual address and each PTE have 32 bits. Thus, we can have 2³² virtual addresses and each address has a 1-byte storage. Suppose the page size is 4 KB (which is the most common value), then the page table size for this 32-bit system would be,</p><pre name="0c95" id="0c95" class="graf graf--pre graf-after--p">size_32 = 2^32B/4KB * (32/8)B = 4 MB</pre><p name="866f" id="866f" class="graf graf--p graf-after--pre">Also, for a 64-bit system, we can have,</p><pre name="592f" id="592f" class="graf graf--pre graf-after--p">size_64 = 2^64/4KB * (64/8)B = 32 PB</pre><p name="5db6" id="5db6" class="graf graf--p graf-after--pre">we can find out that for both the 32-bit system and the 64-bit system, the size of the page tables can be too large for us. Thus, it is not okay for us to have a flat page table in reality.</p><p name="acbd" id="acbd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Recall: Multi-level Page Tables</strong></p><p name="3d9c" id="3d9c" class="graf graf--p graf-after--p">To have the same page table with a lower cost, we can consider using multi-level page tables. In a 2-level page table model, the <strong class="markup--strong markup--p-strong">outer page table</strong> (aka. <strong class="markup--strong markup--p-strong">top page table</strong>) can be treated as the page table directory. The internal page table has proper page table components like PFN and the flags, and they will be held only for those valid physical memory regions.</p><figure name="d045" id="d045" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*5LNK06dN_ynJBspH.png" src="https://cdn-images-1.medium.com/max/800/0*5LNK06dN_ynJBspH.png"></figure><p name="a31b" id="a31b" class="graf graf--p graf-after--figure">The size of the page tables will be significantly reduced because we will not maintain the inner page tables that are never accessed because the corresponding pages in the virtual address space can never be never used.</p><p name="0813" id="0813" class="graf graf--p graf-after--p">This technique is particularly important in 64-bit architectures because not only for the <strong class="markup--strong markup--p-strong">larger page table</strong> requirements but also for <strong class="markup--strong markup--p-strong">more sparse</strong> in the virtual address space. With more sparse, it will have larger gaps in the virtual address space region. Therefore, with 4-level addressing, we may end up saving the entire page table directories as a result of certain gaps in the virtual address space.</p><p name="7c12" id="7c12" class="graf graf--p graf-after--p">The problem with the MLPT is that the page tables are always stored in the memory and we need more access to the memory if we have more levels of the page tables. Thus, the overall performance will be <strong class="markup--strong markup--p-strong">slow</strong> down. We can deal with this by using the page table cache called the <strong class="markup--strong markup--p-strong">translation lookaside buffer (TLB)</strong>.</p><p name="7a61" id="7a61" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Recall: Translation Lookaside Buffer (TLB)</strong></p><p name="812a" id="812a" class="graf graf--p graf-after--p">On most architectures, MMU integrates a hardware cache that’s dedicated to caching address translations. This cache is called the <strong class="markup--strong markup--p-strong">translation lookaside buffer (TLB)</strong>.</p><p name="975c" id="975c" class="graf graf--p graf-after--p">On each address translation, first, the TLB cache is quickly referenced and if the resulting address can be generated from the TLB contents, then we have a <strong class="markup--strong markup--p-strong">TLB hit</strong> and we can bypass all of the other required memory accesses to perform the translation.</p><p name="1ef6" id="1ef6" class="graf graf--p graf-after--p">Or if we have a <strong class="markup--strong markup--p-strong">TLB miss</strong>, this means that the address isn’t present in the TLB cache. Then we have to perform all of the address translation steps by accessing the page tables from memory.</p><figure name="05c3" id="05c3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pXP0BT31XXWwqLdpDVZgMQ.png" data-width="1760" data-height="888" src="https://cdn-images-1.medium.com/max/800/1*pXP0BT31XXWwqLdpDVZgMQ.png"></figure><p name="da7b" id="da7b" class="graf graf--p graf-after--figure">In addition to the proper address translation, the TLB entries will contain all of the necessary <strong class="markup--strong markup--p-strong">protection</strong> and <strong class="markup--strong markup--p-strong">validity bits</strong> to verify that the access is correct or, if necessary, to generate a fault.</p><p name="d636" id="d636" class="graf graf--p graf-after--p">As a result, it turns out that even a small number of entries in the TLB can result in a high TLB hit rate and this is because we have typically a high <strong class="markup--strong markup--p-strong">temporal locality</strong> and <strong class="markup--strong markup--p-strong">spatial locality</strong> in the memory references.</p><p name="0f46" id="0f46" class="graf graf--p graf-after--p">For instance, for <strong class="markup--strong markup--p-strong">each core</strong> on the recent x86 Core i7 platform, there would be,</p><ul class="postList"><li name="e021" id="e021" class="graf graf--li graf-after--p">64-entry <strong class="markup--strong markup--li-strong">TLB for data</strong></li><li name="176f" id="176f" class="graf graf--li graf-after--li">128-entry <strong class="markup--strong markup--li-strong">TLB for instructions</strong></li></ul><p name="4b1d" id="4b1d" class="graf graf--p graf-after--li">And there is also a <strong class="markup--strong markup--p-strong">shared TLB for all the cores</strong> which has 512 entries.</p><p name="f06a" id="f06a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) Inverted Page Tables (IPT)</strong></p><p name="bef5" id="bef5" class="graf graf--p graf-after--p">Another completely different way to deal with the V2P translation is to create so-called <strong class="markup--strong markup--p-strong">inverted page tables</strong>. Instead of each entry in the page table corresponds to a virtual page number, each entry in the page table will be assigned a physical frame number. This is because we can at most have 10 GB physical memories, while we can have an O(PB) virtual memory address space. Thus, a mapping rule based on the physical frame number will make the page table much smaller.</p><figure name="580d" id="580d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*h8TgK6W9sICKmDJcZh5tpQ.png" data-width="1760" data-height="664" src="https://cdn-images-1.medium.com/max/800/1*h8TgK6W9sICKmDJcZh5tpQ.png"></figure><p name="a7bd" id="a7bd" class="graf graf--p graf-after--figure">To find the translation, the inverted page table is searched based on the <strong class="markup--strong markup--p-strong">process ID</strong> (aka. <strong class="markup--strong markup--p-strong">PID</strong>) and the virtual page number. When the appropriate page number is found in the page table, the index will denote the physical frame number of the memory location that’s indexed by this logical address.</p><p name="ffbe" id="ffbe" class="graf graf--p graf-after--p">Since the physical memory can be arbitrarily assigned to different processes, the table <strong class="markup--strong markup--p-strong">isn’t ordered</strong>. So the problem is that we have to perform a linear search in order to find the corresponding PID and page number in the inverted page table. You can imagine that the linear search is really slow and it impacts our performance in general, even though TLB handles most of the V2P translations.</p><p name="00c7" id="00c7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) Hashing Inverted Page Tables</strong></p><p name="86a2" id="86a2" class="graf graf--p graf-after--p">We have discussed that it is such a serious problem for us if we need to do a linear search. To improve the performance, we can consider using the <strong class="markup--strong markup--p-strong">hash table</strong>. Instead of linear searching the whole inverted page table, we will replace it with a hash table with linked lists correspond to each entry of this table. We will conduct hashing on the PID and the page number and then work out a value to indexing the hash table. From this table, we can get a pointer that points to the data bucket. Then we will search the PID and the page number in this bucket and if we find it, we will get the frame number associated with this element in the bucket.</p><figure name="bb7f" id="bb7f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ZbYXv2X8s97fwZ62jVQgrw.png" data-width="1760" data-height="732" src="https://cdn-images-1.medium.com/max/800/1*ZbYXv2X8s97fwZ62jVQgrw.png"></figure><p name="fb78" id="fb78" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(11) Choosing Page Size</strong></p><p name="127c" id="127c" class="graf graf--p graf-after--p">There are some benefits to have a larger page size, for example, we can have</p><ul class="postList"><li name="87cd" id="87cd" class="graf graf--li graf-after--p">fewer page table entries</li><li name="8b71" id="8b71" class="graf graf--li graf-after--li">smaller page tables</li><li name="737d" id="737d" class="graf graf--li graf-after--li">more TLB hits</li><li name="9676" id="9676" class="graf graf--li graf-after--li">etc.</li></ul><p name="1ad6" id="1ad6" class="graf graf--p graf-after--li">However, there can also be some <strong class="markup--strong markup--p-strong">internal fragmentation </strong>and the memory can be wasted because of that. Thus, we will commonly have a 4KB page size.</p><p name="caf8" id="caf8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Segmentation</strong></p><p name="f44c" id="f44c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) An Introduction to Segmentation</strong></p><p name="e852" id="e852" class="graf graf--p graf-after--p">Instead of pages, the V2P translation can also be performed using <strong class="markup--strong markup--p-strong">segments</strong>. In this section, we are going to briefly introduce this approach. Actually, the process is referred to as <strong class="markup--strong markup--p-strong">segmentation</strong>.</p><p name="4e1f" id="4e1f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Segmentation Features</strong></p><p name="33d9" id="33d9" class="graf graf--p graf-after--p">With segments, the address space is divided into components of <strong class="markup--strong markup--p-strong">arbitrary granularity</strong> of arbitrary size. Typically, the different segments will correspond to some logically meaningful components of the address space like</p><ul class="postList"><li name="b31a" id="b31a" class="graf graf--li graf-after--p">the code</li><li name="d8a6" id="d8a6" class="graf graf--li graf-after--li">the heap</li><li name="9f4d" id="9f4d" class="graf graf--li graf-after--li">the data</li><li name="7f41" id="7f41" class="graf graf--li graf-after--li">etc</li></ul><p name="6d98" id="6d98" class="graf graf--p graf-after--li">A segment could be represented with a <strong class="markup--strong markup--p-strong">contiguous portion of physical memory</strong>. In that case, the segment would be defined with its <strong class="markup--strong markup--p-strong">base address</strong> and its <strong class="markup--strong markup--p-strong">limit registers</strong> which implies also the <strong class="markup--strong markup--p-strong">segment size</strong>.</p><p name="0704" id="0704" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Segmentation Implementation</strong></p><p name="d12b" id="d12b" class="graf graf--p graf-after--p">A virtual address in the segmented memory mode includes a <strong class="markup--strong markup--p-strong">segment descriptor</strong> and an <strong class="markup--strong markup--p-strong">offset</strong>. The segment descriptor is used in combination with the <strong class="markup--strong markup--p-strong">descriptor table</strong> to produce information regarding the physical address of the segment.</p><p name="9c9c" id="9c9c" class="graf graf--p graf-after--p">The result of the segmentation is called a linear address instead of a physical address, so segmentation must be used with paging.</p><figure name="2268" id="2268" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DFvt1T9gn0qvejM6IKW_nQ.png" data-width="1760" data-height="1286" src="https://cdn-images-1.medium.com/max/800/1*DFvt1T9gn0qvejM6IKW_nQ.png"></figure><p name="3f39" id="3f39" class="graf graf--p graf-after--figure">Briefly, this is equivalent for us to have,</p><figure name="3c3f" id="3c3f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ZTKOn2yrjD18hd9Gex5BVA.png" data-width="946" data-height="110" src="https://cdn-images-1.medium.com/max/800/1*ZTKOn2yrjD18hd9Gex5BVA.png"></figure><p name="8ae5" id="8ae5" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Segmentation Example</strong></p><p name="1dad" id="1dad" class="graf graf--p graf-after--p">If we look at the intel platforms, the x86 platforms,</p><ul class="postList"><li name="778c" id="778c" class="graf graf--li graf-after--p">on 32-bit hardware: both <strong class="markup--strong markup--li-strong">segmentation</strong> and <strong class="markup--strong markup--li-strong">paging</strong> are supported. Linux allows up to 8000 segments to be available per process and another 8000 global segments.</li><li name="abbc" id="abbc" class="graf graf--li graf-after--li">on 64 bit hardware: even though segmentation and paging are supported for backward compatibility, the <strong class="markup--strong markup--li-strong">default mode is to just use paging</strong></li></ul><p name="2a32" id="2a32" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">4. Memory Allocation</strong></p><p name="bb95" id="bb95" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of Memory Allocation</strong></p><p name="1975" id="1975" class="graf graf--p graf-after--p">We have discussed how to do a V2P translation, but what we didn’t explain was how the OS allocates the memory. This is the job of the <strong class="markup--strong markup--p-strong">memory allocation</strong> mechanisms that are part of the memory management subsystem of an OS, which answers what are the physical addresses that will correspond to a specific virtual address. This mapping is established by the <strong class="markup--strong markup--p-strong">memory allocator</strong>.</p><p name="beea" id="beea" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Kernel-Level and User-Level Memory Allocator</strong></p><p name="aeea" id="aeea" class="graf graf--p graf-after--p">Memory allocators can exist at both the kernel level and the user level.</p><p name="3f69" id="3f69" class="graf graf--p graf-after--p">Kernel-level memory allocators are responsible for allocating memory regions such as,</p><ul class="postList"><li name="0ca2" id="0ca2" class="graf graf--li graf-after--p">kernel pages for various components of the <strong class="markup--strong markup--li-strong">kernel state</strong></li><li name="d63c" id="d63c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">static state </strong>for the processes when they’re created (e.g. for code, stack, or initialize data)</li><li name="c99b" id="c99b" class="graf graf--li graf-after--li">keep track of the <strong class="markup--strong markup--li-strong">free memory</strong> that’s available in the system</li></ul><p name="a7c6" id="a7c6" class="graf graf--p graf-after--li">User-level memory allocators are used for dynamic process states like,</p><ul class="postList"><li name="1877" id="1877" class="graf graf--li graf-after--p">the heaps (i.e. the dynamic process state)</li><li name="a979" id="a979" class="graf graf--li graf-after--li">basic interface: <code class="markup--code markup--li-code">malloc</code> / <code class="markup--code markup--li-code">free</code></li></ul><p name="3efe" id="3efe" class="graf graf--p graf-after--li">Once a kernel allocates some memory to a <code class="markup--code markup--p-code">malloc</code> call, the kernel is no longer involved in the management of that space. That memory will at that point be used by whatever user-level allocator is being used, and there are a number of options out there right now that have certain different trade-offs. We will list some of them here but we are not going to discuss the detailed usage of them. These options are,</p><ul class="postList"><li name="a5c0" id="a5c0" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">dlmalloc</code></li><li name="10cd" id="10cd" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">jemalloc</code></li><li name="d6f2" id="d6f2" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">hoard</code></li><li name="5f34" id="5f34" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">tcmalloc</code></li></ul><p name="34e3" id="34e3" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) External Fragmentation</strong></p><p name="e023" id="e023" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">External fragmentation</strong> can happen if we must allocate a range of coutinous memory. Let’s see an example. Suppose we have 16 frames in the physical memory and we then conduct the following code,</p><pre name="c6bc" id="c6bc" class="graf graf--pre graf-after--p">L1: alloc(2), alloc(4), alloc(4), alloc(4)<br>L2: free(2)<br>L3: alloc(4)</pre><p name="cff1" id="cff1" class="graf graf--p graf-after--pre">If the physical frames are allocated in order from the beginning of the physical memory, then after L1 and L2, we will have,</p><figure name="2924" id="2924" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4Aw-mmRY_mO6j4qT0huPJg.png" data-width="1788" data-height="702" src="https://cdn-images-1.medium.com/max/800/1*4Aw-mmRY_mO6j4qT0huPJg.png"></figure><p name="81ed" id="81ed" class="graf graf--p graf-after--figure">We can find out that even though we have 4 free frames, we can not allocate these 4 frames at a time because these frames are not continuous. Thus, the memory allocator can not meet the requirement for L3. One downside of the external fragmentation is that we will have holes of free memory that are not contiguous. Thus, even we have enough space for allocation, the memory allocator can not conduct that.</p><p name="086c" id="086c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) External Fragmentation: Solution</strong></p><p name="a701" id="a701" class="graf graf--p graf-after--p">Let’s consider an alternative, in which the memory allocator probably knows something about the requests that are coming. So in the 2nd case, when the 2nd request for allocating 4 pages comes, the memory allocator isn’t allocating it immediately after the first allocation, but instead, it’s leaving some gap.</p><figure name="9f90" id="9f90" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Xw1ybfi8Zjpr0VuHpNUSWQ.png" data-width="1788" data-height="702" src="https://cdn-images-1.medium.com/max/800/1*Xw1ybfi8Zjpr0VuHpNUSWQ.png"></figure><p name="c10f" id="c10f" class="graf graf--p graf-after--figure">Now when the free request comes in, these 2 first pages are freed. The system again has 4 free pages and they’re consecutive. Therefore this next request for 4 free pages can actually be satisfied in the system.</p><p name="20c8" id="20c8" class="graf graf--p graf-after--p">What we see in this example, is that when these pages are freed there was an opportunity for the allocator to coalesce or to aggregate these adjacent areas of free pages into one larger free area in order to satisfy the need of future allocate requirements.</p><p name="a787" id="a787" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Linux Memory Allocators #1: Buddy Memory Allocator</strong></p><p name="ab52" id="ab52" class="graf graf--p graf-after--p">Basically, the Linux OS has two kinds of memory allocators that can deal with external fragmentation problems. These memory allocators are called the <strong class="markup--strong markup--p-strong">buddy allocator </strong>and the <strong class="markup--strong markup--p-strong">slab allocator</strong>. We will first have a look at the buddy allocator.</p><p name="87fd" id="87fd" class="graf graf--p graf-after--p">The buddy allocator starts with some consecutive memory region that’s free that’s of a size of 2ⁿ. Whenever a request comes in, the allocator will subdivide this large request into <strong class="markup--strong markup--p-strong">smaller chunks</strong> such that every one of them is also a 2ⁿ. Then it will continue subdividing until it finds the smallest chunk that’s with a size of a power of 2ⁿ that can satisfy the request.</p><p name="8191" id="8191" class="graf graf--p graf-after--p">Let’s see an example. Suppose we want to allocate 8 frames and we have a 64-frame physical space. To have a larger continuous memory region, we will divide the memory region and allocate memory from the bottom of the physical space.</p><figure name="dd25" id="dd25" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*H8aQLIuS4R3ol8cbC8EVPw.png" data-width="1492" data-height="426" src="https://cdn-images-1.medium.com/max/800/1*H8aQLIuS4R3ol8cbC8EVPw.png"></figure><p name="eb01" id="eb01" class="graf graf--p graf-after--figure">The reason why these areas are 2ⁿ is that the addresses of each of the buddies differ by only 1 bit, which makes it easier to perform the necessary checks when combining or splitting chunks.</p><p name="2380" id="2380" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Buddy Memory Allocator Evaluation</strong></p><p name="bbad" id="bbad" class="graf graf--p graf-after--p">The benefit of the buddy algorithm is that the aggregation of the free areas can be performed really well and really <strong class="markup--strong markup--p-strong">fast</strong>.</p><p name="d3ad" id="d3ad" class="graf graf--p graf-after--p">However, the fact that allocations using the buddy algorithm have to be made in the granularity of 2ⁿ means that there will be some <strong class="markup--strong markup--p-strong">internal fragmentation</strong> using the buddy allocator. This is particularly a problem because there are a lot of data structures that are common in the Linux kernel that are not of a size that’s close to 2ⁿ. This is why we have the slab allocator.</p><p name="ebbe" id="ebbe" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Linux Memory Allocators #2: Slub Memory Allocator</strong></p><p name="5cc4" id="5cc4" class="graf graf--p graf-after--p">To fix the internal fragmentation issues, Linux also uses the <strong class="markup--strong markup--p-strong">slab allocator</strong> in the kernel. The allocator builds custom object <strong class="markup--strong markup--p-strong">caches</strong> on top of slabs, and the <strong class="markup--strong markup--p-strong">slabs</strong> themselves represent contiguously allocated physical memory.</p><p name="4b77" id="4b77" class="graf graf--p graf-after--p">When the kernel starts it will pre-create caches for the different object types. For instance, it will have a cache for task_struct or for the directory entry objects, then when an allocation comes for a particular object type then it will go straight to the cache and it will use one of the elements in this cache. If none of the entries is available, then the kernel will create another slab and it will pre-allocate an additional portion of contiguous physical memory to be managed by the slab allocator.</p><figure name="e7d2" id="e7d2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4WJeZm9quOjRJtFL0QHfbQ.png" data-width="1356" data-height="490" src="https://cdn-images-1.medium.com/max/800/1*4WJeZm9quOjRJtFL0QHfbQ.png"></figure><p name="b865" id="b865" class="graf graf--p graf-after--figure">The benefits of the slab allocator are,</p><ul class="postList"><li name="a2ff" id="a2ff" class="graf graf--li graf-after--p">it avoids internal fragmentation</li><li name="74ce" id="74ce" class="graf graf--li graf-after--li">external fragmentation is not really an issue</li></ul><p name="e2d2" id="e2d2" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">5. Demand Paging</strong></p><p name="687a" id="687a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) The Definition of Demand Paging</strong></p><p name="273e" id="273e" class="graf graf--p graf-after--p">Because the virtual address space can be much larger than the physical memory, so sometimes we have to store some information on the secondary storage (e.g. disk) and when we need them, we can load them back from the secondary storage. This is called <strong class="markup--strong markup--p-strong">demand paging</strong>. The procedure of demand paging is illustrated in the following diagram.</p><figure name="99a0" id="99a0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PjZohPWYbUn_UxuD4XI6sw.png" data-width="1486" data-height="576" src="https://cdn-images-1.medium.com/max/800/1*PjZohPWYbUn_UxuD4XI6sw.png"></figure><p name="c255" id="c255" class="graf graf--p graf-after--figure">Finally, we need to have 2 final insights,</p><ul class="postList"><li name="4ac3" id="4ac3" class="graf graf--li graf-after--p">First, the original physical address of this page will very likely be <strong class="markup--strong markup--li-strong">different</strong> from the physical address after this demand paging process was over</li><li name="60a1" id="60a1" class="graf graf--li graf-after--li">Second, if for whatever reason we require a page to be <strong class="markup--strong markup--li-strong">constantly present in memory</strong> or if we require it to maintain this <strong class="markup--strong markup--li-strong">same physical address</strong> during its lifetime, then we will have to <strong class="markup--strong markup--li-strong">pin</strong> the page. So at that point, we basically <strong class="markup--strong markup--li-strong">disable the swapping</strong>. This is for instance useful when the CPU is interacting with devices that support direct memory access (DMA).</li></ul><p name="f549" id="f549" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(9) Page Replacement: When to Swap Out?</strong></p><p name="8715" id="8715" class="graf graf--p graf-after--p">There are two remaining problems of the demand paging,</p><ul class="postList"><li name="1997" id="1997" class="graf graf--li graf-after--p">When should the pages be swapped out from the physical memory onto the disk?</li><li name="83ba" id="83ba" class="graf graf--li graf-after--li">Which pages should be swapped out?</li></ul><p name="35d4" id="35d4" class="graf graf--p graf-after--li">Let’s first answer the first problem. Periodically, when the amount of occupied memory reaches a particular <strong class="markup--strong markup--p-strong">threshold</strong>, the OS will run some <strong class="markup--strong markup--p-strong">page(out) daemon</strong> that will look for pages that can be freed. So in the following two cases, we are going to have some pages swapped out,</p><ul class="postList"><li name="e014" id="e014" class="graf graf--li graf-after--p">when <strong class="markup--strong markup--li-strong">memory usage</strong> is above the threshold, this is called high watermark</li><li name="59a8" id="59a8" class="graf graf--li graf-after--li">when <strong class="markup--strong markup--li-strong">CPU usage</strong> is below the threshold, this is called low watermark</li></ul><p name="3cdd" id="3cdd" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(10) Page Replacement: Which to Swap Out?</strong></p><p name="63a4" id="63a4" class="graf graf--p graf-after--p">To answer the second question, one obvious answer would be that the pages that will not be used in the future are the ones that should be swapped out. The problem is how do we know which pages will vs won’t be used in the future? The answer is that we have to make some <strong class="markup--strong markup--p-strong">predictions</strong> regarding the page usage by some <strong class="markup--strong markup--p-strong">historic information</strong>.</p><p name="755e" id="755e" class="graf graf--p graf-after--p">One common algorithm is called the <strong class="markup--strong markup--p-strong">least recently used (LRU) policy</strong>, which will remove the page that hasn’t been accessed for a long time. This can rely on the <strong class="markup--strong markup--p-strong">access bit</strong>.</p><p name="974c" id="974c" class="graf graf--p graf-after--p">Another case is that when the page doesn’t need to be written out, we can swap them out with no disk written overheads. This can rely on the <strong class="markup--strong markup--p-strong">dirty bit</strong>.</p><p name="c859" id="c859" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">6. Hardware Supported Techniques</strong></p><p name="ae2f" id="ae2f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Copy-On-Write (COW)</strong></p><p name="22fe" id="22fe" class="graf graf--p graf-after--p">Let’s see what happens during process creation. When we need to create a new process, we need to recreate the entire parent process by copying its entire address space. However, because many of the pages don’t change, we don’t need to keep multiple copies.</p><p name="d2f8" id="d2f8" class="graf graf--p graf-after--p">In order to avoid unnecessary copying, the newly created process will be mapped to the original page that had the original address space content. If the contents of this page are read-only, then we’re going to save both the memory and time for doing this copy.</p><figure name="52e4" id="52e4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fsXs-MZW6jHsmi_sZQ3f4w.png" data-width="1486" data-height="536" src="https://cdn-images-1.medium.com/max/800/1*fsXs-MZW6jHsmi_sZQ3f4w.png"></figure><p name="dd71" id="dd71" class="graf graf--p graf-after--figure">However, if a write request is issued for this memory area via either one of these virtual addresses, then the MMU would detect that the page is <strong class="markup--strong markup--p-strong">write protecte</strong>d and will generate a <strong class="markup--strong markup--p-strong">page fault</strong>. At that point, the OS will see what is the reason for this page fault, will create the actual copy.</p><figure name="6cdf" id="6cdf" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fzZa4IbrTp5vSKXSUR_Bog.png" data-width="1486" data-height="536" src="https://cdn-images-1.medium.com/max/800/1*fzZa4IbrTp5vSKXSUR_Bog.png"></figure><p name="74ca" id="74ca" class="graf graf--p graf-after--figure">We call this mechanism <strong class="markup--strong markup--p-strong">copy-on-write</strong> because the copy cost will only be paid when we need to perform a <strong class="markup--strong markup--p-strong">write operation</strong>.</p><p name="cba5" id="cba5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Checkpointing</strong></p><p name="2be7" id="2be7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Checkpointing</strong> is a technique that’s used as part of the <strong class="markup--strong markup--p-strong">failure and recovery management</strong> that the OS supports. The idea behind checkpointing is to periodically save the entire process state. The failure may be unavoidable, however, with checkpointing the process doesn’t have to be restarted from the beginning. It can be restarted from the nearest checkpoint point and so the recovery will be much faster.</p><p name="c914" id="c914" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Checkpointing Implementation</strong></p><p name="85d9" id="85d9" class="graf graf--p graf-after--p">A simple approach to checkpointing would be to pause the execution of the process and copy its entire state.</p><p name="dc71" id="dc71" class="graf graf--p graf-after--p">A better approach will take advantage of the hardware support for memory management. Using the hardware support, we can <strong class="markup--strong markup--p-strong">write-protect</strong> the entire address space of the process and try to copy everything at once. However since the process will continue executing, it will continue dirtying pages. So then we can track the <strong class="markup--strong markup--p-strong">dirtied pages</strong> again using MMU and we will copy only the difference, which will allow us to provide for an incremental checkpoint.</p><p name="cbca" id="cbca" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Rewind Replay</strong></p><p name="4cd1" id="4cd1" class="graf graf--p graf-after--p">Debugging relies often on a technique called <strong class="markup--strong markup--p-strong">rewind replay</strong>.<strong class="markup--strong markup--p-strong"> Rewind</strong> means that we will restart the execution of the same process from some earlier point. We can gradually go back to older and older checkpoints until we find the error.</p><p name="4516" id="4516" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Migration</strong></p><p name="c02e" id="c02e" class="graf graf--p graf-after--p graf--trailing">With <strong class="markup--strong markup--p-strong">migration</strong>, it’s like we checkpoint the process to another machine and then we restart it on the other machine, then this process will continue its execution. This is useful in scenarios such as <strong class="markup--strong markup--p-strong">disaster recovery</strong> and <strong class="markup--strong markup--p-strong">consolidation</strong>.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/6b819b9de555"><time class="dt-published" datetime="2021-03-19T16:20:28.209Z">March 19, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/operating-system-17-memory-management-pages-and-page-tables-segmentation-memory-allocation-6b819b9de555" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>