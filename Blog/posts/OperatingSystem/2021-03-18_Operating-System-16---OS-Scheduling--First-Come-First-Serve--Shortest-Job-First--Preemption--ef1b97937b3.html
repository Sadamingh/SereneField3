<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Operating System 16 | OS Scheduling, First-Come&amp;First-Serve, Shortest-Job First, Preemption…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Operating System 16 | OS Scheduling, First-Come&amp;First-Serve, Shortest-Job First, Preemption…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Operating System
</section>
<section data-field="body" class="e-content">
<section name="362e" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="373d" id="373d" class="graf graf--h3 graf--leading graf--title">Operating System 16 | OS Scheduling, <strong class="markup--strong markup--h3-strong">First-Come&amp;First-Serve,</strong> <strong class="markup--strong markup--h3-strong">Shortest-Job First, Preemption, Priority, Timeslicing, Runqueue, Linux Scheduler, and Scheduling on Multi-CPU Systems</strong></h3><figure name="4ed7" id="4ed7" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*x0v8q-CjojCQgxTz.png" data-width="1508" data-height="794" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*x0v8q-CjojCQgxTz.png"></figure><ol class="postList"><li name="1c53" id="1c53" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Introduction to OS Schedular</strong></li></ol><p name="19b4" id="19b4" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) The Definition of Tasks</strong></p><p name="986b" id="986b" class="graf graf--p graf-after--p">In this section, we will use the term <strong class="markup--strong markup--p-strong">tasks </strong>to interchangeably mean either <strong class="markup--strong markup--p-strong">processes or threads</strong>.</p><p name="f240" id="f240" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Definition of the Scheduler</strong></p><p name="5cd0" id="5cd0" class="graf graf--p graf-after--p">The CPU scheduler is a process that decides how and when the tasks access the shared CPUs. The scheduler considers both the ULPs and ULTs as well as the KLTs.</p><p name="190b" id="190b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) CPU Scheduler Functions</strong></p><p name="3ab8" id="3ab8" class="graf graf--p graf-after--p">Recall this figure we used when we talked originally about the processes and process scheduling,</p><figure name="eca7" id="eca7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*T1XF6l4R6sVRLoJt.png" data-width="1672" data-height="518" src="https://cdn-images-1.medium.com/max/800/0*T1XF6l4R6sVRLoJt.png"></figure><p name="31d3" id="31d3" class="graf graf--p graf-after--figure">the responsibility of the CPU scheduler is to,</p><ul class="postList"><li name="6b15" id="6b15" class="graf graf--li graf-after--p">When the CPU becomes <strong class="markup--strong markup--li-strong">idle</strong>, choose one of the tasks in the <strong class="markup--strong markup--li-strong">ready queue </strong>(which is implemented by a runqueue data structure)</li><li name="e116" id="e116" class="graf graf--li graf-after--li">The scheduler <strong class="markup--strong markup--li-strong">dispatches</strong> this task onto the CPU. This means that it will perform a context switch, enter the user mode, set the PC appropriately, and we are ready to execute this task on the CPU.</li></ul><p name="7493" id="7493" class="graf graf--p graf-after--li">Tasks may become <strong class="markup--strong markup--p-strong">ready </strong>in the<strong class="markup--strong markup--p-strong"> ready queue </strong>so they may enter the ready queue after,</p><ul class="postList"><li name="3705" id="3705" class="graf graf--li graf-after--p">an <strong class="markup--strong markup--li-strong">I/O operation</strong> they have been waiting on has completed</li><li name="d07d" id="d07d" class="graf graf--li graf-after--li">they have been woken up from a wait for an <strong class="markup--strong markup--li-strong">interrupt</strong></li><li name="0feb" id="0feb" class="graf graf--li graf-after--li">when somebody has <strong class="markup--strong markup--li-strong">forked</strong> a new thread</li><li name="6eba" id="6eba" class="graf graf--li graf-after--li">the tasks with their <strong class="markup--strong markup--li-strong">time slice expired</strong></li></ul><p name="14b1" id="14b1" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Three Options for Task Selection</strong></p><p name="a4d6" id="a4d6" class="graf graf--p graf-after--p">For an OS scheduler, the choices of how to schedule are analyzed in terms of tasks. These are being scheduled onto the CPUs that are managed by the OS scheduler. Basically, we can have three options to handle tasks,</p><ul class="postList"><li name="00c1" id="00c1" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Assign tasks immediately</strong>: for an OS scheduler, we can choose a simple approach to schedule tasks in a <strong class="markup--strong markup--li-strong">first-come, first-serve (FCFS, aka. FIFO)</strong> manner. This is quite simple and we don’t have to spend overheads on the scheduler itself.</li><li name="80f2" id="80f2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Assign simple tasks first</strong>: the OS scheduler can also assign or dispatch simple tasks first. The outcome of this kind of schedule can be that the <strong class="markup--strong markup--li-strong">throughput</strong> of the system overall is maximized. The schedulers that actually follow this kind of algorithm are called <strong class="markup--strong markup--li-strong">shortest-job first (SJF)</strong> schedulers.</li><li name="bc98" id="bc98" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Assign complex tasks first</strong>: the OS schedular can also assign or dispatch complex first. The goal of the scheduler is to <strong class="markup--strong markup--li-strong">maximize the utilization of all the resources</strong> of the system (e.g. CPUs, memory, other devices, etc.).</li></ul><p name="61e6" id="61e6" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(5) Scheduling Performance</strong></p><p name="7bc5" id="7bc5" class="graf graf--p graf-after--p">Since we will be talking about different scheduling algorithms, it will be important for us to have some metrics. These meaningful metrics include</p><ul class="postList"><li name="6385" id="6385" class="graf graf--li graf-after--p">throughput</li><li name="c8b8" id="c8b8" class="graf graf--li graf-after--li">the average time it took for tasks to complete</li><li name="19c3" id="19c3" class="graf graf--li graf-after--li">the average time that tasks spent waiting before they were scheduled</li><li name="f01a" id="f01a" class="graf graf--li graf-after--li">the overall CPU utilization</li></ul><p name="edb8" id="edb8" class="graf graf--p graf-after--li">We will use some of these metrics to compare some of the algorithms that we will talk about.</p><p name="f22f" id="f22f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Run-to-Completion Scheduling</strong></p><p name="bf22" id="bf22" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Run-to-Completion Scheduling</strong></p><p name="7019" id="7019" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">run-to-completion scheduling</strong> assumes that as soon as a task is assigned to a CPU, it will run until it finishes or until it completes. For this scheduling, we will have the following assumptions,</p><ul class="postList"><li name="430d" id="430d" class="graf graf--li graf-after--p">we have a <strong class="markup--strong markup--li-strong">group</strong> of tasks we have to schedule</li><li name="214a" id="214a" class="graf graf--li graf-after--li">we know exactly how much time these tasks need to execute</li><li name="0edb" id="0edb" class="graf graf--li graf-after--li">no <strong class="markup--strong markup--li-strong">preemptions/interrupts</strong> in the system</li><li name="cfb0" id="cfb0" class="graf graf--li graf-after--li">single CPU</li></ul><p name="4373" id="4373" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Algorithm #1: First-Come, First-Serve (FCFS)</strong></p><p name="8d5e" id="8d5e" class="graf graf--p graf-after--p">The first and simplest algorithm we are going to discuss is the <strong class="markup--strong markup--p-strong">first-come, first-serve (FCFS) algorithm</strong>. In this algorithm, tasks are scheduled on the CPU in the <strong class="markup--strong markup--p-strong">same order</strong> in which they arrive, regardless of their execution time, of load in the system, or anything else.</p><p name="a6b7" id="a6b7" class="graf graf--p graf-after--p">Whenever a new task becomes ready, it will be placed at the end of the run queue. Then whenever the scheduler needs to pick the next task to execute, it will pick from the front of the queue.</p><figure name="f128" id="f128" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*h3JckErZKhjyBQzx8gKrIw.png" data-width="1394" data-height="538" src="https://cdn-images-1.medium.com/max/800/1*h3JckErZKhjyBQzx8gKrIw.png"></figure><p name="2393" id="2393" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Algorithm #2: Shortest-Job First (SJF)</strong></p><p name="d82b" id="d82b" class="graf graf--p graf-after--p">Another algorithm that’s called <strong class="markup--strong markup--p-strong">Shortest Job First (SJF)</strong> and the goal of this algorithm is to schedule the tasks <strong class="markup--strong markup--p-strong">in the order of a short-to-long execution time </strong>sequence. This schedular can be implemented by a <strong class="markup--strong markup--p-strong">balanced tree</strong> instead of a queue because it will not follow the FIFO rule. We can also use an <strong class="markup--strong markup--p-strong">order queue</strong> that sorts the elements every time a new task is put in the queue, but this will be more complex than a balanced tree.</p><figure name="003f" id="003f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*uaGRQsSl51OWdmENWSn3Yw.png" data-width="1394" data-height="624" src="https://cdn-images-1.medium.com/max/800/1*uaGRQsSl51OWdmENWSn3Yw.png"></figure><p name="ea39" id="ea39" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Algorithm #3: Complex First</strong></p><p name="0c8b" id="0c8b" class="graf graf--p graf-after--p">We can also call <strong class="markup--strong markup--p-strong">the most complex one</strong> first so as to make full use of the system resources.</p><figure name="69c4" id="69c4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*eqymc3trU5a4tLWosD3gMQ.png" data-width="1394" data-height="590" src="https://cdn-images-1.medium.com/max/800/1*eqymc3trU5a4tLWosD3gMQ.png"></figure><p name="d4ac" id="d4ac" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Scheduling Performance for Algorithms: An Example</strong></p><p name="525c" id="525c" class="graf graf--p graf-after--p">Now, let’s see an example about the performance of these algorithms. Suppose we have three tasks T1, T2, and T3, which have the following execution time,</p><pre name="3b58" id="3b58" class="graf graf--pre graf-after--p">T1 =  1 s<br>T2 = 10 s<br>T3 =  1 s</pre><p name="6db3" id="6db3" class="graf graf--p graf-after--pre">Let’s assume that they will arrive in the order of <code class="markup--code markup--p-code">T1 -&gt; T2 -&gt; T3</code> so that the performance of the three algorithms will be,</p><figure name="aa20" id="aa20" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*TJl6P4EZ42zae8CIaY6Bqg.png" data-width="1562" data-height="272" src="https://cdn-images-1.medium.com/max/800/1*TJl6P4EZ42zae8CIaY6Bqg.png"></figure><p name="9858" id="9858" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">3. Preemptive Scheduling</strong></p><p name="b7b7" id="b7b7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of Preemptive Scheduling</strong></p><p name="2825" id="2825" class="graf graf--p graf-after--p">So far in this discussion, we assumed that a task that’s executing on the CPU cannot be interrupted or preempted. Let’s now relax that requirement and start talking about <strong class="markup--strong markup--p-strong">preemptive scheduling</strong>, in which the tasks don’t hog the CPU until they’re completed.</p><p name="3b21" id="3b21" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) SJF + Preemption With Execution Time</strong></p><p name="f2f9" id="f2f9" class="graf graf--p graf-after--p">Now, let’s see an example of preemption scheduling with the SJF algorithm. The T1 and T3 thread will arrive 2 seconds after T1 arrives. Thus, we can have,</p><pre name="c543" id="c543" class="graf graf--pre graf-after--p">Task         ExeTime        ArrTime<br>------       ---------      ---------<br>T1           1              2<br>T2           10             0<br>T3           1              2</pre><p name="dc8d" id="dc8d" class="graf graf--p graf-after--pre">Because T2 arrives first, it will be executed in the first place. However, when T1 and T3 arrive, they will be preempted and scheduled to T1 and T3 because we have to follow the SJF algorithm. When T1 and T3 complete, we can take the remaining time to execute T2. Thus, the execution chart will be,</p><figure name="e2a6" id="e2a6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jDaQOeuU9H7-KojrPl-RHw.png" data-width="1394" data-height="382" src="https://cdn-images-1.medium.com/max/800/1*jDaQOeuU9H7-KojrPl-RHw.png"></figure><p name="ce94" id="ce94" class="graf graf--p graf-after--figure">So basically what would happen is whenever tasks enter the run queue like T1 and T3, the scheduler needs to be <strong class="markup--strong markup--p-strong">invoked</strong>. So it can inspect the execution times, and then decide whether to <strong class="markup--strong markup--p-strong">preempt</strong> the currently running task (T2) and schedule one of the newly ready tasks.</p><p name="c86b" id="c86b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) SJF + Preemption Without Execution Time</strong></p><p name="1f92" id="1f92" class="graf graf--p graf-after--p">So far we talked as if we know the execution time of the task, but it’s really hard to claim that we know the execution time of a task. It depends on a number of factors, for example,</p><ul class="postList"><li name="383a" id="383a" class="graf graf--li graf-after--p">the inputs of the task</li><li name="38b4" id="38b4" class="graf graf--li graf-after--li">whether the data is present in the cache or not</li><li name="c1ed" id="c1ed" class="graf graf--li graf-after--li">which other tasks are running in the system</li><li name="a18c" id="a18c" class="graf graf--li graf-after--li">etc.</li></ul><p name="fbcf" id="fbcf" class="graf graf--p graf-after--li">Therefore, we have to use some kind of heuristics in order to guesstimate what the execution time of a task will be and <strong class="markup--strong markup--p-strong">history</strong> is a good predictor of what will happen. For instance, we can think about,</p><ul class="postList"><li name="8228" id="8228" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Point Estimate</strong>: how long a task ran the very last time it was executed?</li><li name="a149" id="a149" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Windowed Average</strong>: how long a task ran the last N time it was executed on average? The windowed average means</li></ul><p name="7d78" id="7d78" class="graf graf--p graf-after--li">So we compute some kind of metrics based on a window of values from the past and use that average for the prediction of the behavior of the task in the future.</p><p name="d4fb" id="d4fb" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) SJF + Preemption With Priority</strong></p><p name="0046" id="0046" class="graf graf--p graf-after--p">SJF Priority is another way to decide how to schedule tasks and when to preempt a particular task, and tasks that have different priority levels, that is a pretty common situation. For instance, several operating system-level threads run OS tasks that would have higher priority than other threads that support by ULPs. With priorities, the schedulers will have to run the task with the <strong class="markup--strong markup--p-strong">highest priority in the first place</strong>. It is clear that <strong class="markup--strong markup--p-strong">preemption</strong> will be needed for this implementation. This scenario is also called <strong class="markup--strong markup--p-strong">priority scheduling</strong>.</p><p name="006b" id="006b" class="graf graf--p graf-after--p">Now, let’s see an example.</p><pre name="50c8" id="50c8" class="graf graf--pre graf-after--p">Task         ExeTime        ArrTime       Priority<br>------       ---------      ---------     ----------<br>T1           1              2             P1<br>T2           10             0             P2<br>T3           1              2             P3</pre><p name="1cc9" id="1cc9" class="graf graf--p graf-after--pre">In this particular example, the priorities P1, P2, and P3 are such that the first thread P1 has the lowest priority. Again, we start with the execution of T2 since it’s the only thread in the system. However, when T1 and T3 become ready at time 2, we will look at the priorities and see that T3 has the highest priority. So when threads T1 and T3 are ready, T2 is preempted and the execution of T3 will start. When T3 completes, T2 starts running again, and then T1 will have to wait until T2 completes to start.</p><figure name="95da" id="95da" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Av_yYopzFptciiG5JkmBCg.png" data-width="1394" data-height="382" src="https://cdn-images-1.medium.com/max/800/1*Av_yYopzFptciiG5JkmBCg.png"></figure><p name="ee0d" id="ee0d" class="graf graf--p graf-after--figure">If the scheduler needs to be a priority-based scheduler, it will somehow need to quickly access the priorities and the execution context in order to select the one that has the highest priority to execute next.</p><p name="4208" id="4208" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Priority Scheduling Implementation</strong></p><p name="a61e" id="a61e" class="graf graf--p graf-after--p">To implement priority scheduling, we can have <strong class="markup--strong markup--p-strong">multiple run queue</strong> structures for each priority level. And then have the scheduler select a task from the run queue that corresponds to the <strong class="markup--strong markup--p-strong">highest priority level</strong>.</p><figure name="4887" id="4887" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*hSP2lqPd3pcimGQI6kKMzw.png" data-width="1442" data-height="326" src="https://cdn-images-1.medium.com/max/800/1*hSP2lqPd3pcimGQI6kKMzw.png"></figure><p name="7d31" id="7d31" class="graf graf--p graf-after--figure">Another option is to have some ordered data structures like the <strong class="markup--strong markup--p-strong">balanced tree</strong> that we have discussed.</p><p name="de72" id="de72" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Priority Scheduling Danger #1: Starvation</strong></p><p name="c505" id="c505" class="graf graf--p graf-after--p">Basically, there can be a possible danger called starvation if we want to have priority scheduling. <strong class="markup--strong markup--p-strong">Starvation</strong> means that a low priority task can be stuck in a run queue just because there are constantly higher priority tasks show up in some of the other parts of the run queue.</p><p name="ff29" id="ff29" class="graf graf--p graf-after--p">One mechanism that we use to protect against starvation is so-called <strong class="markup--strong markup--p-strong">priority aging</strong>, which means that the priority of the task isn’t just a fixed priority. Instead, it is a function of the actual priority and the time spent in the run queue.</p><figure name="0de4" id="0de4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fOASNX5dpmmEXAq2ULz6Yg.png" data-width="1242" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*fOASNX5dpmmEXAq2ULz6Yg.png"></figure><p name="8651" id="8651" class="graf graf--p graf-after--figure">The idea is that the longer a task spends in a run queue, the higher the priority should become. So eventually, the task will become the highest priority task in the system and it will run. In this manner, starvation will be prevented.</p><p name="bb7d" id="bb7d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Priority Scheduling Danger #2: Priority Inversion</strong></p><p name="4d15" id="4d15" class="graf graf--p graf-after--p">An interesting phenomenon called <strong class="markup--strong markup--p-strong">priority inversion</strong> happens once we introduce locks into the scheduling. In the following diagram, suppose the priority is: T1 &gt; T2 &gt; T3 and T3 acquires a mutex lock in the first 3 seconds without unlocking. Note that task T1 will also require this lock at time 7, so it will be blocked since T3 holds the lock.</p><figure name="fd6c" id="fd6c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5Aamm-VZ_ojlrzpkIiTALA.png" data-width="1242" data-height="378" src="https://cdn-images-1.medium.com/max/800/1*5Aamm-VZ_ojlrzpkIiTALA.png"></figure><p name="e487" id="e487" class="graf graf--p graf-after--figure">A solution to this problem is to <strong class="markup--strong markup--p-strong">temporarily boost the priority</strong> of the mutex owner. This means that when the highest priority thread needs to acquire the lock that’s owned by a lower priority thread, the priority of T3 is temporarily boosted to be basically at the same level as T1. This technique can be used to keep track of who is the current owner of the mutex.</p><p name="2422" id="2422" class="graf graf--p graf-after--p">Obviously, if we are temporarily boosting the priority of the mutex owner, we should lower its priority once it releases the mutex. This is a common technique that’s currently present in many operating systems.</p><p name="b111" id="b111" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4. Round Robin (RR) Scheduling</strong></p><p name="df9a" id="df9a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Round Robin Without Time slicing</strong></p><p name="7251" id="7251" class="graf graf--p graf-after--p">In practice, we can hardly tell how much time we need form executing a task, especially when there’s an I/O operation. But in the previous discussions, we have assumed that we know the executing time for each task. The round-robin scheduling can be used when we don’t know the execution time for each task and we will conduct a context switch when we have to wait for an I/O operation.</p><p name="df96" id="df96" class="graf graf--p graf-after--p">Let’s see an example. Suppose we have 3 tasks arrive at time 0. Each of them has an execution time of 2 seconds. However, task T1 has to conduct an I/O operation and it has to wait from time 1. The I/O operation of T1 is supposed to finish in 2 cycles, but because we don’t assign a priority in this case, the task T1 will have to wait for time 3 to continue.</p><figure name="0449" id="0449" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fUQCeM3xjJmK2zXJTeqNzQ.png" data-width="1336" data-height="376" src="https://cdn-images-1.medium.com/max/800/1*fUQCeM3xjJmK2zXJTeqNzQ.png"></figure><p name="71d9" id="71d9" class="graf graf--p graf-after--figure">So what if we assign priorities for this case? Let’s suppose that we have priority T3 &gt; T2 &gt; T1. In order to implement priorities, we also have to include preemptions.</p><figure name="b00e" id="b00e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*bUur55C28bcsXc3lkiA13w.png" data-width="1336" data-height="376" src="https://cdn-images-1.medium.com/max/800/1*bUur55C28bcsXc3lkiA13w.png"></figure><p name="bd49" id="bd49" class="graf graf--p graf-after--figure">If T2 and T3 have the same priority higher than T1, we will have,</p><figure name="5485" id="5485" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*wyW2cZg7l_9VapNS6VQpPw.png" data-width="1336" data-height="376" src="https://cdn-images-1.medium.com/max/800/1*wyW2cZg7l_9VapNS6VQpPw.png"></figure><p name="1a53" id="1a53" class="graf graf--p graf-after--figure">Note that when we haven’t got any I/O operations, the RR schedule will be exactly the same as the FCFS.</p><p name="99b5" id="99b5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Definition of Time slicing</strong></p><p name="10e0" id="10e0" class="graf graf--p graf-after--p">Further modification made for Round-robin scheduling is not waiting for tasks to yield explicitly. Instead, we would interrupt them by <strong class="markup--strong markup--p-strong">mixing the tasks</strong> that are in the system at the same time. This mechanism is called <strong class="markup--strong markup--p-strong">time-slicing</strong>. For example, we can give each task a time slice of one time unit (when there are no priorities), and then we will have,</p><figure name="e887" id="e887" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*p1dVjS8XTj8N0-1mXhNznQ.png" data-width="1336" data-height="376" src="https://cdn-images-1.medium.com/max/800/1*p1dVjS8XTj8N0-1mXhNznQ.png"></figure><p name="c667" id="c667" class="graf graf--p graf-after--figure">A <strong class="markup--strong markup--p-strong">time slice </strong>(aka. <strong class="markup--strong markup--p-strong">time quantum</strong>)<strong class="markup--strong markup--p-strong"> </strong>is the <strong class="markup--strong markup--p-strong">maximum amount</strong> of uninterrupted time that can be given to a task. Based on the definition, a task can actually run less amount time than what the time slice specifies, especially for the case when there is,</p><ul class="postList"><li name="961b" id="961b" class="graf graf--li graf-after--p">I/O operations</li><li name="8257" id="8257" class="graf graf--li graf-after--li">synchronization (e.g. mutex, cv, etc.)</li></ul><p name="b22f" id="b22f" class="graf graf--p graf-after--li">The tasks that use timeslices are interleaved, which means that they are time-sharing the CPU. For CPU-bound tasks, time slice is the only way we can achieve time sharing. They will be preempted after some amount of time specified by the time slice and we will schedule the next CPU-bound task.</p><p name="fbd3" id="fbd3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Timeslicing Performance</strong></p><p name="c2f5" id="c2f5" class="graf graf--p graf-after--p">Let’s now use an example to see the performance of the time-slicing. Suppose we have the following non-priority tasks,</p><pre name="cb03" id="cb03" class="graf graf--pre graf-after--p">T1 =  1 s<br>T2 = 10 s<br>T3 =  1 s</pre><p name="ac7a" id="ac7a" class="graf graf--p graf-after--pre">Then we can draw the diagrams of when we have a RR scheduling without time-slicing (exactly the same as FCFS) and when we have a RR scheduling with time slides of 1 unit,</p><figure name="a32a" id="a32a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lFDYbHEQfdXpWnUcFQgzPw.png" data-width="1928" data-height="444" src="https://cdn-images-1.medium.com/max/800/1*lFDYbHEQfdXpWnUcFQgzPw.png"></figure><p name="9811" id="9811" class="graf graf--p graf-after--figure">Then the performance of these two situations will be,</p><figure name="c2c9" id="c2c9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KMrHoHzvK9RdDKUgh-a5FA.png" data-width="1856" data-height="338" src="https://cdn-images-1.medium.com/max/800/1*KMrHoHzvK9RdDKUgh-a5FA.png"></figure><p name="cf4e" id="cf4e" class="graf graf--p graf-after--figure">We can find out that even without knowing the execution time, we can also achieve a performance that is close to SJF based on the RR scheduling with time-slicing.</p><p name="8a26" id="8a26" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Timeslicing Evaluation</strong></p><p name="bfee" id="bfee" class="graf graf--p graf-after--p">There are really some <strong class="markup--strong markup--p-strong">benefits</strong> of using the time-slicing when the time slice is relatively short,</p><ul class="postList"><li name="03f7" id="03f7" class="graf graf--li graf-after--p">no need for execution times</li><li name="51b7" id="51b7" class="graf graf--li graf-after--li">short tasks finish sooner</li><li name="9d1c" id="9d1c" class="graf graf--li graf-after--li">achieve a schedule that is <strong class="markup--strong markup--li-strong">more responsive</strong></li><li name="535e" id="535e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">I/O operations</strong> can be executed and initiated as soon as possible</li></ul><p name="4a1e" id="4a1e" class="graf graf--p graf-after--li">The <strong class="markup--strong markup--p-strong">downside</strong> is that,</p><ul class="postList"><li name="c26e" id="c26e" class="graf graf--li graf-after--p">more pure <strong class="markup--strong markup--li-strong">overheads</strong> for <strong class="markup--strong markup--li-strong">interrupting</strong> the running task, <strong class="markup--strong markup--li-strong">running the scheduler,</strong> and performing the <strong class="markup--strong markup--li-strong">context switch</strong></li></ul><p name="309d" id="309d" class="graf graf--p graf-after--li">Because of these overheads, in practice, the throughput will go down, the average waiting time goes up, and the average completion time goes up.</p><figure name="5ce9" id="5ce9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AmvUYTyWevVPTg7tZ5H7OQ.png" data-width="1856" data-height="338" src="https://cdn-images-1.medium.com/max/800/1*AmvUYTyWevVPTg7tZ5H7OQ.png"></figure><p name="9ee4" id="9ee4" class="graf graf--p graf-after--figure">So as long as the time slice values are significantly larger than the context switch time, we should be able to minimize these overheads. In general, we should consider both the nature of the tasks as well as the overheads in the system when determining meaningful values for the time slice.</p><p name="9836" id="9836" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Determining the Time Slice</strong></p><p name="a11a" id="a11a" class="graf graf--p graf-after--p">We have seen that we can have a better performance if we have a shorter time slice because the scheduler can then become more responsive. However, we can also have significant overheads when the time slice is too short. Therefore, we have to make a <strong class="markup--strong markup--p-strong">balance</strong> between these two to get a proper length of the time slice. To find this properly balanced value, we have to look at two different scenarios in the next place.</p><ul class="postList"><li name="c572" id="c572" class="graf graf--li graf-after--p">For <strong class="markup--strong markup--li-strong">I/O-Bound</strong> tasks: means that these tasks are mostly running for I/O operations</li><li name="d606" id="d606" class="graf graf--li graf-after--li">For <strong class="markup--strong markup--li-strong">CPU-Bound</strong> tasks: means that these tasks are mostly running on the CPU without I/O operations</li></ul><p name="a663" id="a663" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(6) Determining the Time Slice: For CPU-Bound Tasks</strong></p><p name="0650" id="0650" class="graf graf--p graf-after--p">In summary, a CPU-bound task prefers a <strong class="markup--strong markup--p-strong">larger time slice (ts)</strong>. Let’s see why by example.</p><p name="bd79" id="bd79" class="graf graf--p graf-after--p">Suppose we are given 2 CPU-bound tasks T1 and T2 with 10-sec execution time each. Assume the context switching time is 0.1 sec. Then, for the case when we have 1-sec time slice, 5-sec time slice, or ∞ sec (also means without) time slice, we can have the following diagram,</p><figure name="83c7" id="83c7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ENU4wYh8RbXM44jq3ndqJA.png" data-width="1856" data-height="694" src="https://cdn-images-1.medium.com/max/800/1*ENU4wYh8RbXM44jq3ndqJA.png"></figure><p name="8a85" id="8a85" class="graf graf--p graf-after--figure">Thus, the performance will be,</p><figure name="7f60" id="7f60" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ey4LHlFgOzR9-UuK1TGgew.png" data-width="1396" data-height="314" src="https://cdn-images-1.medium.com/max/800/1*Ey4LHlFgOzR9-UuK1TGgew.png"></figure><p name="befe" id="befe" class="graf graf--p graf-after--figure">From this table, we can know that because CPU-bound tasks care more about the throughput and the average completion time, and they care less about the average wait time, the performance of these tasks will be better if we have a larger time slice. What’s more, it will be even better when we don’t have time slices.</p><p name="702f" id="702f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Determining the Time Slice: For I/O-Bound Tasks</strong></p><p name="497f" id="497f" class="graf graf--p graf-after--p">One conclusion we can make is that for only I/O bound tasks, the value of the time slice is <strong class="markup--strong markup--p-strong">not really relevant</strong>. Let’s see an example.</p><p name="98cf" id="98cf" class="graf graf--p graf-after--p">Suppose we have 2 tasks T1 and T2 and each of them takes 10 seconds for execution. The context switching time is 0.1 seconds and the I/O operations are issued every 1 second with each I/O operation completes in 0.5 seconds.</p><p name="93b2" id="93b2" class="graf graf--p graf-after--p">We can see from the following diagram that if both T1 and T2 are I/O bound tasks, we will get exactly the same performance because the current task will be interrupted for RR scheduling. However, when only T2 is I/O-bound with a CPU-bound T1, the performance can be different because T1 can keep executing because there will be no interrupts for I/O waiting.</p><figure name="6846" id="6846" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*d08SZEtpd4c0BqQuNGUS-w.png" data-width="1602" data-height="668" src="https://cdn-images-1.medium.com/max/800/1*d08SZEtpd4c0BqQuNGUS-w.png"></figure><p name="c197" id="c197" class="graf graf--p graf-after--figure">Thus, in general, the performance will be,</p><figure name="cb68" id="cb68" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PFvoUqvuUFlPu9cTxJdmuQ.png" data-width="1458" data-height="288" src="https://cdn-images-1.medium.com/max/800/1*PFvoUqvuUFlPu9cTxJdmuQ.png"></figure><p name="cb01" id="cb01" class="graf graf--p graf-after--figure">In this case, although the average completion time will be better, we would finally choose a <strong class="markup--strong markup--p-strong">smaller</strong> time slice.</p><p name="12ec" id="12ec" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Timeslice Length Summary</strong></p><p name="16d1" id="16d1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">CPU-bound tasks</strong> prefer <strong class="markup--strong markup--p-strong">longer</strong> time slices. The longer the time slice, the fewer context switches that need to be performed. So that basically <strong class="markup--strong markup--p-strong">limits the context switching</strong> overhead that the scheduling will introduce. Also, longer time slices keep the <strong class="markup--strong markup--p-strong">CPU utilization</strong> and <strong class="markup--strong markup--p-strong">throughput</strong> high.</p><p name="4b19" id="4b19" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">I/O bound tasks</strong> prefer <strong class="markup--strong markup--p-strong">shorter</strong> time slices. This allows I/O bound tasks to issue I/O operations as soon as possible and, as a result, we achieve both <strong class="markup--strong markup--p-strong">higher CPU and device utilization</strong> as well as the performance that the user perceives that the system is <strong class="markup--strong markup--p-strong">responsive</strong> and that it responds to its commands and displays output.</p><p name="6274" id="6274" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) CPU Utilization Calculation</strong></p><p name="04c0" id="04c0" class="graf graf--p graf-after--p">When we need to calculate the utilization of the CPU, we can use the following formula.</p><figure name="a3f7" id="a3f7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*QXeQ1R35nt8aWQsAAmUHtg.png" data-width="1458" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*QXeQ1R35nt8aWQsAAmUHtg.png"></figure><p name="d468" id="d468" class="graf graf--p graf-after--figure">And we should follow the steps,</p><ul class="postList"><li name="fa99" id="fa99" class="graf graf--li graf-after--p">Determine a consistent, recurring interval</li><li name="1272" id="1272" class="graf graf--li graf-after--li">In the interval, each task should be given an opportunity to run</li><li name="1058" id="1058" class="graf graf--li graf-after--li">During that interval, how much time is spent computing? This is the <strong class="markup--strong markup--li-strong">cpu_running_time</strong></li><li name="6b4c" id="6b4c" class="graf graf--li graf-after--li">During that interval, how much time is spent context switching? This is the <strong class="markup--strong markup--li-strong">context_switching_overheads</strong></li><li name="abb7" id="abb7" class="graf graf--li graf-after--li">Calculate</li></ul><p name="d096" id="d096" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">5. Runqueue Data Structure</strong></p><p name="8f12" id="8f12" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Assigning Different Lengths of Time Slices</strong></p><p name="1819" id="1819" class="graf graf--p graf-after--p">If we want the I/O or CPU bound tasks to have different time slice values, we have two options,</p><ul class="postList"><li name="5438" id="5438" class="graf graf--li graf-after--p">Maintaining the <strong class="markup--strong markup--li-strong">same runqueue data structure</strong> for both the I/O and CPU-bound tasks. Then we do classifications for telling whether this is an I/O-bound task or a CPU-bound task, and with the result of that, we can assign different time slices for each type.</li><li name="6808" id="6808" class="graf graf--li graf-after--li">Maintaining <strong class="markup--strong markup--li-strong">two different runqueue data structures</strong> for I/O and CPU-bound tasks respectively. With each run queue, we will associate a different kind of policies that suitable for this kind of task that it is related with.</li></ul><p name="b6f9" id="b6f9" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(2) Multiqueue Data Structure</strong></p><p name="a130" id="a130" class="graf graf--p graf-after--p">Now, let’s see the implementation of a multi-queue runqueue data structure. Suppose we have three runqueues in the system each of them will be assigned for different lengths of time slices (e.g. 8 ms, 16 ms, or 20 ms),</p><figure name="a98b" id="a98b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*E6DlxzyAl0LN10ufpfVHkQ.png" data-width="1298" data-height="270" src="https://cdn-images-1.medium.com/max/800/1*E6DlxzyAl0LN10ufpfVHkQ.png"></figure><p name="a8d0" id="a8d0" class="graf graf--p graf-after--figure">Based on the discussions above, the CPU-bound tasks tend to perform better when we have a longer time slice, while the I/O-bound tasks tend to perform better when we have a shorter time slice. Then we will conduct a classification of the tasks before we enter these runqueues.</p><figure name="eae6" id="eae6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YjvTnIbkeYMd0afGC5M6BA.png" data-width="1298" data-height="270" src="https://cdn-images-1.medium.com/max/800/1*YjvTnIbkeYMd0afGC5M6BA.png"></figure><ul class="postList"><li name="1a9d" id="1a9d" class="graf graf--li graf-after--figure">The most I/O intensive tasks will be assigned to the 1st queue (ts = 8 ms)</li><li name="cc0b" id="cc0b" class="graf graf--li graf-after--li">Medium I/O intensive tasks will be assigned to the 2nd queue (ts = 16 ms)</li><li name="b2be" id="b2be" class="graf graf--li graf-after--li">CPU intensive tasks will be assigned to the 3rd queue (ts = 20ms). Because the time slice for CPU intensive tasks is quite long, we can</li></ul><p name="12d8" id="12d8" class="graf graf--p graf-after--li">The <strong class="markup--strong markup--p-strong">benefits</strong> that can be gained from these tasks will be,</p><ul class="postList"><li name="d2a4" id="d2a4" class="graf graf--li graf-after--p">Time slicing benefits provided for I/O-bounds tasks</li><li name="d324" id="d324" class="graf graf--li graf-after--li">Time slicing overhead avoided for CPU-bounds tasks</li></ul><p name="f17c" id="f17c" class="graf graf--p graf-after--li">The<strong class="markup--strong markup--p-strong"> downside</strong> is that,</p><ul class="postList"><li name="318e" id="318e" class="graf graf--li graf-after--p">It can be hard for us to know whether we have an I/O-intensive bound</li></ul><p name="f869" id="f869" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Task Classification</strong></p><p name="fb18" id="fb18" class="graf graf--p graf-after--p">It can be a problem for us to decide how I/O intensive of a task. We can use,</p><ul class="postList"><li name="cad6" id="cad6" class="graf graf--li graf-after--p">history-based heuristics</li></ul><p name="ba10" id="ba10" class="graf graf--p graf-after--li">But this can be even more complex when we have new tasks or some tasks that will change the behavior during the execution.</p><p name="7ef0" id="7ef0" class="graf graf--p graf-after--p">In order to deal with all these problems, Fernando Corbato developed the <strong class="markup--strong markup--p-strong">multi-level feedback queue </strong>data structure along with other work on the time-sharing system. He is awarded Turing Award because of this in 1990.</p><p name="7793" id="7793" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Multi-Level Feedback Queue (MLFQ)</strong></p><p name="f185" id="f185" class="graf graf--p graf-after--p">The idea of the <strong class="markup--strong markup--p-strong">multi-level feedback queue (MLFQ)</strong> is quite simple. Instead of classifying the task types all the time and this is quite complicated, we will decide not to do any classifications.</p><p name="661b" id="661b" class="graf graf--p graf-after--p">To deal with these problems, we will treat these three queues not as three separate run queues but as one single multi-queue data structure as follows,</p><figure name="76e8" id="76e8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*GhBc-P8Kk34RKIsY5igSWg.png" data-width="1664" data-height="174" src="https://cdn-images-1.medium.com/max/800/1*GhBc-P8Kk34RKIsY5igSWg.png"></figure><p name="fecb" id="fecb" class="graf graf--p graf-after--figure">This data structure can be used as follows,</p><ul class="postList"><li name="fb40" id="fb40" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Step 1</strong>. Tasks enter the topmost queue. This queue has the lowest time slice and we are expecting that the I/O-bound tasks are the most demanding task and we would be a benefit from the context switching.</li><li name="6936" id="6936" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Step 2</strong>. If the task stops executing before 8 ms, then this means that we made a good choice. The task will be kept at this level.</li><li name="0701" id="0701" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Step 3</strong>. If the task used up the entire 8 ms time slice, this means that this task is more intensive on the CPU. So we will push it down to a lower level with a 16 ms time slice.</li><li name="a00e" id="a00e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Step 4</strong>. If the task used up the entire 16 ms time slice, this means that this task is more intensive on the CPU. So we will push it down to a lower level with a 20 ms time slice.</li><li name="75c5" id="75c5" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Step 5</strong>. If the task in the low queue has an I/O wait, it will have a priority boost when it is released.</li></ul><p name="9c0e" id="9c0e" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">6. Linux Scheduler</strong></p><p name="7d98" id="7d98" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Linux Scheduler Example: O(1) Scheduler</strong></p><p name="c221" id="c221" class="graf graf--p graf-after--p">The Linux has a scheduler called <strong class="markup--strong markup--p-strong">O(1)</strong> and we will talk about it next. It’s like a multi-level feedback queue with 60 levels, and also some things feedback rules that determine how and when a thread gets pushed up and down these different levels.</p><p name="c6d3" id="c6d3" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">O(1)</strong> scheduler receives its name because it’s able to perform task management operations such as selecting a task from the run queue or adding a task to it in <strong class="markup--strong markup--p-strong">constant time</strong>.</p><p name="c574" id="c574" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Numeric Priority Numbers</strong></p><p name="38c2" id="38c2" class="graf graf--p graf-after--p">The O(1) scheduler has 140 priority levels with 0 being the highest (ts = 100 ms) and 139 (ts = 10 ms) is the lowest. The priority levels can be grouped into two classes,</p><ul class="postList"><li name="3036" id="3036" class="graf graf--li graf-after--p">0~99: <strong class="markup--strong markup--li-strong">real-time tasks</strong></li><li name="0e3d" id="0e3d" class="graf graf--li graf-after--li">100~139: <strong class="markup--strong markup--li-strong">time-sharing class</strong></li></ul><p name="addb" id="addb" class="graf graf--p graf-after--li">Each of the ULPs has one of the time-sharing priority levels,</p><ul class="postList"><li name="540e" id="540e" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">default priority</strong>: 120</li><li name="dea5" id="dea5" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">nice value</strong>: -20 ~ 19.</li></ul><p name="a29b" id="a29b" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(3) Sleep Time and Priority</strong></p><p name="e899" id="e899" class="graf graf--p graf-after--p">The time slice value will depend on the <strong class="markup--strong markup--p-strong">priority</strong> and the <strong class="markup--strong markup--p-strong">feedback </strong>from the sleep time.</p><ul class="postList"><li name="8e35" id="8e35" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">longer sleep time</strong>: means that the task needs interactive. So when longer sleep time is detected, we will<strong class="markup--strong markup--li-strong"> boost</strong> the priority by reducing the priority by 5.</li><li name="16f3" id="16f3" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">smaller sleep time</strong>: means that the task is more compute-intensive. So we lowered the priority by reducing the priority by 5.</li></ul><p name="f698" id="f698" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Active Array and Expired Array</strong></p><p name="1af2" id="1af2" class="graf graf--p graf-after--p">The runqueue in the O(1) scheduler is organized as 2 arrays of tasks.</p><p name="df5a" id="df5a" class="graf graf--p graf-after--p">When a task first comes into the run queue or <strong class="markup--strong markup--p-strong">active</strong>, it will be put at the back of the <strong class="markup--strong markup--p-strong">active arrive</strong> array corresponding to its priority value. When the CPU becomes available, the scheduler will select and pick up the next task from the active array to run with a constant time. The remaining tasks will be left in the queue and wait until their time slices expire.</p><p name="3911" id="3911" class="graf graf--p graf-after--p">When the time slice of a task is expired, this task will be <strong class="markup--strong markup--p-strong">inactive</strong> and it will be put into the <strong class="markup--strong markup--p-strong">expired array</strong>. When there are no more tasks in the active array, the pointers of these two lists will be swapped. As a result, the expired array can become the new active one and the tasks in it can be executed again.</p><p name="a43b" id="a43b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Problems for O(1) Scheduler</strong></p><p name="4717" id="4717" class="graf graf--p graf-after--p">The O(1) was first introduced in the Linux kernel 2.5 by Ingo Molnar. In spite of its really nice property of being able to operate in constant time, the O(1) really affected the performance of interactive tasks significantly. As the workloads changed, as typical applications are becoming more time-sensitive (skype, movie streaming, gaming) the jitter that was introduced by the O(1) scheduler was becoming unacceptable. For O(1) schedulers, there can be 2 problems,</p><ul class="postList"><li name="ae50" id="ae50" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Performance of Interactive Tasks</strong>: tasks that are put on the expired list will not be rescheduled until the array switches. As a result, the performance of the interactive tasks will become worse</li><li name="fd7c" id="fd7c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Fairness</strong>: fairness means that in a given time interval, all of the tasks should be able to run for an amount of time that is proportional to their priority. A lack of fairness means that even though there are multiple formal definitions of fairness, and O(1) doesn’t make any fairness guarantees</li></ul><p name="7534" id="7534" class="graf graf--p graf-after--li">For that reason, the O(1) scheduler was replaced with the completely fair scheduler <strong class="markup--strong markup--p-strong">CFS</strong> and it became the default scheduler in kernel 2.6.23. Ironically, both of these schedulers are developed by the same person. Both the O(1) and CFS scheduler are part of the standard Linux distribution. CFS is the default. But you can switch to O(1) scheduler to execute your tasks.</p><p name="37bf" id="37bf" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Linux Completely Fair Scheduler (LCFS)</strong></p><p name="f5c8" id="f5c8" class="graf graf--p graf-after--p">The CFS is used after Linux kernel 2.6.23 and it is the default scheduler for all the non-real-time tasks. The main idea of a CFS is that it uses a <a href="https://algs4.cs.princeton.edu/33balanced/" data-href="https://algs4.cs.princeton.edu/33balanced/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">red-black tree</strong></a><strong class="markup--strong markup--p-strong"> </strong>(aka. <strong class="markup--strong markup--p-strong">balanced search tree</strong>)<strong class="markup--strong markup--p-strong"> </strong>and the red-black tree belongs to dynamic tree structures. The tree will be self-balanced as nodes are added or removed from the tree. We will not discuss this data structure in detail because it is beyond the scope of this series. But you should know that CFS always schedules the task which has the <strong class="markup--strong markup--p-strong">least amount of time</strong> on the CPU so that typically would be the <strong class="markup--strong markup--p-strong">leftmost node</strong> in the tree.</p><p name="cfb6" id="cfb6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">7. Scheduling on Multi-CPU Systems</strong></p><p name="04d6" id="04d6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Computer Architecture Details</strong></p><p name="bf97" id="bf97" class="graf graf--p graf-after--p">Before we discuss the scheduling in the multi-CPU systems, we have to know something about the architecture. The <strong class="markup--strong markup--p-strong">shared memory</strong> can be accessed by all the processors and these processors are called the <strong class="markup--strong markup--p-strong">shared memory multi-processors</strong> (aka. <strong class="markup--strong markup--p-strong">SMPs</strong>). Each of the processors has its <strong class="markup--strong markup--p-strong">own caches</strong> (e.g. L1 cache, L2 cache, etc.) and the <strong class="markup--strong markup--p-strong">last level cache</strong> (aka. <strong class="markup--strong markup--p-strong">LLC</strong>) may or may not be shared among the CPUs. There is a system memory called <strong class="markup--strong markup--p-strong">dynamic random-access memory</strong> (aka. <strong class="markup--strong markup--p-strong">DRAM</strong>) that is shared among all the processors. We will assume that in the following case, we only have one memory component, but it’s possible that there are multiple memory components in reality.</p><p name="8d47" id="8d47" class="graf graf--p graf-after--p">However, in the current multicore world, there can be some differences from the SMP system. In general, the common differences are,</p><ul class="postList"><li name="3524" id="3524" class="graf graf--li graf-after--p">each CPU can have multiple cores</li><li name="ad1e" id="ad1e" class="graf graf--li graf-after--li">each core can have private caches</li><li name="c155" id="c155" class="graf graf--li graf-after--li">the entire CPU will share a single LLC</li></ul><p name="8e85" id="8e85" class="graf graf--p graf-after--li">You can refer to the following picture for more insights,</p><figure name="4f69" id="4f69" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zCFMmGjSrxhqVIhKvAtmWA.png" data-width="1586" data-height="622" src="https://cdn-images-1.medium.com/max/800/1*zCFMmGjSrxhqVIhKvAtmWA.png"></figure><p name="e0d6" id="e0d6" class="graf graf--p graf-after--figure">Nowadays, it’s more common to have 6 or 8 cores for PCs in order to have multiple CPUs. We will discuss the following topics based on the SMP system because the multi-core system is quite similar and the same stuff can be easily applied.</p><p name="eed1" id="eed1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Cache-Affinity and Solution</strong></p><p name="72ae" id="72ae" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">cache affinity</strong> happens because the processors have private LLCs in an SMP system. If a task has been run on one CPU, then we switch the execution of it on another CPU, we will probably meet a cold cache problem. If we assign this task to the same CPU, instead, we will have a cache hit which improves the performance. That’s why we have this cache affinity issue. To achieve this cache affinity, we have to keep tasks on the<strong class="markup--strong markup--p-strong"> same CPU </strong>as much as possible.</p><p name="72d6" id="72d6" class="graf graf--p graf-after--p">We can implement <strong class="markup--strong markup--p-strong">cache affinity</strong> by the hierarchical scheduler architecture where at the top level, a <strong class="markup--strong markup--p-strong">load balancing entity</strong> in the scheduler divides the tasks among the CPUs. Then a per CPU scheduler with a per CPU run queue repeatedly schedules those tasks on a given CPU as much as possible.</p><p name="ea0f" id="ea0f" class="graf graf--p graf-after--p">The top-level entity in the scheduler can look at information such as</p><ul class="postList"><li name="5817" id="5817" class="graf graf--li graf-after--p">the length of each of these queues</li><li name="f883" id="f883" class="graf graf--li graf-after--li">whether a CPU is idle</li></ul><p name="889c" id="889c" class="graf graf--p graf-after--li">to decide how to balance tasks among these CPUs.</p><figure name="fe12" id="fe12" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pw2rdtrNKTUyoDVy4kVSpw.png" data-width="1586" data-height="622" src="https://cdn-images-1.medium.com/max/800/1*pw2rdtrNKTUyoDVy4kVSpw.png"></figure><p name="49bf" id="49bf" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Non-Uniform Memory Access Platforms (NUMA)</strong></p><p name="d79d" id="d79d" class="graf graf--p graf-after--p">In addition, it is also possible to have multiple memory nodes. However, the CPU and the memory nodes will be <strong class="markup--strong markup--p-strong">interconnected </strong>via some type of interface. For example, for modern intel platforms, there is an interconnect quick pipe interconnect (aka. QPI) that is used to connect the CPU with a memory. If that is the case, the access from a CPU to one memory node can be fast, but the CPU to the other memory node can be slow.</p><p name="429e" id="429e" class="graf graf--p graf-after--p">So it is clear that, from a scheduling perspective, it makes sense if the tasks are bound to those CPUs whose memory nodes are close to the state of those tasks. We call this type of scheduling <strong class="markup--strong markup--p-strong">NUMA-aware scheduling</strong>.</p><p name="1669" id="1669" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Hyperthreading</strong></p><p name="268b" id="268b" class="graf graf--p graf-after--p">If we use hardware to implement multi-threads, we can have <strong class="markup--strong markup--p-strong">hyperthreading </strong>(aka. <strong class="markup--strong markup--p-strong">hardware multithreading</strong>, <strong class="markup--strong markup--p-strong">chip multithreading</strong>, <strong class="markup--strong markup--p-strong">CMT</strong>, <strong class="markup--strong markup--p-strong">simultaneous</strong> <strong class="markup--strong markup--p-strong">multithreading</strong>, <strong class="markup--strong markup--p-strong">SMT</strong>). One way this has been achieved is to have <strong class="markup--strong markup--p-strong">multiple sets of registers</strong>. The context switching between hyper-threads is very <strong class="markup--strong markup--p-strong">fast</strong> because nothing has to be saved or restored and the CPU just needs to switch from using this set of registers to another set of registers.</p><p name="6d12" id="6d12" class="graf graf--p graf-after--p">One of the features of today’s hardware is that you can enable or disable this hardware multithreading at boot time. If it is enabled, each of these hardware contexts appears to the OS scheduler will be treated as a <strong class="markup--strong markup--p-strong">separate virtual CPU</strong>. In that way, when there is a memory I/O that can cost much longer than the cost of a context switch, which is almost 0 cycle in this case, the context on the CPU will be switched to another hyper thread. Therefore, hyperthreading can also hide memory access latency.</p><p name="3f77" id="3f77" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Scheduling on Hyperthreading Platforms: Assumptions</strong></p><p name="1783" id="1783" class="graf graf--p graf-after--p">The following discussion will be based on Fedorova’s paper named <a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-fedorova-paper.pdf" data-href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-fedorova-paper.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Chip Multithreading Systems Need a New Operating System Scheduler</em></a><em class="markup--em markup--p-em">.</em> Let’s first see the basic assumptions of this paper,</p><ul class="postList"><li name="e9bc" id="e9bc" class="graf graf--li graf-after--p">a thread can issue an instruction on every single cycles, so the maximum IPC will be 1</li><li name="d965" id="d965" class="graf graf--li graf-after--li">the memory access takes 4 cycles</li><li name="7a9c" id="7a9c" class="graf graf--li graf-after--li">time for context switch between hardware is instantaneous</li><li name="5e68" id="5e68" class="graf graf--li graf-after--li">we have an SMT system with 2 hardware threads</li></ul><p name="6d28" id="6d28" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(6) Scheduling on Hyperthreading Platforms: 2 Compute-Bound Tasks</strong></p><p name="6220" id="6220" class="graf graf--p graf-after--p">Now, let’s see the first case. Suppose that both of the hyperthreads are compute-bound, which means that they are both intensive to the CPU. However, given that there is only a CPU pipeline, only one of them can execute at any given point of time. As a result, these threads will <strong class="markup--strong markup--p-strong">interfere </strong>with each other and <strong class="markup--strong markup--p-strong">contend</strong> with the CPU pipeline resources. In the best case, every one of them will basically spend 1 cycle idling while the other thread issues its instruction. You can see this from the following diagram where C means <code class="markup--code markup--p-code">compute</code> and X means <code class="markup--code markup--p-code">idle</code>.</p><figure name="880a" id="880a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EhLTWmRfiiq2GwJxXdH_GQ.png" data-width="1360" data-height="222" src="https://cdn-images-1.medium.com/max/800/1*EhLTWmRfiiq2GwJxXdH_GQ.png"></figure><p name="c057" id="c057" class="graf graf--p graf-after--figure">As a result, for each of the threads,</p><ul class="postList"><li name="1d01" id="1d01" class="graf graf--li graf-after--p">the performance will degrade by 2</li><li name="d06c" id="d06c" class="graf graf--li graf-after--li">the memory controller is idle</li></ul><p name="9282" id="9282" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(7) Scheduling on Hyperthreading Platforms: 2 Memory-Bound Tasks</strong></p><p name="0345" id="0345" class="graf graf--p graf-after--p">Now, let’s see the second case. Suppose that both of the hyperthreads are memory-bound. Because we have to wait 4 cycles to return from the memory, we can have 2 cycles that are unused for every 4 cycles. Now the diagram would be,</p><figure name="6f3b" id="6f3b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fVt-zdY3aKNiFUbw_tuUuQ.png" data-width="1360" data-height="222" src="https://cdn-images-1.medium.com/max/800/1*fVt-zdY3aKNiFUbw_tuUuQ.png"></figure><p name="4044" id="4044" class="graf graf--p graf-after--figure">This strategy leads to some bad results because we are wasting the CPU cycles.</p><p name="3cd3" id="3cd3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Scheduling on Hyperthreading Platforms: 1MB-1CB Tasks</strong></p><p name="fbc0" id="fbc0" class="graf graf--p graf-after--p">The final option is to consider mixing some CPU-intensive threads and memory-intensive threads. As a result, we end up fully utilizing each of the CPU cycles, and whenever there is a thread that needs to perform a memory reference, we context switch to that thread in the hardware and then context switch back to the CPU-intensive thread. Now the diagram would be,</p><figure name="ece5" id="ece5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HZCy3Hudc0aaOzY4v1ZpzQ.png" data-width="1360" data-height="222" src="https://cdn-images-1.medium.com/max/800/1*HZCy3Hudc0aaOzY4v1ZpzQ.png"></figure><p name="6f7b" id="6f7b" class="graf graf--p graf-after--figure">This option is perfect because it</p><ul class="postList"><li name="b747" id="b747" class="graf graf--li graf-after--p">avoids/limits the contention on the processor’s pipeline</li><li name="0849" id="0849" class="graf graf--li graf-after--li">all the CPU and memory are well utilized</li></ul><p name="e909" id="e909" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(8) CPU-Bound Tasks or Memory-Bound Tasks: by Counters</strong></p><p name="46ab" id="46ab" class="graf graf--p graf-after--p">Well, we finally come to this question if we want to implement the scheduling above. The answer is to use a <strong class="markup--strong markup--p-strong">history-based counter</strong>. It is not convenient for us to use the sleep time and you can think about why. Fortunately, many hardware counters (e.g. L1, L2, LLC, power and energy data, etc.) can provide us useful information to decide whether it is a CPU-bound or a memory-bound task. In addition, there are also a number of interfaces that can be used for us, for example, the <a href="https://oprofile.sourceforge.io/news/" data-href="https://oprofile.sourceforge.io/news/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Oprofile</strong></a> and the <strong class="markup--strong markup--p-strong">Linux perf tool</strong>.</p><p name="065f" id="065f" class="graf graf--p graf-after--p">For instance, schedulers can look at LLC misses, and using this counter as a scheduler that can decide whether it is memory-bound or compute-bound. Or the same counter can also tell the scheduler that something changed in the execution of the thread so that now it is executing with some different data in a different phase of its execution, or maybe it is running with a cold cache.</p><p name="1ec1" id="1ec1" class="graf graf--p graf-after--p">In addition, most modern processors will,</p><ul class="postList"><li name="b0c9" id="b0c9" class="graf graf--li graf-after--p">typically use <strong class="markup--strong markup--li-strong">multiple counters</strong></li><li name="79db" id="79db" class="graf graf--li graf-after--li">use some models for a specific <strong class="markup--strong markup--li-strong">architecture</strong></li><li name="a568" id="a568" class="graf graf--li graf-after--li">be trained based on well-understood <strong class="markup--strong markup--li-strong">workloads</strong></li></ul><p name="6a7f" id="6a7f" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(9) IPC as a Good Counter</strong></p><p name="9d15" id="9d15" class="graf graf--p graf-after--p">Fedorova observes that a memory-bound thread takes a lot of cycles to complete the instruction therefore it has a <strong class="markup--strong markup--p-strong">high CPI</strong>. Whereas a CPU-bound thread will complete an instruction every cycle or near that and therefore have a CPI of one or a <strong class="markup--strong markup--p-strong">low CPI</strong>. She speculates that it would be useful to gather this type of information this counter about the cycles per instruction and use that as a metric in scheduling threads on hyperthreaded platforms.</p><p name="8aa6" id="8aa6" class="graf graf--p graf-after--p">Given that there isn’t an exact CPI counter on the processors that Fedorova uses in her work and computing something like 1/IPC would require software computation so that wouldn’t be acceptable.</p><p name="4de4" id="4de4" class="graf graf--p graf-after--p">For that reason, Fedorova uses a simulator that supposedly the CPU does have a CPI counter, and then she looks at whether a scheduler can take that information and make good decisions. Her hope is that if she can demonstrate that CPI is a useful metric, then hardware engineers will add this particular type of counter in <strong class="markup--strong markup--p-strong">future architectures</strong>.</p><p name="8cc0" id="8cc0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) Experimental Information for Fedorova’s Paper</strong></p><p name="1f91" id="1f91" class="graf graf--p graf-after--p">In order to prove that IPC is a good metric in the future, Fedorova designs an experiment and simulates the results. Here are some basic information about this experiment,</p><ul class="postList"><li name="90e2" id="90e2" class="graf graf--li graf-after--p">4-core 4-way SMT system (totally 16 hardware context)</li><li name="6e35" id="6e35" class="graf graf--li graf-after--li">the synthetic workload with CPI of 1, 6, 11, 16</li><li name="4404" id="4404" class="graf graf--li graf-after--li">4 threads of each kind of CPI</li><li name="dc9a" id="dc9a" class="graf graf--li graf-after--li">the metric should be IPC and its maximum value can be 4</li></ul><p name="2286" id="2286" class="graf graf--p graf-after--li">Then the following experimental table is designed. She designed 4 experiments a, b, c, and d. And within each of these experiments, she assigned different workloads for each core.</p><figure name="f561" id="f561" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gIMHreQyskFOp3h8rruyyg.png" data-width="1360" data-height="470" src="https://cdn-images-1.medium.com/max/800/1*gIMHreQyskFOp3h8rruyyg.png"></figure><p name="5606" id="5606" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(11) Experimental Results for Fedorova’s Paper</strong></p><p name="efba" id="efba" class="graf graf--p graf-after--p">So here is the final result. Because we have discussed that with a mixed CPI, we can result in better performance, the experiments a and b must have better performance than experiments c and d. If we look at c and d, the core 2 and core 3 of them will only be compute-bound or memory-bound and this is not good for us.</p><figure name="a57a" id="a57a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CA2YC2BQDasDH0mVox__yw.png" data-width="1308" data-height="424" src="https://cdn-images-1.medium.com/max/800/1*CA2YC2BQDasDH0mVox__yw.png"></figure><p name="5eea" id="5eea" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(12) Realistic Workloads</strong></p><p name="7a1f" id="7a1f" class="graf graf--p graf-after--p">Although in the paper, it can be quite good for us to use the IPC metric as a metric, it is not the case in reality. Let’s see the following tables.</p><figure name="1311" id="1311" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*iHIt_4d4aVknydPEmlTu1g.png" data-width="1294" data-height="498" src="https://cdn-images-1.medium.com/max/800/1*iHIt_4d4aVknydPEmlTu1g.png"></figure><p name="098d" id="098d" class="graf graf--p graf-after--figure">We can find out that all the common applications have a CPI of around 2.5 ~ 4.5, and these are exactly different from the synthetic workload of 1 ~ 16 that we mentioned in the paper. Therefore, the idea of using IPC as a scheduling metric will not be useful for a real workload.</p><p name="4f09" id="4f09" class="graf graf--p graf-after--p graf--trailing">In fact, the LLC will be a better counter in reality but we will not explain why in this series because it is beyond the scope.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/ef1b97937b3"><time class="dt-published" datetime="2021-03-18T17:59:35.684Z">March 18, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/operating-system-16-os-scheduling-first-come-first-serve-shortest-job-first-preemption-ef1b97937b3" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>