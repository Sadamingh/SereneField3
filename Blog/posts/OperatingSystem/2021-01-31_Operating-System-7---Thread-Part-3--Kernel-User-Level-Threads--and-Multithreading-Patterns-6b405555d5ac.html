<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Operating System 7 | Thread Part 3, Kernel/User-Level Threads, and Multithreading Patterns</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Operating System 7 | Thread Part 3, Kernel/User-Level Threads, and Multithreading Patterns</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Operating System
</section>
<section data-field="body" class="e-content">
<section name="7b58" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="dcc5" id="dcc5" class="graf graf--h3 graf--leading graf--title">Operating System 7 | Thread Part 3, <strong class="markup--strong markup--h3-strong">Kernel/User-Level Threads, and Multithreading Patterns</strong></h3><figure name="4263" id="4263" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*aoTko88H7htOIFZ4.png" data-width="1508" data-height="794" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*aoTko88H7htOIFZ4.png"></figure><ol class="postList"><li name="bfe1" id="bfe1" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Kernel/User-Level Threads</strong></li></ol><p name="1c85" id="1c85" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Kernel-Level Threads</strong></p><p name="0a30" id="0a30" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">kernel-level threads</strong> imply that the operating system itself is actually multithreaded. The kernel threads are visible to the kernel and they are managed only by the kernel-level components like the kernel scheduler. Some kernel-level threads are used to support the operating system, while others are used for supporting the user-level threads.</p><p name="730a" id="730a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) User-Level Threads</strong></p><p name="9fe0" id="9fe0" class="graf graf--p graf-after--p">At the user level, the processes themselves are multi-threaded and each of them contains several user-level threads. For the execution of a user-level thread, it must associate with a kernel-level thread. So there has to be some sort of relationship between the user-level threads and the kernel-level threads.</p><p name="4ecc" id="4ecc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) One-To-One Model</strong></p><p name="76ad" id="76ad" class="graf graf--p graf-after--p">The first model between the user-level threads and the kernel-level threads is a <strong class="markup--strong markup--p-strong">one-to-one model</strong>, which means that each of the user-level threads has a corresponding kernel-level thread. So the operating system can see all the user-level threads and the OS also knows the synchronization, the blocking, and extra. By this model, the user-level threads can directly benefit from the thread <strong class="markup--strong markup--p-strong">supports</strong> that are available in the kernel.</p><figure name="d970" id="d970" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*XjK-XEVdYMHSAFNAXIGl7A.png" data-width="1494" data-height="572" src="https://cdn-images-1.medium.com/max/800/1*XjK-XEVdYMHSAFNAXIGl7A.png"></figure><p name="a021" id="a021" class="graf graf--p graf-after--figure">However, this model also has some downsides.</p><ul class="postList"><li name="0858" id="0858" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Expensive</strong>: For every operation, we have to go to the kernel. So we have to pay the cost of the system calls</li><li name="f86e" id="f86e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">OS Policies</strong>: The OS can have limited policies, for example, the process can be restricted by the number of the threads</li><li name="134f" id="134f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Portability</strong>: We also need to think about the portability concerns</li></ul><p name="d8cc" id="d8cc" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Many-To-One Model</strong></p><p name="0ba3" id="0ba3" class="graf graf--p graf-after--p">Suppose we have a relationship that several user-level threads are mapped onto a single kernel-level thread. To implement this model, we need to have a <strong class="markup--strong markup--p-strong">thread management library</strong> in the user lever that decides which one of the user-level threads will be mapped onto the kernel-level thread.</p><figure name="5bef" id="5bef" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NHI7vTjX9mqoIlqbobqFYQ.png" data-width="1494" data-height="572" src="https://cdn-images-1.medium.com/max/800/1*NHI7vTjX9mqoIlqbobqFYQ.png"></figure><p name="173a" id="173a" class="graf graf--p graf-after--figure">The benefit of this model is that it is totally <strong class="markup--strong markup--p-strong">portable</strong>. This means that almost everything can be done in the thread management library and we don’t rely on any kernel-level support.</p><p name="36f6" id="36f6" class="graf graf--p graf-after--p">However, this model has some downsides as well,</p><ul class="postList"><li name="53fa" id="53fa" class="graf graf--li graf-after--p">OS has no insights into the application’s needs</li><li name="9bba" id="9bba" class="graf graf--li graf-after--li">the OS may block the whole process if one user-level thread blocks on I/O</li></ul><p name="406d" id="406d" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(5) Many-To-Many Model</strong></p><p name="9e4c" id="9e4c" class="graf graf--p graf-after--p">The many-to-many model allows some user-level threads mapped to some kernel-level threads. The kernel knows that the process is multi-threaded different user-level threads can be mapped to different kernel-level threads. If some user-level threads are blocked from I/O because the corresponding kernel-level thread is blocked, the process overall can have some other kernel-level threads that can do the I/O operations.</p><figure name="00ec" id="00ec" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8WT6JnsIqa3gkhdlvwLP2Q.png" data-width="1494" data-height="572" src="https://cdn-images-1.medium.com/max/800/1*8WT6JnsIqa3gkhdlvwLP2Q.png"></figure><p name="990e" id="990e" class="graf graf--p graf-after--figure">The downside of this model is that now we must need some coordination between the user-level thread management and the kernel-level thread management.</p><p name="7135" id="7135" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) The Definition of System Scope</strong></p><p name="c158" id="c158" class="graf graf--p graf-after--p">There are different levels in which multi-threading is supported, at the entire system or within a process and each level affects the <strong class="markup--strong markup--p-strong">scope </strong>of the thread management system. At the kernel level, we have system-wide management that is supported by the <strong class="markup--strong markup--p-strong">operating system thread managers, </strong>and this scope of thread management is called the <strong class="markup--strong markup--p-strong">system scope</strong>.</p><p name="2545" id="2545" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) The Definition of Process Scope</strong></p><p name="7c90" id="7c90" class="graf graf--p graf-after--p">We have also talked about that at the user level, there’s a <strong class="markup--strong markup--p-strong">user-level thread library</strong> (links to a process) that manages all the threads that are only within that single process. Different processes will be managed by different instances of the same library or even different processes may link entirely to different user-level libraries. This scope of thread management is called the <strong class="markup--strong markup--p-strong">process scope</strong>.</p><p name="8f45" id="8f45" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) System Scope Vs. Process Scope</strong></p><p name="e39b" id="e39b" class="graf graf--p graf-after--p">Suppose we have two processes, one process has 50 threads and the other has only 5 threads. If we use the <strong class="markup--strong markup--p-strong">process scope </strong>for thread management, the OS will not know what is going on for each of these two processes. Then the OS will potentially allocate equal kernel-level threads for these two threads and the former one will be slow.</p><p name="a96b" id="a96b" class="graf graf--p graf-after--p">However, if we use the <strong class="markup--strong markup--p-strong">system scope</strong> for thread management, the OS will allocate different resources for these two threads because the OS really knows how many threads they have.</p><p name="e566" id="e566" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Multithreading Patterns</strong></p><p name="bbc0" id="bbc0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Boss-workers Pattern</strong></p><p name="c5ca" id="c5ca" class="graf graf--p graf-after--p">This is a popular multithreading pattern with one <strong class="markup--strong markup--p-strong">boss thread</strong> assigning works to the other threads and other <strong class="markup--strong markup--p-strong">worker threads</strong> perform the entire task that is assigned to them. Since we only have one boss thread that must execute on every single piece of work that arrives in the system, the throughput of the system is limited by the boss thread. Therefore, we must keep the boss thread efficient. So the less time the boss spent on each order, the more efficient our system will be.</p><p name="c03f" id="c03f" class="graf graf--p graf-after--p">The boss can actually assign the works to the workers by two approaches,</p><ul class="postList"><li name="4b54" id="4b54" class="graf graf--li graf-after--p">The boss <strong class="markup--strong markup--li-strong">keeps track</strong> of the free workers and then directly signaling the specific worker. Also, the worker has to the boss that it accepts the work, like a handshake. The advantage of this approach is that the <strong class="markup--strong markup--li-strong">workers don’t have to synchronize</strong> with each other. However, because the boss needs to supervise all these worker threads, the boss will spend more time on each order and the general <strong class="markup--strong markup--li-strong">throughput will go down</strong>.</li><li name="7e21" id="7e21" class="graf graf--li graf-after--li">The boss and the workers <strong class="markup--strong markup--li-strong">share an order queue</strong>. When the boss grabs an order, it will place the order in the shared queue so that each of the free workers can look into it and then picks up some pending work. The downside of this method is that the boss and the worker now have to <strong class="markup--strong markup--li-strong">synchronize</strong> the access to the shared queue. Because the boss now has less work to do for each order, it will spend less time on each order and the general <strong class="markup--strong markup--li-strong">throughput will go up</strong>.</li></ul><p name="156f" id="156f" class="graf graf--p graf-after--li">Usually, we tend to pick the latter approach for better system performance if we are willing to use the boss-workers pattern for multithreading.</p><figure name="6a2a" id="6a2a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*u7o8Tk_y2P5eh0Pd3ClNvQ.png" data-width="1938" data-height="872" src="https://cdn-images-1.medium.com/max/800/1*u7o8Tk_y2P5eh0Pd3ClNvQ.png"></figure><p name="d7e1" id="d7e1" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Determine the Number of Workers</strong></p><p name="f7b5" id="f7b5" class="graf graf--p graf-after--p">Suppose we have fewer workers than we need, the shared queue will be easy to be full. When the queue is full, the boss has to wait for an empty space so the average time the boss spends on each order will increase. This will result in a worse throughput. However, if we have more workers than we need, we are going to add some overheads to the system and it will result in a waste of resources. So how many workers are enough for this boss-worker pattern? Actually, we usually have two approaches for this, we can have either a <strong class="markup--strong markup--p-strong">statically sized</strong> worker pool or we can have a <strong class="markup--strong markup--p-strong">dynamically sized</strong> worker pool. We are not going to go into the details of this problem and here, we can just have this brief introduction.</p><p name="54b0" id="54b0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Boss-workers Pattern Evaluation</strong></p><ul class="postList"><li name="6067" id="6067" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Simplicity</strong>: 1 thread assigns work to all other threads</li><li name="7326" id="7326" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Overheads</strong>: we have to think about the worker thread pool management including synchronization for a shared buffer</li><li name="9c6a" id="9c6a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Ignore Locality</strong>: the works are randomly assigned to the workers instead of concerning about the optimization if we have specifications among the workers (because the worker thread will have the information in the <strong class="markup--strong markup--li-strong">cache</strong> if they are assigned to a similar task). Actually, instead of all workers randomly assigned with tasks, we can have workers specialized for certain tasks. Even though this requires the boss to look into each of the order and classify different kinds of jobs, the performance loss will be offset by the improvement of the specialization of the workers. This is called the <strong class="markup--strong markup--li-strong">quality of service management (QoS management)</strong>. However, <strong class="markup--strong markup--li-strong">load balancing</strong> (how many workers we need for each type of task) can become a serious problem if we decide to specialize the workers.</li></ul><p name="62de" id="62de" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) Pipeline Pattern</strong></p><p name="b691" id="b691" class="graf graf--p graf-after--p">Another approach to assigning the works is by pipeline. Each of the tasks will be divided into several <strong class="markup--strong markup--p-strong">subtasks</strong> that each of them is performed by a separate thread. So the entire task will be equivalent to the pipeline of the threads. Multiple tasks can concurrently exist in the system and each of them will be in different pipeline stages.</p><figure name="7804" id="7804" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RrAiJ2QRqKY04IHBqSo1LQ.png" data-width="1796" data-height="350" src="https://cdn-images-1.medium.com/max/800/1*RrAiJ2QRqKY04IHBqSo1LQ.png"></figure><p name="3ea8" id="3ea8" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Long Time Stage for Pipeline</strong></p><p name="3608" id="3608" class="graf graf--p graf-after--p">Similar to the barrel effect, the throughput of this pipeline is determined by the stage that takes the longest time to execute. Well ideally, we would like to have a pipeline with the same time of execution on each stage. In order to deal with the stage that needs more time to execute, we can <strong class="markup--strong markup--p-strong">assign more threads</strong> to that stage (subtask). The <strong class="markup--strong markup--p-strong">thread pool </strong>of a subtask consists of all the threads we assigned to this subtask.</p><figure name="bd34" id="bd34" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*em4kxC3c6bBcVjNmxyob6g.png" data-width="2014" data-height="484" src="https://cdn-images-1.medium.com/max/800/1*em4kxC3c6bBcVjNmxyob6g.png"></figure><p name="6110" id="6110" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(6) Short Time Difference for Pipeline</strong></p><p name="fcbc" id="fcbc" class="graf graf--p graf-after--p">Even though we can have more threads assigned to a subtask that takes a longer time to execute, it can be quite a problem for us to deal with the short time difference between different stages. For example, if stage 3 is still executing when stage 2 is ready, stage 2 has to wait for the stage to finish. This will reduce the overall throughput of the pipeline. To deal with this problem, shared buffers are used for communications between different stages or threads.</p><p name="4004" id="4004" class="graf graf--p graf-after--p">When a thread finishes, the result will be stored in a buffer and then it will start to execute the next one. If the next thread finishes, it can grab the result from the last thread directly from the shared buffer.</p><figure name="8e82" id="8e82" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*40R_BLnTHFYBb44nxOoWLQ.png" data-width="2000" data-height="522" src="https://cdn-images-1.medium.com/max/800/1*40R_BLnTHFYBb44nxOoWLQ.png"></figure><p name="d12c" id="d12c" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(7) Pipeline Evaluation</strong></p><ul class="postList"><li name="3492" id="3492" class="graf graf--li graf-after--p">[Adv] Specialization and Locality: highly specialized threads for specific subtasks</li><li name="3cb2" id="3cb2" class="graf graf--li graf-after--li">[Dis] Balancing and Synchronization Overheads</li></ul><p name="1e5b" id="1e5b" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(8) Layered Pattern</strong></p><p name="e723" id="e723" class="graf graf--p graf-after--p">Each layer of a layered pattern is related to a group of subtasks and each layer can execute any one of the subtasks that correspond to it. An end-to-end task must go up and down all the layers to finish.</p><p name="4f57" id="4f57" class="graf graf--p graf-after--p">The advantages of a layered pattern are its <strong class="markup--strong markup--p-strong">specialization</strong> and <strong class="markup--strong markup--p-strong">less fine-grained</strong> than the pipeline (this means that it is much easier to decide how many threads to allocate for each layer). The downside are that,</p><ul class="postList"><li name="cd87" id="cd87" class="graf graf--li graf-after--p">not suitable for all applications (this pattern highly relies on the subtasks)</li><li name="4275" id="4275" class="graf graf--li graf-after--li graf--trailing">require more synchronization because all the layers have to communicate</li></ul></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/6b405555d5ac"><time class="dt-published" datetime="2021-01-31T18:25:25.393Z">January 31, 2021</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/operating-system-7-thread-part-3-kernel-user-level-threads-and-multithreading-patterns-6b405555d5ac" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>