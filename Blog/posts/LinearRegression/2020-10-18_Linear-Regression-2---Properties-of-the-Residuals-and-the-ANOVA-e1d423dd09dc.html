<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Linear Regression 2 | Properties of the Residuals and the ANOVA</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Linear Regression 2 | Properties of the Residuals and the ANOVA</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Linear Regression
</section>
<section data-field="body" class="e-content">
<section name="3b88" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="484a" id="484a" class="graf graf--h3 graf--leading graf--title">Linear Regression 2 | <strong class="markup--strong markup--h3-strong">Properties of the Residuals and the ANOVA</strong></h3><figure name="4a05" id="4a05" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*JRXYaS8AmzV2HWueCj2mRA.png" data-width="1514" data-height="716" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*JRXYaS8AmzV2HWueCj2mRA.png"></figure><ol class="postList"><li name="97b3" id="97b3" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Recall: Basic Definition</strong></li></ol><p name="fc45" id="fc45" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) The Definition of the Fitted Value</strong></p><p name="c782" id="c782" class="graf graf--p graf-after--p">For a given xi, we can calculate a yi-cap through the fitted line of the linear regression, then this yi-cap is the so-called fitted value given xi.</p><figure name="336b" id="336b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*E9_P76XUYSfLWZM_.png" data-width="1126" data-height="62" src="https://cdn-images-1.medium.com/max/800/0*E9_P76XUYSfLWZM_.png"></figure><p name="4a54" id="4a54" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) The Definition of the Residuals</strong></p><p name="e5d3" id="e5d3" class="graf graf--p graf-after--p">As we have defined, residual is the difference between the yi-cap and the true value yi as the residual of xi, which can be denoted as ei.</p><figure name="9cd1" id="9cd1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*JSqIjtQXEX07B41V.png" data-width="1126" data-height="62" src="https://cdn-images-1.medium.com/max/800/0*JSqIjtQXEX07B41V.png"></figure><p name="ab4c" id="ab4c" class="graf graf--p graf-after--figure">Thus, by the mathematical model of the linear regression, we can have,</p><figure name="9d35" id="9d35" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*S56AcVdBqfR6to7N.png" data-width="1042" data-height="72" src="https://cdn-images-1.medium.com/max/800/0*S56AcVdBqfR6to7N.png"></figure><p name="1897" id="1897" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) The Definition of Prediction</strong></p><p name="b5d0" id="b5d0" class="graf graf--p graf-after--p">At a particular x*, we can use the fitted line to calculate the fitted value y-cap(x*) and we can do this even if x* is <strong class="markup--strong markup--p-strong">NOT</strong> in our original dataset. If this x* is not in our original dataset, then this y-cap(x*) is called a predictor.</p><figure name="5df8" id="5df8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Yt5WHSfS7UHjGyw0R-tRzQ.png" data-width="1138" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*Yt5WHSfS7UHjGyw0R-tRzQ.png"></figure><figure name="701c" id="701c" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*jK9M6jZSRyGNYe9HYrSGQA.png" data-width="2212" data-height="708" src="https://cdn-images-1.medium.com/max/800/1*jK9M6jZSRyGNYe9HYrSGQA.png"></figure><p name="c06c" id="c06c" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) A Critical Conclusion by Mean</strong></p><p name="12b8" id="12b8" class="graf graf--p graf-after--p">By definition of the sample mean of xi, we can derive that,</p><figure name="5924" id="5924" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4BXDmsMcS5eQn1b2sEyAYw.png" data-width="1138" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*4BXDmsMcS5eQn1b2sEyAYw.png"></figure><p name="3a21" id="3a21" class="graf graf--p graf-after--figure">Also, because x-bar is a constant,</p><figure name="098d" id="098d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9lwJiMJmsVNOTZE4j-sTbA.png" data-width="1138" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*9lwJiMJmsVNOTZE4j-sTbA.png"></figure><p name="cadc" id="cadc" class="graf graf--p graf-after--figure">thus,</p><figure name="915e" id="915e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BjiyBvOJwDQVifHEKeHjHg.png" data-width="1138" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*BjiyBvOJwDQVifHEKeHjHg.png"></figure><p name="00b6" id="00b6" class="graf graf--p graf-after--figure">then, the first central moment of xi must equal to zero,</p><figure name="850b" id="850b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*N3VYxXA4ITrOl9xPesLtOA.png" data-width="1202" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*N3VYxXA4ITrOl9xPesLtOA.png"></figure><p name="0a20" id="0a20" class="graf graf--p graf-after--figure">and the second central moment can be as follows,</p><figure name="dd1b" id="dd1b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*bJ-thKH5oGlL6eAu8dfeaw.png" data-width="1202" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*bJ-thKH5oGlL6eAu8dfeaw.png"></figure><p name="c6e7" id="c6e7" class="graf graf--p graf-after--figure">Also, with,</p><figure name="811e" id="811e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*TcnjQvUGoFuQWMfVy3NFRQ.png" data-width="1202" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*TcnjQvUGoFuQWMfVy3NFRQ.png"></figure><p name="65e9" id="65e9" class="graf graf--p graf-after--figure">then,</p><figure name="bdcb" id="bdcb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ijshWudGAl-KpZwDVLNZuw.png" data-width="1202" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*ijshWudGAl-KpZwDVLNZuw.png"></figure><p name="5c3d" id="5c3d" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">2. Properties of the Residual</strong></p><ul class="postList"><li name="e9d4" id="e9d4" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Property #1: Zero Sum</strong></li></ul><figure name="3389" id="3389" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*eVmPeH6H5Xw9qKbX_Mv_hw.png" data-width="1038" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*eVmPeH6H5Xw9qKbX_Mv_hw.png"></figure><p name="570e" id="570e" class="graf graf--p graf-after--figure">This is not an assumption since we are already under the assumption of a Gaussian distributed error.</p><p name="5dee" id="5dee" class="graf graf--p graf-after--p">Proof:</p><figure name="03e1" id="03e1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kgTaIrKx2iYRlzb93zIGww.png" data-width="1038" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*kgTaIrKx2iYRlzb93zIGww.png"></figure><p name="e872" id="e872" class="graf graf--p graf-after--figure">then,</p><figure name="1978" id="1978" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jQXOmFWPg9JJvODFWIM_kQ.png" data-width="1038" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*jQXOmFWPg9JJvODFWIM_kQ.png"></figure><p name="bf15" id="bf15" class="graf graf--p graf-after--figure">then, by solution of the OLS,</p><figure name="e5cb" id="e5cb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*g0Y_dkG0XDhwiNjlJXGpaw.png" data-width="1038" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*g0Y_dkG0XDhwiNjlJXGpaw.png"></figure><ul class="postList"><li name="9f86" id="9f86" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Property #2: Zero Expectation</strong></li></ul><figure name="fa7c" id="fa7c" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*C8PXRr7a6q7FniaEB4Husg.png" data-width="1038" data-height="52" src="https://cdn-images-1.medium.com/max/800/1*C8PXRr7a6q7FniaEB4Husg.png"></figure><p name="0d55" id="0d55" class="graf graf--p graf-after--figure">Proof:</p><p name="1718" id="1718" class="graf graf--p graf-after--p">By the zero-sum property,</p><figure name="ecee" id="ecee" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jOW--nqRR-MljPQZ52haaQ.png" data-width="1038" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*jOW--nqRR-MljPQZ52haaQ.png"></figure><ul class="postList"><li name="253f" id="253f" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Property #3: Zero Covariance for One Term</strong></li></ul><p name="5cee" id="5cee" class="graf graf--p graf-after--li">In order to get this assumption, we have to make a crucial assumption, we want it to be the case that knowing something about x does not give us any information about e, so that they are completely unrelated. That is to say that,</p><figure name="3518" id="3518" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KA7VBYpDhgErUxizCXhKTg.png" data-width="1038" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*KA7VBYpDhgErUxizCXhKTg.png"></figure><p name="3e22" id="3e22" class="graf graf--p graf-after--figure">Based on this assumption, we can know that the covariance of the residual e and any term in the regression model is zero, that is,</p><figure name="8617" id="8617" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*C-4bBCVhvNEi90n2ZPMK_g.png" data-width="1092" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*C-4bBCVhvNEi90n2ZPMK_g.png"></figure><p name="043c" id="043c" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Proof Method #1: </strong>with<strong class="markup--strong markup--p-strong"> </strong>the<strong class="markup--strong markup--p-strong"> </strong>crucial assumption</p><p name="19ed" id="19ed" class="graf graf--p graf-after--p">First of all, by the law of iterated expectations,</p><figure name="76a0" id="76a0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qoji-G2TapyIaVsSYstQGQ.png" data-width="1092" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*qoji-G2TapyIaVsSYstQGQ.png"></figure><p name="7b8b" id="7b8b" class="graf graf--p graf-after--figure">then by conditional rule,</p><figure name="96e4" id="96e4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9wq2ImeAU5-pkHXHIIvCbA.png" data-width="1092" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*9wq2ImeAU5-pkHXHIIvCbA.png"></figure><p name="002c" id="002c" class="graf graf--p graf-after--figure">then because we are under the crucial assumption that,</p><figure name="95d6" id="95d6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KA7VBYpDhgErUxizCXhKTg.png" data-width="1038" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*KA7VBYpDhgErUxizCXhKTg.png"></figure><p name="bfc6" id="bfc6" class="graf graf--p graf-after--figure">This is to say that,</p><figure name="a2f8" id="a2f8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*93LB9ei6fSrX0TkQIZCNMA.png" data-width="1092" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*93LB9ei6fSrX0TkQIZCNMA.png"></figure><p name="0d8a" id="0d8a" class="graf graf--p graf-after--figure">Then we can know that, because,</p><figure name="000d" id="000d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RbAHjnxYAWBNKWIEAKtPeg.png" data-width="1092" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*RbAHjnxYAWBNKWIEAKtPeg.png"></figure><p name="1ddc" id="1ddc" class="graf graf--p graf-after--figure">then,</p><figure name="e84c" id="e84c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vEbXRB333oh0VeX4Fi06fA.png" data-width="1092" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*vEbXRB333oh0VeX4Fi06fA.png"></figure><p name="7b28" id="7b28" class="graf graf--p graf-after--figure">By the definition of the covariance,</p><figure name="f740" id="f740" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*IfeMy_Q1li9QC0-m_A_mIA.png" data-width="1092" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*IfeMy_Q1li9QC0-m_A_mIA.png"></figure><p name="1950" id="1950" class="graf graf--p graf-after--figure">by property #2,</p><figure name="a5d3" id="a5d3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*C8PXRr7a6q7FniaEB4Husg.png" data-width="1038" data-height="52" src="https://cdn-images-1.medium.com/max/800/1*C8PXRr7a6q7FniaEB4Husg.png"></figure><p name="a847" id="a847" class="graf graf--p graf-after--figure">then,</p><figure name="189b" id="189b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Af83ETFxzSFKjWAqZ_8mhg.png" data-width="1092" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*Af83ETFxzSFKjWAqZ_8mhg.png"></figure><p name="ff11" id="ff11" class="graf graf--p graf-after--figure">thus,</p><figure name="146c" id="146c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*84Tw-f83QIK76C2_vOdZLw.png" data-width="1092" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*84Tw-f83QIK76C2_vOdZLw.png"></figure><p name="4610" id="4610" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Proof Method #2: </strong>without<strong class="markup--strong markup--p-strong"> </strong>crucial assumption</p><p name="edbf" id="edbf" class="graf graf--p graf-after--p">Let’s see, what if we loose the previous assumption. This is quite interesting because we are going to use this for further discussions. We have already known than</p><p name="78a1" id="78a1" class="graf graf--p graf-after--p">Suppose we don’t have the property #2 and we don’t have the assumption that,</p><figure name="6c8f" id="6c8f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KA7VBYpDhgErUxizCXhKTg.png" data-width="1038" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*KA7VBYpDhgErUxizCXhKTg.png"></figure><p name="db36" id="db36" class="graf graf--p graf-after--figure">What we only have is that,</p><figure name="d66d" id="d66d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2ooQ5_RllUlP0lYAtFenfw.png" data-width="1050" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*2ooQ5_RllUlP0lYAtFenfw.png"></figure><p name="dc4f" id="dc4f" class="graf graf--p graf-after--figure">can we still have this conclusion that the covariance of the residual e and any term in the regression model is zero? The answer is yes, and here’s our proof:</p><p name="2098" id="2098" class="graf graf--p graf-after--p">First of all, by the law of iterated expectations,</p><figure name="f971" id="f971" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qoji-G2TapyIaVsSYstQGQ.png" data-width="1092" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*qoji-G2TapyIaVsSYstQGQ.png"></figure><p name="9805" id="9805" class="graf graf--p graf-after--figure">then by conditional rule,</p><figure name="6288" id="6288" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9wq2ImeAU5-pkHXHIIvCbA.png" data-width="1092" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*9wq2ImeAU5-pkHXHIIvCbA.png"></figure><p name="1dda" id="1dda" class="graf graf--p graf-after--figure">then because we are under the assumption that,</p><figure name="d189" id="d189" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2ooQ5_RllUlP0lYAtFenfw.png" data-width="1050" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*2ooQ5_RllUlP0lYAtFenfw.png"></figure><p name="771f" id="771f" class="graf graf--p graf-after--figure">then,</p><figure name="3fbd" id="3fbd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*z_dcI3xOURN9S2jruQSYjA.png" data-width="1012" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*z_dcI3xOURN9S2jruQSYjA.png"></figure><p name="09eb" id="09eb" class="graf graf--p graf-after--figure">thus,</p><figure name="86ab" id="86ab" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KrNedsa0Sk2qWMBw_ehbPg.png" data-width="1012" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*KrNedsa0Sk2qWMBw_ehbPg.png"></figure><p name="6f06" id="6f06" class="graf graf--p graf-after--figure">By the definition of the covariance,</p><figure name="6f78" id="6f78" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*IfeMy_Q1li9QC0-m_A_mIA.png" data-width="1092" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*IfeMy_Q1li9QC0-m_A_mIA.png"></figure><p name="a4a2" id="a4a2" class="graf graf--p graf-after--figure">then,</p><figure name="dbdf" id="dbdf" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9zkWEVPKhj5BYVLMZxq8tw.png" data-width="1012" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*9zkWEVPKhj5BYVLMZxq8tw.png"></figure><p name="fc11" id="fc11" class="graf graf--p graf-after--figure">Thus, we can find out that the 0 covariance property holds if we are only given the assumption that,</p><figure name="9316" id="9316" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lQ2P0P_cWMtDkmMQWi9Z0w.png" data-width="1012" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*lQ2P0P_cWMtDkmMQWi9Z0w.png"></figure><ul class="postList"><li name="1dcc" id="1dcc" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Property #4: Zero Covariance for All Terms</strong></li></ul><figure name="4820" id="4820" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*Df_xOKRrdScUFhU08_Md2A.png" data-width="1078" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*Df_xOKRrdScUFhU08_Md2A.png"></figure><p name="59c7" id="59c7" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Proof:</strong></p><p name="5189" id="5189" class="graf graf--p graf-after--p">By the fitting line,</p><figure name="e5e7" id="e5e7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*M1hE54X-IHoL7TG5oFFDlQ.png" data-width="1078" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*M1hE54X-IHoL7TG5oFFDlQ.png"></figure><p name="5982" id="5982" class="graf graf--p graf-after--figure">then,</p><figure name="90dc" id="90dc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yzin1lHk8rBAqE6PGgOQfw.png" data-width="1078" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*yzin1lHk8rBAqE6PGgOQfw.png"></figure><p name="0f27" id="0f27" class="graf graf--p graf-after--figure">then,</p><figure name="1d19" id="1d19" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RyePjg_dSmoAH-MQjDYFog.png" data-width="1078" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*RyePjg_dSmoAH-MQjDYFog.png"></figure><p name="dc10" id="dc10" class="graf graf--p graf-after--figure">then,</p><figure name="8f32" id="8f32" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YeAkeXF6P7y9Jb4_gOBPKQ.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*YeAkeXF6P7y9Jb4_gOBPKQ.png"></figure><p name="1185" id="1185" class="graf graf--p graf-after--figure">then,</p><figure name="5cb3" id="5cb3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*H-LmYD0BiVZXz2raf2ayDw.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*H-LmYD0BiVZXz2raf2ayDw.png"></figure><p name="adb6" id="adb6" class="graf graf--p graf-after--figure">by the law of iterated expectations,</p><figure name="3908" id="3908" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qoji-G2TapyIaVsSYstQGQ.png" data-width="1092" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*qoji-G2TapyIaVsSYstQGQ.png"></figure><p name="6034" id="6034" class="graf graf--p graf-after--figure">then by conditional rule,</p><figure name="ee14" id="ee14" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9wq2ImeAU5-pkHXHIIvCbA.png" data-width="1092" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*9wq2ImeAU5-pkHXHIIvCbA.png"></figure><p name="762a" id="762a" class="graf graf--p graf-after--figure">then because we are under the assumption that,</p><figure name="81d2" id="81d2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2ooQ5_RllUlP0lYAtFenfw.png" data-width="1050" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*2ooQ5_RllUlP0lYAtFenfw.png"></figure><p name="714d" id="714d" class="graf graf--p graf-after--figure">then,</p><figure name="6a5e" id="6a5e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*z_dcI3xOURN9S2jruQSYjA.png" data-width="1012" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*z_dcI3xOURN9S2jruQSYjA.png"></figure><p name="27e0" id="27e0" class="graf graf--p graf-after--figure">thus,</p><figure name="9047" id="9047" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KrNedsa0Sk2qWMBw_ehbPg.png" data-width="1012" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*KrNedsa0Sk2qWMBw_ehbPg.png"></figure><p name="312d" id="312d" class="graf graf--p graf-after--figure">then,</p><figure name="5d15" id="5d15" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PcaFwDim_Fsjh0ZzeWM_Yw.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*PcaFwDim_Fsjh0ZzeWM_Yw.png"></figure><p name="63e3" id="63e3" class="graf graf--p graf-after--figure">thus,</p><figure name="b7db" id="b7db" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*19NrvhylXsjP74TDGjwERQ.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*19NrvhylXsjP74TDGjwERQ.png"></figure><p name="6c34" id="6c34" class="graf graf--p graf-after--figure">Also, to have this conclusion, what we only need to assume is,</p><figure name="fa87" id="fa87" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2ooQ5_RllUlP0lYAtFenfw.png" data-width="1050" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*2ooQ5_RllUlP0lYAtFenfw.png"></figure><p name="b21f" id="b21f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">3. Parameter (Estimator) Distribution and Estimator Testing</strong></p><p name="6cf6" id="6cf6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Recall: The Variance of the Estimator</strong></p><p name="5f72" id="5f72" class="graf graf--p graf-after--p">Based on our discussion in the last section, we can have that,</p><figure name="92f3" id="92f3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*LJ5MUg99l6dYRUM5.png" data-width="1016" data-height="98" src="https://cdn-images-1.medium.com/max/800/0*LJ5MUg99l6dYRUM5.png"></figure><p name="7484" id="7484" class="graf graf--p graf-after--figure">and,</p><figure name="c5ef" id="c5ef" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*7S3_5-k98Fjgj2-J.png" data-width="1102" data-height="98" src="https://cdn-images-1.medium.com/max/800/0*7S3_5-k98Fjgj2-J.png"></figure><p name="65e8" id="65e8" class="graf graf--p graf-after--figure">However, notice that we have a problem that we don’t know anything about the σ² in practice, because we don’t have the statistics about the truth by any given dataset. Thus, in fact, these formulas are only valid in the theory.</p><p name="bee3" id="bee3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Definition of the Sum of Squared Errors (aka. SSE)</strong></p><p name="9643" id="9643" class="graf graf--p graf-after--p">We have already the residuals in the discussions above, and now, we would like to given the definition of the sum of squared errors. Actually, it is called the sum of squared errors but it is actually not a sum of square errors. It is in fact defined by the sum of square residuals and this can be quite tricky. This is to say that, we define</p><figure name="394e" id="394e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2FDUuC1178p7bPXS2k3DDQ.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*2FDUuC1178p7bPXS2k3DDQ.png"></figure><p name="8daf" id="8daf" class="graf graf--p graf-after--figure">Based on the OLS, we can know that the estimators are BLUE if SSE is minimized. By our model and the Gaussian assumption, we can know that,</p><figure name="ef6d" id="ef6d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*piND-r32ta6NE5DnNx-w_g.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*piND-r32ta6NE5DnNx-w_g.png"></figure><p name="d74a" id="d74a" class="graf graf--p graf-after--figure">Also,</p><figure name="eec6" id="eec6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NMcp0HpOcWxRVcsH9fRV_w.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*NMcp0HpOcWxRVcsH9fRV_w.png"></figure><p name="71a6" id="71a6" class="graf graf--p graf-after--figure">then,</p><figure name="d365" id="d365" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tW6RBHK8pUdBacwGP-QcWQ.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*tW6RBHK8pUdBacwGP-QcWQ.png"></figure><p name="3085" id="3085" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Recall: Chi-Square Distribution</strong></p><p name="a506" id="a506" class="graf graf--p graf-after--p">If X1, …, Xn are independently identical distributed (aka, i.i.d) normal random variables with mean μ and variance σ², then,</p><figure name="d2f2" id="d2f2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*x3vUvZmfH_2yRZ3U.png" data-width="1266" data-height="222" src="https://cdn-images-1.medium.com/max/800/0*x3vUvZmfH_2yRZ3U.png"></figure><p name="829a" id="829a" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) The Definition of Degree of Freedom to the Residuals</strong></p><p name="a73d" id="a73d" class="graf graf--p graf-after--p">In linear regression, the definition of the degree of freedom to the residuals is the number of the instance in the sample minus the number of the parameters in our model (of course, including the intercept). This is to say that,</p><figure name="b5a3" id="b5a3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DaitLRowPmIaEsMX-x0ZCg.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*DaitLRowPmIaEsMX-x0ZCg.png"></figure><p name="63ec" id="63ec" class="graf graf--p graf-after--figure">Particularly, in the model of SLR, because we have two parameters (a slope and an intercept), then we can conclude that the degree of freedom for SLR is,</p><figure name="1711" id="1711" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Kvhynef8bBWLOW1aueMdWw.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*Kvhynef8bBWLOW1aueMdWw.png"></figure><p name="b8ed" id="b8ed" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) The Definition of the Mean of Squared Errors (aka. MSE)</strong></p><p name="dd3f" id="dd3f" class="graf graf--p graf-after--p">Because y1, …, yn are independently identically distributed normal random variables with the mean (β0+β1x1) and the variance σ², then by the definition of the Chi-Square Distribution, we can conclude that,</p><figure name="c2aa" id="c2aa" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*B6pYIscvH6TnLa3D6l4Bfw.png" data-width="1078" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*B6pYIscvH6TnLa3D6l4Bfw.png"></figure><p name="1fb3" id="1fb3" class="graf graf--p graf-after--figure">This is not a formal rigorous proof, and I will add a more rigorous if time permits in the future.</p><p name="531d" id="531d" class="graf graf--p graf-after--p">Buy this formula, we can know by the property of the χ² distribution,</p><figure name="165d" id="165d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kEzRtcvxpC8DAqloF_bUpg.png" data-width="1078" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*kEzRtcvxpC8DAqloF_bUpg.png"></figure><p name="b08c" id="b08c" class="graf graf--p graf-after--figure">Because n-2 and σ² are all constant, then,</p><figure name="d2c0" id="d2c0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NB2_EUDWZyxOOoHeJYDxMg.png" data-width="1078" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*NB2_EUDWZyxOOoHeJYDxMg.png"></figure><p name="1168" id="1168" class="graf graf--p graf-after--figure">Suppose we define the mean of squared errors (MSE) as,</p><figure name="90fe" id="90fe" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*n9djDwMAip0SMZyrVvMnug.png" data-width="1078" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*n9djDwMAip0SMZyrVvMnug.png"></figure><p name="259e" id="259e" class="graf graf--p graf-after--figure">Then we can have that,</p><figure name="96fb" id="96fb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*r6WC88_t_wjXSKT90yAvTA.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*r6WC88_t_wjXSKT90yAvTA.png"></figure><p name="7e5b" id="7e5b" class="graf graf--p graf-after--figure">This means that MSE is an unbiased estimator of the population variance. Thus, we can use this to replace σ² for estimators if we are given only a dataset.</p><p name="e4ce" id="e4ce" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Estimator Distribution with known σ²</strong></p><p name="a6a0" id="a6a0" class="graf graf--p graf-after--p">By our previous discussions, when we have a known σ², then,</p><figure name="c387" id="c387" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*q8DYVCTRr9JhNsFFdayh5g.png" data-width="1078" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*q8DYVCTRr9JhNsFFdayh5g.png"></figure><p name="fb3a" id="fb3a" class="graf graf--p graf-after--figure">also,</p><figure name="295b" id="295b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YfF0wldSrGQzQmAr69eLzw.png" data-width="1078" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*YfF0wldSrGQzQmAr69eLzw.png"></figure><p name="e444" id="e444" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(7) Estimator Distribution with unknown σ²</strong></p><p name="ea18" id="ea18" class="graf graf--p graf-after--p">Suppose we don’t have a given σ², then, the distribution of β1-cap is a student’s T distribution.</p><figure name="b720" id="b720" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kZ_nlmmz6qkA9Vt7k2M8CA.png" data-width="1078" data-height="180" src="https://cdn-images-1.medium.com/max/800/1*kZ_nlmmz6qkA9Vt7k2M8CA.png"></figure><p name="9061" id="9061" class="graf graf--p graf-after--figure">This is also to say that,</p><figure name="409f" id="409f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xxft02Tt2fXxuS2mbpz4pQ.png" data-width="1078" data-height="124" src="https://cdn-images-1.medium.com/max/800/1*xxft02Tt2fXxuS2mbpz4pQ.png"></figure><p name="05b7" id="05b7" class="graf graf--p graf-after--figure">and this is also,</p><figure name="59ca" id="59ca" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MjznRFSMcPeTR9_o0nMXwA.png" data-width="1078" data-height="124" src="https://cdn-images-1.medium.com/max/800/1*MjznRFSMcPeTR9_o0nMXwA.png"></figure><p name="4e4a" id="4e4a" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(8) Application of the Estimator Distribution: </strong>Significance Testing</p><p name="58f8" id="58f8" class="graf graf--p graf-after--p">Because we have known the distribution of the estimator, then we are able to test whether an estimator is significant by hypothesis testing. We can define the null hypothesis Ho: β1 = 0, and the alternative hypothesis H1: β1 ≠ 0. By this method, we can know that, if we can reject Ho, we can know that H1 is significant on the given significant level. Thus, we can have the T statistics equals,</p><figure name="1325" id="1325" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2Sc_IwSjUyIt4-NlaOBoKw.png" data-width="1078" data-height="140" src="https://cdn-images-1.medium.com/max/800/1*2Sc_IwSjUyIt4-NlaOBoKw.png"></figure><p name="96f6" id="96f6" class="graf graf--p graf-after--figure">If,</p><figure name="bdb1" id="bdb1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8uXiCO3c92-6TQnGSgGYRA.png" data-width="1078" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*8uXiCO3c92-6TQnGSgGYRA.png"></figure><p name="7383" id="7383" class="graf graf--p graf-after--figure">or,</p><figure name="d55c" id="d55c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ELfLuE9k7OvBPWs-oxVL6Q.png" data-width="1078" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*ELfLuE9k7OvBPWs-oxVL6Q.png"></figure><p name="728b" id="728b" class="graf graf--p graf-after--figure">then, we are able to reject Ho. And this is equivalent to say that the independent variable x is significant to the dependent variable y.</p><p name="431a" id="431a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. ANOVA</strong></p><p name="3361" id="3361" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Introduction of ANOVA</strong></p><p name="a67e" id="a67e" class="graf graf--p graf-after--p">The whole name of ANOVA is called the analysis of variance, and it is a way for us to test the significance of more than one parameter. It is more commonly used in MLR (multiple linear regression) it saves time to test that if there’s any parameter that is not significant. ANOVA also tests the interval parameter in SLR.</p><p name="ab11" id="ab11" class="graf graf--p graf-after--p">For SLR, we can have the following ANOVA table,</p><figure name="564d" id="564d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*L2a6nXduS87vpIS_up7dBQ.png" data-width="1386" data-height="288" src="https://cdn-images-1.medium.com/max/800/1*L2a6nXduS87vpIS_up7dBQ.png"></figure><p name="a655" id="a655" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) The Definition of the Total Sum of Squares</strong></p><p name="63cb" id="63cb" class="graf graf--p graf-after--p">The total sum of squares is the variance given the total dataset. Thus, by the definition of the sample distribution, we can then have,</p><figure name="e81a" id="e81a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*cYghfq9bCaxhVG3B_i5J5Q.png" data-width="1026" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*cYghfq9bCaxhVG3B_i5J5Q.png"></figure><p name="7d78" id="7d78" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) The Definition of the Regression Sum of Squares</strong></p><p name="1fa8" id="1fa8" class="graf graf--p graf-after--p">The total sum of squares is the variance given by values generated by the fitted line. It is actually the natural variance of variance that we can get if x is strictly and linearly related to y. So the source of this variance is from the regression itself.</p><figure name="ec9a" id="ec9a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PQcr3YU8STv8XiU0IHIDzQ.png" data-width="1026" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*PQcr3YU8STv8XiU0IHIDzQ.png"></figure><p name="eab0" id="eab0" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4)</strong> <strong class="markup--strong markup--p-strong">Total Sum of Squares Explanation</strong></p><p name="1ddf" id="1ddf" class="graf graf--p graf-after--p">Based on the definitions above, we can have the theorem that,</p><figure name="48ea" id="48ea" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AP-L0rp9Eq64pMKq75_j1w.png" data-width="1026" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*AP-L0rp9Eq64pMKq75_j1w.png"></figure><p name="bfb8" id="bfb8" class="graf graf--p graf-after--figure">This is also to say that,</p><figure name="7954" id="7954" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Wvx1K6kLYsnNX1pg--Ctgg.png" data-width="1026" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*Wvx1K6kLYsnNX1pg--Ctgg.png"></figure><figure name="d298" id="d298" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*bvQcFi7zJIpu28fVHZ-bsQ.png" data-width="1026" data-height="332" src="https://cdn-images-1.medium.com/max/800/1*bvQcFi7zJIpu28fVHZ-bsQ.png"></figure><p name="1be8" id="1be8" class="graf graf--p graf-after--figure">Proof:</p><figure name="8fc7" id="8fc7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tetLigtcWk3ilAxuS_pdhA.png" data-width="1026" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*tetLigtcWk3ilAxuS_pdhA.png"></figure><p name="dca2" id="dca2" class="graf graf--p graf-after--figure">then,</p><figure name="3fd8" id="3fd8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Fga1lx-RZGjYVhdnXkNBVQ.png" data-width="1124" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*Fga1lx-RZGjYVhdnXkNBVQ.png"></figure><p name="3774" id="3774" class="graf graf--p graf-after--figure">because</p><figure name="b629" id="b629" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*LAIu4pd55xk8BUWDbCv3iA.png" data-width="1124" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*LAIu4pd55xk8BUWDbCv3iA.png"></figure><p name="655d" id="655d" class="graf graf--p graf-after--figure">then,</p><figure name="e74d" id="e74d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3Ej7f2YXJzX5XubuLOn7WQ.png" data-width="1124" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*3Ej7f2YXJzX5XubuLOn7WQ.png"></figure><p name="1383" id="1383" class="graf graf--p graf-after--figure">then,</p><figure name="ef2b" id="ef2b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*SfjRw75aIIX-t4wv5tuowg.png" data-width="1124" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*SfjRw75aIIX-t4wv5tuowg.png"></figure><p name="125c" id="125c" class="graf graf--p graf-after--figure">then, by property #1 and property #3 of the residual,</p><figure name="3609" id="3609" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*inHF2PG5d1EBW-aelB_QRw.png" data-width="1124" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*inHF2PG5d1EBW-aelB_QRw.png"></figure><p name="4a29" id="4a29" class="graf graf--p graf-after--figure">thus,</p><figure name="b301" id="b301" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zWkAxtAyhvWsP7ALcguVNA.png" data-width="1124" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*zWkAxtAyhvWsP7ALcguVNA.png"></figure><p name="2590" id="2590" class="graf graf--p graf-after--figure">thus,</p><figure name="117c" id="117c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4vMZwAf4-3uWxGvucf29Yw.png" data-width="1124" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*4vMZwAf4-3uWxGvucf29Yw.png"></figure><p name="f5de" id="f5de" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) ANOVA F Testing for SLR</strong></p><p name="87fd" id="87fd" class="graf graf--p graf-after--p">Suppose we have an SLR model and we would like to test Ho: β1 = 0. By ANOVA, we can then conduct F testing. This is called an ANOVA F testing.</p><p name="1957" id="1957" class="graf graf--p graf-after--p">The reason why we can use F testing is that, when β1 = 0, we are able to draw the conclusion that,</p><figure name="3435" id="3435" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*7cdgo7IDCe3Bl5AyFAorUw.png" data-width="1124" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*7cdgo7IDCe3Bl5AyFAorUw.png"></figure><p name="9732" id="9732" class="graf graf--p graf-after--figure">this means that the residuals contribute all the variance and the independent variable can not explain anything of the variance.</p><p name="3190" id="3190" class="graf graf--p graf-after--p">However, when β1 ≠ 0, we are able to draw the conclusion that,</p><figure name="d69b" id="d69b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1Biu4dddow1xsla3FmM3xw.png" data-width="1124" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*1Biu4dddow1xsla3FmM3xw.png"></figure><p name="68d9" id="68d9" class="graf graf--p graf-after--figure">Thus, we are expecting,</p><figure name="1a44" id="1a44" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*OaPo6BhNNB26hZidKHxaHg.png" data-width="1124" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*OaPo6BhNNB26hZidKHxaHg.png"></figure><p name="2606" id="2606" class="graf graf--p graf-after--figure">for a given sample, both MSR and MSE are unbiased, thus,</p><figure name="3588" id="3588" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CS2MeqVy7yVTb-Ub5kaw_w.png" data-width="1124" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*CS2MeqVy7yVTb-Ub5kaw_w.png"></figure><p name="9331" id="9331" class="graf graf--p graf-after--figure">Because MSR and MSE are two variances with degrees of freedom 1 and n-2 respectively, thus, based on the definition of F distribution, we can know that the ratio of them follows,</p><figure name="e82e" id="e82e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gYbiwKXAY9pqMMAEO8gOdg.png" data-width="1124" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*gYbiwKXAY9pqMMAEO8gOdg.png"></figure><p name="09da" id="09da" class="graf graf--p graf-after--figure">Thus, we reject Ho if,</p><figure name="e05b" id="e05b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*A6sZd8_An-yS0L9Fn8gskQ.png" data-width="1124" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*A6sZd8_An-yS0L9Fn8gskQ.png"></figure><p name="5947" id="5947" class="graf graf--p graf-after--figure">or,</p><figure name="b39f" id="b39f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*GPVZmcbPxjPmi7gtavAvwA.png" data-width="1124" data-height="52" src="https://cdn-images-1.medium.com/max/800/1*GPVZmcbPxjPmi7gtavAvwA.png"></figure><p name="2ae7" id="2ae7" class="graf graf--p graf-after--figure">and then we can confirm that the parameter β1 is significant.</p><p name="59d3" id="59d3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) The Definition of the Goodness of Fit (aka. R-Square or R²)</strong></p><p name="f317" id="f317" class="graf graf--p graf-after--p">If we want to measure how much the variance is explained by our model, then we can define this with a ratio of SSR over SST. This is to say that,</p><figure name="c67a" id="c67a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*998UNzPW_vvJpknICcGsSg.png" data-width="1124" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*998UNzPW_vvJpknICcGsSg.png"></figure><p name="bb39" id="bb39" class="graf graf--p graf-after--figure">This is called the goodness of fit or R-Square.</p><p name="e273" id="e273" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Second Form of Goodness of Fit</strong></p><p name="fbce" id="fbce" class="graf graf--p graf-after--p">The goodness of fit can also be the square of the sample correlation between x and y,</p><figure name="8a68" id="8a68" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*eRJdNx-HHWT1RjyrWxOAGw.png" data-width="1124" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*eRJdNx-HHWT1RjyrWxOAGw.png"></figure><p name="9d8f" id="9d8f" class="graf graf--p graf-after--figure">We are not going to prove this here, but we can have a quick reference from the following <a href="https://stats.stackexchange.com/questions/99669/the-equivalence-of-sample-correlation-and-r-statistic-for-simple-linear-regressi" data-href="https://stats.stackexchange.com/questions/99669/the-equivalence-of-sample-correlation-and-r-statistic-for-simple-linear-regressi" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">link</a>.</p><p name="d0d4" id="d0d4" class="graf graf--p graf-after--p graf--trailing">The conclusion of this is that R² is a constant from 0 to 1 and we are able to say that, when R² is closer to 1, then this indicates a better fit of the model.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/e1d423dd09dc"><time class="dt-published" datetime="2020-10-18T21:26:07.562Z">October 18, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/linear-regression-2-properties-of-the-residuals-and-anova-e1d423dd09dc" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>