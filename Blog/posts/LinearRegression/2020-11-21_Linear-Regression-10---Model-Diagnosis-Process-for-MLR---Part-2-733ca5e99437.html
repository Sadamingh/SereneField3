<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Linear Regression 10 | Model Diagnosis Process for MLR — Part 2</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Linear Regression 10 | Model Diagnosis Process for MLR — Part 2</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Linear Regression
</section>
<section data-field="body" class="e-content">
<section name="f53c" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="eadf" id="eadf" class="graf graf--h3 graf--leading graf--title">Linear Regression 10 | Model Diagnosis Process for MLR — Part 2</h3><figure name="7c45" id="7c45" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*JRXYaS8AmzV2HWueCj2mRA.png" data-width="1514" data-height="716" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*JRXYaS8AmzV2HWueCj2mRA.png"></figure><ol class="postList"><li name="9044" id="9044" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Model Diagnosis Process for MLR</strong></li></ol><ul class="postList"><li name="0371" id="0371" class="graf graf--li graf-after--li">(0) Goal of Modeling</li><li name="df54" id="df54" class="graf graf--li graf-after--li">(1) Step 1. Check Multicollinearity</li><li name="8576" id="8576" class="graf graf--li graf-after--li">(2) Step 2. Fit the Initial Model</li><li name="5873" id="5873" class="graf graf--li graf-after--li">(3) Step 3. Check Influential Points</li><li name="c8ff" id="c8ff" class="graf graf--li graf-after--li">(4) Step 4. Check Heteroscedasticity</li></ul><p name="73a9" id="73a9" class="graf graf--p graf-after--li">You can find the topics above in part 1.</p><div name="cdb5" id="cdb5" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/adamedelwiess/linear-regression-9-model-diagnosis-process-for-mlr-part-1-205c1f71d581" data-href="https://medium.com/adamedelwiess/linear-regression-9-model-diagnosis-process-for-mlr-part-1-205c1f71d581" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/linear-regression-9-model-diagnosis-process-for-mlr-part-1-205c1f71d581"><strong class="markup--strong markup--mixtapeEmbed-strong">Linear Regression 9 | Model Diagnosis Process for MLR - Part 1</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Linear Regression</em>medium.com</a><a href="https://medium.com/adamedelwiess/linear-regression-9-model-diagnosis-process-for-mlr-part-1-205c1f71d581" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="2d17825b7b0e09bb7d23da6be7685041" data-thumbnail-img-id="1*JRXYaS8AmzV2HWueCj2mRA.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*JRXYaS8AmzV2HWueCj2mRA.png);"></a></div><p name="eb7d" id="eb7d" class="graf graf--p graf-after--mixtapeEmbed">Now let’s continue our discussion.</p><p name="07e5" id="07e5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Step 5. Check Normality</strong></p><ul class="postList"><li name="b9c0" id="b9c0" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Reason: </strong>We have made an assumption of normally distributed error terms. As we have said in the SLR part, the normally distributed error terms assumption is a “strong” assumption so that it won’t affect the BLUE feature of the OLSE. Normally, the normality assumption is presented but it is described as a <strong class="markup--strong markup--li-strong">less important</strong> assumption compared with the other assumptions.</li></ul><div name="f4f8" id="f4f8" class="graf graf--mixtapeEmbed graf-after--li"><a href="https://medium.com/adamedelwiess/linear-regression-4-slr-prediction-and-diagnosis-of-slr-model-8d905b5c5135" data-href="https://medium.com/adamedelwiess/linear-regression-4-slr-prediction-and-diagnosis-of-slr-model-8d905b5c5135" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/linear-regression-4-slr-prediction-and-diagnosis-of-slr-model-8d905b5c5135"><strong class="markup--strong markup--mixtapeEmbed-strong">Linear Regression 4 | SLR Prediction and Diagnosis of SLR Model</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Linear Regression</em>medium.com</a><a href="https://medium.com/adamedelwiess/linear-regression-4-slr-prediction-and-diagnosis-of-slr-model-8d905b5c5135" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="5a5c12a1cedf64693a5333d6dcf288c9" data-thumbnail-img-id="1*JRXYaS8AmzV2HWueCj2mRA.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*JRXYaS8AmzV2HWueCj2mRA.png);"></a></div><ul class="postList"><li name="587c" id="587c" class="graf graf--li graf-after--mixtapeEmbed"><strong class="markup--strong markup--li-strong">Results:</strong></li></ul><p name="4030" id="4030" class="graf graf--p graf-after--li">a. The OLSE is <strong class="markup--strong markup--p-strong">still</strong> the best linear unbiased estimate (BLUE).</p><p name="6959" id="6959" class="graf graf--p graf-after--p">b. Normality is needed to perform t-test, confidence interval and the ANOVA. So if there’s no normality, then the inference can no longer work (However, there’s actually an exception because of the central limit theorm).</p><p name="24c0" id="24c0" class="graf graf--p graf-after--p">c. Because the central limit theorm tells us that if the sample size is large enough, then it’s distribution will turns out to be a normal distribution. We can say that when the sample size is large (empirically &gt; 30), then the CLT guarantees that it’s approximately normally distributed. So that we can still use the perform t-test, confidence interval and the ANOVA.</p><ul class="postList"><li name="ccfd" id="ccfd" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Detection</strong></li></ul><p name="c899" id="c899" class="graf graf--p graf-after--li">a. <strong class="markup--strong markup--p-strong">Q-Q Plot</strong> is a way to examine normality. If a QQ plot results in approximate linear on diagonal of its plot, then it suggests the normality.</p><figure name="9634" id="9634" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*ueuDrg_YnhJuvBYj.png" data-width="1666" data-height="678" src="https://cdn-images-1.medium.com/max/800/0*ueuDrg_YnhJuvBYj.png"></figure><p name="9bd3" id="9bd3" class="graf graf--p graf-after--figure">b. The <strong class="markup--strong markup--p-strong">Skewness</strong> and the <strong class="markup--strong markup--p-strong">Kurtosis</strong> also indicates a normality of the residuals. It will be helpful if you refer to the “moment” part of the following article.</p><div name="b27b" id="b27b" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/adamedelwiess/probability-and-statistics-4-expectation-variance-moment-and-features-of-common-distributions-fb65980ceb23" data-href="https://medium.com/adamedelwiess/probability-and-statistics-4-expectation-variance-moment-and-features-of-common-distributions-fb65980ceb23" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/probability-and-statistics-4-expectation-variance-moment-and-features-of-common-distributions-fb65980ceb23"><strong class="markup--strong markup--mixtapeEmbed-strong">Probability and Statistics 4 | Expectation, Variance, Moment, and Features of Common Distributions</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Probability and Statistics</em>medium.com</a><a href="https://medium.com/adamedelwiess/probability-and-statistics-4-expectation-variance-moment-and-features-of-common-distributions-fb65980ceb23" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="071cf2fc252bb3bdff12dee20d984bbe" data-thumbnail-img-id="1*YTeSIjiYirbk4CZavQXCqg.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*YTeSIjiYirbk4CZavQXCqg.jpeg);"></a></div><figure name="a731" id="a731" class="graf graf--figure graf-after--mixtapeEmbed"><img class="graf-image" data-image-id="1*noB6ArlfLZowPSJQLZuKBA.png" data-width="1050" data-height="322" src="https://cdn-images-1.medium.com/max/800/1*noB6ArlfLZowPSJQLZuKBA.png"></figure><p name="7c0a" id="7c0a" class="graf graf--p graf-after--figure">And there’s also,</p><figure name="a864" id="a864" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*iSzcNvWzsAz91UELLq6JVQ.png" data-width="1076" data-height="334" src="https://cdn-images-1.medium.com/max/800/1*iSzcNvWzsAz91UELLq6JVQ.png"></figure><p name="b0ea" id="b0ea" class="graf graf--p graf-after--figure">When σ for the error term is unknown, we can estimate this value by the sample variance of the residuals. Thus,</p><figure name="a8ed" id="a8ed" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*t6VexgYL4MSM2u7CeiJe7g.png" data-width="1068" data-height="122" src="https://cdn-images-1.medium.com/max/800/1*t6VexgYL4MSM2u7CeiJe7g.png"></figure><p name="b402" id="b402" class="graf graf--p graf-after--figure">then,</p><figure name="a606" id="a606" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_akTcbjcL7KsCMR07vAkYg.png" data-width="1068" data-height="122" src="https://cdn-images-1.medium.com/max/800/1*_akTcbjcL7KsCMR07vAkYg.png"></figure><p name="4191" id="4191" class="graf graf--p graf-after--figure">Note that we have to detect both the skewness and the kurtosis for being zero in order to make a normality conclusion, or there will be a non-normality problem for the error terms.</p><p name="e756" id="e756" class="graf graf--p graf-after--p">However, it is always not accurate to use an approximate 0 as our evidence for normality. It’s a better idea if we can use a hypothesis inference to figure out our conclusion.</p><p name="daa2" id="daa2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">c. D’Agostino’s Omnibus K-Squared Normality Test</strong></p><p name="9409" id="9409" class="graf graf--p graf-after--p">Suppose we define that,</p><figure name="081e" id="081e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yvs8kts52ZnOGEOJrXeh5w.png" data-width="996" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*yvs8kts52ZnOGEOJrXeh5w.png"></figure><p name="9e4c" id="9e4c" class="graf graf--p graf-after--figure">Then the <strong class="markup--strong markup--p-strong">Omnibus K-Squared </strong>statistics is defined as,</p><figure name="b1eb" id="b1eb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*a6fT_NfSZX0C7Tt3hZRo3g.png" data-width="862" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*a6fT_NfSZX0C7Tt3hZRo3g.png"></figure><p name="88e1" id="88e1" class="graf graf--p graf-after--figure">Where Z1 and Z2 are two functions that we don’t have to know or understand them here, but <a href="https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test" data-href="https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this page</a> may be helpful if you want to know more about these two functions.</p><p name="901d" id="901d" class="graf graf--p graf-after--p">Because the omnibus K-Square statistics follows the Chi-Square distribution (we don’t have to know the reason), then we can construct a hypothesis testing of it.</p><figure name="e85f" id="e85f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gxmdkYiGqzOfJyBuUhcw4g.png" data-width="1030" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*gxmdkYiGqzOfJyBuUhcw4g.png"></figure><p name="59a1" id="59a1" class="graf graf--p graf-after--figure">Then, because</p><figure name="ac0a" id="ac0a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3gq1nzNB3vOh-P3wN0IaEA.png" data-width="806" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*3gq1nzNB3vOh-P3wN0IaEA.png"></figure><p name="a500" id="a500" class="graf graf--p graf-after--figure">Then we reject H0 if the p-value of this statistics is less than 0.05, this means that the normality of this model is not true.</p><p name="8f57" id="8f57" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">d. Jarque–Bera Test</strong></p><p name="c905" id="c905" class="graf graf--p graf-after--p">The Jarque-Bera test is another way to find out whether the error terms are normally distributed. It is quite similar to the D’Agostino’s Test. The null hypothesis and the alternative hypothesis of the JB test is,</p><figure name="d977" id="d977" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9W5BmpnBGAvyW0sizN6QiQ.png" data-width="996" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*9W5BmpnBGAvyW0sizN6QiQ.png"></figure><p name="0a8e" id="0a8e" class="graf graf--p graf-after--figure">Then the Jarque-Bera Statistics is defined as,</p><figure name="f44c" id="f44c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PrIO-KwY78pPBvetJaaNQg.png" data-width="884" data-height="78" src="https://cdn-images-1.medium.com/max/800/1*PrIO-KwY78pPBvetJaaNQg.png"></figure><p name="0c26" id="0c26" class="graf graf--p graf-after--figure">Then, because,</p><figure name="6ee5" id="6ee5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*iDOT2ZkT1yUG6c4AF8YJ9w.png" data-width="884" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*iDOT2ZkT1yUG6c4AF8YJ9w.png"></figure><p name="8041" id="8041" class="graf graf--p graf-after--figure">Then we reject H0 if the p-value of this statistics is less than 0.05, this means that the normality of this model is not true.</p><p name="a210" id="a210" class="graf graf--p graf-after--p">Note that the Jarque-Bera test is very sensitive to the outliers.</p><ul class="postList"><li name="e412" id="e412" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Solutions</strong></li></ul><p name="7c49" id="7c49" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">a. Natural-Log Transformation on y</strong></p><p name="c13a" id="c13a" class="graf graf--p graf-after--p">To deal with the non-normality problem, first of all, we can try the natural-log transformation out,</p><figure name="589e" id="589e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*LitOCABrhkpyOOHhFytTHw.png" data-width="770" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*LitOCABrhkpyOOHhFytTHw.png"></figure><p name="6599" id="6599" class="graf graf--p graf-after--figure">We can use this method because the natural-log transformation can be used to make highly positively skewed distributions less skewed.</p><figure name="cf39" id="cf39" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/334c29b9ed16f70d488079df911653ad.js"></script></figure><figure name="084e" id="084e" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*OuyAJOcnd49SkLix0hwKCw.png" data-width="1614" data-height="380" src="https://cdn-images-1.medium.com/max/800/1*OuyAJOcnd49SkLix0hwKCw.png"></figure><p name="63ad" id="63ad" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">b. Box-Cox Transformation on y</strong></p><p name="b274" id="b274" class="graf graf--p graf-after--p">Based on the log transformation, we can know that the log transformation works pretty well for the positively skewed data, while we have to make some change to the negatively skewed data (in the case above, we have to use 80-i). This is not convenient for us.</p><p name="9e18" id="9e18" class="graf graf--p graf-after--p">In fact, we want to construct a transformation that can work for both the positively skewed data and the negatively skewed data, then the answer of this kind of transformation is the box-cox transformation. It is defined as follows,</p><figure name="b37a" id="b37a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xVtTH_zlnyPHppH7lC9O-A.png" data-width="770" data-height="86" src="https://cdn-images-1.medium.com/max/800/1*xVtTH_zlnyPHppH7lC9O-A.png"></figure><p name="5dd0" id="5dd0" class="graf graf--p graf-after--figure">Why we can have this formula? Let’s think about what it will be like when λ is limited to zero. By L’Hôpital’s rule,</p><figure name="0d42" id="0d42" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yd_ee9bLnQvdN15Qld9I2w.png" data-width="942" data-height="86" src="https://cdn-images-1.medium.com/max/800/1*yd_ee9bLnQvdN15Qld9I2w.png"></figure><p name="024d" id="024d" class="graf graf--p graf-after--figure">So when λ is close to zero, then the box-cox transformation will be approximatly equal to the natural-log transformation.</p><figure name="54b2" id="54b2" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/16ae8d2730eeda21ec33726a486dfdda.js"></script></figure><figure name="eb83" id="eb83" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*bSZhUZBJ-y5cQ-yqCegOdA.png" data-width="1348" data-height="526" src="https://cdn-images-1.medium.com/max/800/1*bSZhUZBJ-y5cQ-yqCegOdA.png"></figure><p name="3f40" id="3f40" class="graf graf--p graf-after--figure">When λ is negative or positive, we can have two different forms of this transformation for the negatively skewed data and the positively skewed data, respectively,</p><figure name="c659" id="c659" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/ad126d485a2119ead8c6bcd48a165058.js"></script></figure><figure name="59da" id="59da" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*HcdFJjtWRPo4GXkpnZ5isA.png" data-width="996" data-height="310" src="https://cdn-images-1.medium.com/max/800/1*HcdFJjtWRPo4GXkpnZ5isA.png"></figure><p name="1622" id="1622" class="graf graf--p graf-after--figure">Then we redo the Box-Cox transformation for our case above and we can find out it also works for our data,</p><figure name="ccc5" id="ccc5" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/20c00a45c44758da1aee6ee7ee7301b8.js"></script></figure><figure name="ecc7" id="ecc7" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*cba6Oi7d2IGkZ5vTmkCZoQ.png" data-width="1632" data-height="378" src="https://cdn-images-1.medium.com/max/800/1*cba6Oi7d2IGkZ5vTmkCZoQ.png"></figure><p name="e6eb" id="e6eb" class="graf graf--p graf-after--figure">Okay, now here’s another question. How can we choose a proper λ for the Box-Cox transformation? Practically, we don’t have to calculate the proper lambda on our own, instead, we can call the stats.boxcox function in python to calculate a proper lambda for us.</p><pre name="87ba" id="87ba" class="graf graf--pre graf-after--p">from scipy import stats</pre><pre name="d0a8" id="d0a8" class="graf graf--pre graf-after--pre">y = [10,12,17,25,30,37,40,42,47,64,87,109,140,181,253,319,480,564]</pre><pre name="8b1b" id="8b1b" class="graf graf--pre graf-after--pre">fitted_data, fitted_lambda = stats.boxcox(y)<br>print(&#39;The fitted lambda is:&#39;, fitted_lambda)</pre><p name="ddda" id="ddda" class="graf graf--p graf-after--pre">The output is,</p><pre name="6392" id="6392" class="graf graf--pre graf-after--p">The fitted lambda is: -0.08379804288959128</pre><p name="2966" id="2966" class="graf graf--p graf-after--pre">This is quite close to what we have chosen (-0.1) in the previous case.</p><p name="50cb" id="50cb" class="graf graf--p graf-after--p">c. <strong class="markup--strong markup--p-strong">Collect more data!</strong> Because when the sample size is large, then the skewness of the data won’t impact the model estimation or inference too much.</p><p name="5c53" id="5c53" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Step 6. Check Linearity</strong></p><ul class="postList"><li name="e62d" id="e62d" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Reason</strong>: When there is a non-linearity problem between the dependent variable and the independent variables, there can be two reasons. First, there may be some<strong class="markup--strong markup--li-strong"> non-linear terms</strong> (i.e. squared terms). Second, there may be some <strong class="markup--strong markup--li-strong">interaction terms</strong>. Note that although there are many ways to adjust a non-linear model, we have to be extremely careful.</li><li name="834a" id="834a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Solution for Non-linear Terms</strong></li></ul><p name="035d" id="035d" class="graf graf--p graf-after--li">a. <strong class="markup--strong markup--p-strong">Nonlinear Approach</strong>: This is the most complicated way. By adding polynomial terms in MLR, it adds the number of the parameters and it can also adds the probability of multicollinearity. The cost for this approach is that all the testing methods can become unstable and invalid. Also, there will be no analytical expressions for the MLR estimation.</p><p name="8817" id="8817" class="graf graf--p graf-after--p">b. <strong class="markup--strong markup--p-strong">Log-log Transformation</strong>: it is a powerful tool because it will not lose any inference.</p><p name="2c15" id="2c15" class="graf graf--p graf-after--p">c. <strong class="markup--strong markup--p-strong">Transformation on x</strong></p><ul class="postList"><li name="83ad" id="83ad" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Solution for Interaction Terms</strong></li></ul><p name="7f3d" id="7f3d" class="graf graf--p graf-after--li">In practice, we only add interaction terms if we obtain the knowledge in advance that a significant interaction may exist in the study.</p><p name="7587" id="7587" class="graf graf--p graf-after--p">We will continue our discussion in part 3.</p><ul class="postList"><li name="2cf1" id="2cf1" class="graf graf--li graf-after--p">(7) Step 7. Modify the initial model and fit the data again</li><li name="be9a" id="be9a" class="graf graf--li graf-after--li">(8) Step 8. Best Subset Model Selection: based on adjusted R²</li><li name="791c" id="791c" class="graf graf--li graf-after--li">(9) Step 9. Step-wise Model Selection: based on t-test</li><li name="5298" id="5298" class="graf graf--li graf-after--li graf--trailing">(10) Step 10. AIC/BIC Method</li></ul></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/733ca5e99437"><time class="dt-published" datetime="2020-11-21T18:14:44.480Z">November 21, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/linear-regression-10-model-diagnosis-process-for-mlr-part-2-733ca5e99437" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>