<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Linear Regression 5 | MLR, Hat Matrix, and MLR-OLS Evaluation</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Linear Regression 5 | MLR, Hat Matrix, and MLR-OLS Evaluation</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Linear Regression
</section>
<section data-field="body" class="e-content">
<section name="57fb" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="e842" id="e842" class="graf graf--h3 graf--leading graf--title">Linear Regression 5 | MLR, Hat Matrix, and MLR-<strong class="markup--strong markup--h3-strong">OLS Evaluation</strong></h3><figure name="1057" id="1057" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*JRXYaS8AmzV2HWueCj2mRA.png" data-width="1514" data-height="716" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*JRXYaS8AmzV2HWueCj2mRA.png"></figure><ol class="postList"><li name="6d80" id="6d80" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Multiple Linear Regression</strong></li></ol><p name="7bdc" id="7bdc" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Recall: Simple Linear Regression Model</strong></p><p name="a319" id="a319" class="graf graf--p graf-after--p">The form of the simple linear regression for a given sample of two variables x and y (or a dataset of two variables) is,</p><figure name="1a72" id="1a72" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*-7C4KNidv1Kx7S_1.png" data-width="1042" data-height="72" src="https://cdn-images-1.medium.com/max/800/0*-7C4KNidv1Kx7S_1.png"></figure><p name="afc2" id="afc2" class="graf graf--p graf-after--figure">What if we have more than two variables for a linear model?</p><p name="f4ba" id="f4ba" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Definition of Multiple Linear Regression</strong></p><p name="ac51" id="ac51" class="graf graf--p graf-after--p">Suppose we have p variables, and x1 to xp-1 are our independent variables and y is our dependent variable, then, the formula can be written as,</p><figure name="e217" id="e217" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KAHdde9UBDzQShc6UbzzBA.png" data-width="1036" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*KAHdde9UBDzQShc6UbzzBA.png"></figure><p name="d5b5" id="d5b5" class="graf graf--p graf-after--figure">this is also,</p><figure name="00a0" id="00a0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HcuBo1IiozqazT8B-5M7JA.png" data-width="1036" data-height="240" src="https://cdn-images-1.medium.com/max/800/1*HcuBo1IiozqazT8B-5M7JA.png"></figure><p name="6c00" id="6c00" class="graf graf--p graf-after--figure">Because this is a linear equation system, we can also rewrite this as the form of matrix production.</p><figure name="c8dd" id="c8dd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1oLHPgPno-R1C9upLduHWw.png" data-width="1198" data-height="178" src="https://cdn-images-1.medium.com/max/800/1*1oLHPgPno-R1C9upLduHWw.png"></figure><p name="c181" id="c181" class="graf graf--p graf-after--figure">And,</p><figure name="401a" id="401a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*iBh3-9EI_Bj3qb-6dUagVQ.png" data-width="1036" data-height="226" src="https://cdn-images-1.medium.com/max/800/1*iBh3-9EI_Bj3qb-6dUagVQ.png"></figure><p name="fdbb" id="fdbb" class="graf graf--p graf-after--figure">thus, we have,</p><figure name="1036" id="1036" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qKvglm-BC5MQjZJxhcHTIA.png" data-width="1036" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*qKvglm-BC5MQjZJxhcHTIA.png"></figure><p name="df5e" id="df5e" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Strong Assumption of Multiple Linear Regression</strong></p><p name="1d51" id="1d51" class="graf graf--p graf-after--p">Similarly to the simple linear regression, we are able to say, for the real model,</p><figure name="af88" id="af88" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*L3DBfPSLdc-XMjnFzznNqA.png" data-width="1036" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*L3DBfPSLdc-XMjnFzznNqA.png"></figure><p name="349e" id="349e" class="graf graf--p graf-after--figure">the error term should follow,</p><figure name="d2ef" id="d2ef" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*cC6PxjKlob807Lk1KHhebA.png" data-width="1036" data-height="194" src="https://cdn-images-1.medium.com/max/800/1*cC6PxjKlob807Lk1KHhebA.png"></figure><p name="91cb" id="91cb" class="graf graf--p graf-after--figure">Suppose we denote multivariate normal distribution as MN, then the strong assumption can be written as,</p><figure name="a107" id="a107" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kXLnW1Tr7u_8h0z5AtEL6A.png" data-width="1036" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*kXLnW1Tr7u_8h0z5AtEL6A.png"></figure><p name="72c4" id="72c4" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) OLS: Sum of Squared Residuals</strong></p><p name="1aed" id="1aed" class="graf graf--p graf-after--p">Recall in the SLR, we have defined that,</p><figure name="b503" id="b503" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nUkJudesCH5Yz1TPulfs1A.png" data-width="1036" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*nUkJudesCH5Yz1TPulfs1A.png"></figure><p name="b686" id="b686" class="graf graf--p graf-after--figure">Similarly, we define that,</p><figure name="f037" id="f037" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vagLQ3xAuITtIbZVD2nhew.png" data-width="1036" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*vagLQ3xAuITtIbZVD2nhew.png"></figure><p name="70be" id="70be" class="graf graf--p graf-after--figure">then, to solve OLS estimators, our goal is to optimize the following problem,</p><figure name="7e5c" id="7e5c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*l933Xk25Q5PSnEtjbuImFw.png" data-width="1036" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*l933Xk25Q5PSnEtjbuImFw.png"></figure><p name="5525" id="5525" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Projection Matrix and OLS for Multiple Linear Regression</strong></p><p name="fc56" id="fc56" class="graf graf--p graf-after--p">Recall what we have talked about for the projection matrix,</p><div name="3fca" id="3fca" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/adamedelwiess/linear-algebra-6-full-rank-projection-matrix-and-orthogonal-matrix-6e6bf1074c35" data-href="https://medium.com/adamedelwiess/linear-algebra-6-full-rank-projection-matrix-and-orthogonal-matrix-6e6bf1074c35" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/linear-algebra-6-full-rank-projection-matrix-and-orthogonal-matrix-6e6bf1074c35"><strong class="markup--strong markup--mixtapeEmbed-strong">Linear Algebra 6 | Full Rank, Projection Matrix, And Orthogonal Matrix</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Linear Algebr</em>medium.com</a><a href="https://medium.com/adamedelwiess/linear-algebra-6-full-rank-projection-matrix-and-orthogonal-matrix-6e6bf1074c35" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8d10da21c00196270ac7532b8eba585e" data-thumbnail-img-id="1*Yj4DEKztkbPyDrnciYX9dQ.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*Yj4DEKztkbPyDrnciYX9dQ.jpeg);"></a></div><p name="b541" id="b541" class="graf graf--p graf-after--mixtapeEmbed">Now let’s define the projection of vector <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">y</em></strong> onto the column space of matrix <strong class="markup--strong markup--p-strong">X</strong> as,</p><figure name="f76d" id="f76d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gp-EYRJk4kdTbkBBZJQqjw.png" data-width="1036" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*gp-EYRJk4kdTbkBBZJQqjw.png"></figure><p name="67db" id="67db" class="graf graf--p graf-after--figure">Based on the fact of the matrix projection,</p><figure name="4e62" id="4e62" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*N0SuqdWyWmiR1Qq3u-FeCg.png" data-width="1036" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*N0SuqdWyWmiR1Qq3u-FeCg.png"></figure><p name="5c1b" id="5c1b" class="graf graf--p graf-after--figure">then,</p><figure name="99df" id="99df" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3sBS6hgJfnfGFXwucW442w.png" data-width="1036" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*3sBS6hgJfnfGFXwucW442w.png"></figure><p name="877a" id="877a" class="graf graf--p graf-after--figure">This is equivalent to,</p><figure name="d6aa" id="d6aa" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xSPsBohaM0VVXIXFtyWNNw.png" data-width="1036" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*xSPsBohaM0VVXIXFtyWNNw.png"></figure><p name="1bc5" id="1bc5" class="graf graf--p graf-after--figure">Then, the optimize problem is equivalent to solve when,</p><figure name="7202" id="7202" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kL1CGWgYqMqei23OWILYlA.png" data-width="1036" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*kL1CGWgYqMqei23OWILYlA.png"></figure><p name="5c22" id="5c22" class="graf graf--p graf-after--figure">this is to say that,</p><figure name="6853" id="6853" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*sljpkTT49KignhiATugTZA.png" data-width="1036" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*sljpkTT49KignhiATugTZA.png"></figure><p name="4460" id="4460" class="graf graf--p graf-after--figure">then,</p><figure name="8fdc" id="8fdc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*F2wmAz0Vq_5aGYmkAX0VFg.png" data-width="1036" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*F2wmAz0Vq_5aGYmkAX0VFg.png"></figure><p name="61b0" id="61b0" class="graf graf--p graf-after--figure">then,</p><figure name="404a" id="404a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4zm3FXs2bhtbWfA3ns8bvg.png" data-width="1036" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*4zm3FXs2bhtbWfA3ns8bvg.png"></figure><p name="a26c" id="a26c" class="graf graf--p graf-after--figure">then,</p><figure name="af40" id="af40" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rrv8CQUl-M9bW8ZYD8pGVw.png" data-width="1036" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*rrv8CQUl-M9bW8ZYD8pGVw.png"></figure><p name="a260" id="a260" class="graf graf--p graf-after--figure">We can also solve this result by matrix differentiation, but because we didn’t introduce this in our sections, it is not the most preferred proof in this case.</p><p name="fe36" id="fe36" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Hat Matrix</strong></p><p name="341a" id="341a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Hat Matrix for MLR</strong></p><p name="91b3" id="91b3" class="graf graf--p graf-after--p">The hat matrix in regression is just another name for the projection matrix. For a given model with independent variables and a dependent variable, the hat matrix is the projection matrix to project vector <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">y</em></strong> onto the column space of X.</p><figure name="125e" id="125e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*iUTV15ESUBfV1mMeqnBZmw.png" data-width="1108" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*iUTV15ESUBfV1mMeqnBZmw.png"></figure><p name="45ef" id="45ef" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Properties of the Hat Matrix</strong></p><p name="36f9" id="36f9" class="graf graf--p graf-after--p">Because the hat matrix is a projection matrix, so it has all the features of the projection matrix,</p><ul class="postList"><li name="14ee" id="14ee" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Property #1: Symmetric</strong></li></ul><p name="2228" id="2228" class="graf graf--p graf-after--li">Because the hat matrix is a specific kind of projection matrix, then it should be a symmetric matrix.</p><p name="801f" id="801f" class="graf graf--p graf-after--p">Proof:</p><p name="fc05" id="fc05" class="graf graf--p graf-after--p">For any square and invertable matrix A, the inverse and transpose operator commute,</p><figure name="7a68" id="7a68" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vT8WSaLolxJ4lhAy0ytyPA.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*vT8WSaLolxJ4lhAy0ytyPA.png"></figure><p name="bf6b" id="bf6b" class="graf graf--p graf-after--figure">Then, we have,</p><figure name="1f9a" id="1f9a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5QmB8JavEfC3O_NCY8Yrhw.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*5QmB8JavEfC3O_NCY8Yrhw.png"></figure><p name="7919" id="7919" class="graf graf--p graf-after--figure">So, X^TX is a symmetric matrix. Then,</p><figure name="e282" id="e282" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*H-nuAcBoTc2cQRurvtVr4A.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*H-nuAcBoTc2cQRurvtVr4A.png"></figure><p name="c5ef" id="c5ef" class="graf graf--p graf-after--figure">then,</p><figure name="2e34" id="2e34" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Vt8PnuaByVK138c51tDIiQ.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*Vt8PnuaByVK138c51tDIiQ.png"></figure><p name="4247" id="4247" class="graf graf--p graf-after--figure">then,</p><figure name="d5c6" id="d5c6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jhwl3pNm3NrThFGGKHzK0Q.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*jhwl3pNm3NrThFGGKHzK0Q.png"></figure><p name="3fb6" id="3fb6" class="graf graf--p graf-after--figure">then,</p><figure name="9f7c" id="9f7c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*sbnRN6xKtH72ugrS14IEDA.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*sbnRN6xKtH72ugrS14IEDA.png"></figure><ul class="postList"><li name="a3e1" id="a3e1" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Property #2: Idempotent</strong></li></ul><p name="145d" id="145d" class="graf graf--p graf-after--li">Because the definition of a project matrix is to project a vector onto the column space of another matrix, then it will be idempotent.</p><figure name="ecf2" id="ecf2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CSGNLvki_CRrcKu4Qmo9oQ.png" data-width="1108" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*CSGNLvki_CRrcKu4Qmo9oQ.png"></figure><p name="152c" id="152c" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Fitted Value</strong></p><p name="efbc" id="efbc" class="graf graf--p graf-after--p">Based on our conclusion of the OLS estimator, we can then have the fitted value as,</p><figure name="07ec" id="07ec" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*04Rg956Bz-aaLz2WcXFMoA.png" data-width="1108" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*04Rg956Bz-aaLz2WcXFMoA.png"></figure><p name="06a1" id="06a1" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) The Residual of the OLS</strong></p><p name="2bea" id="2bea" class="graf graf--p graf-after--p">The difference of the observed values and the fitted values is called the residual,</p><figure name="758c" id="758c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pYiUv7On2LgMv_v4tMpuLQ.png" data-width="1108" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*pYiUv7On2LgMv_v4tMpuLQ.png"></figure><p name="edd4" id="edd4" class="graf graf--p graf-after--figure">Proof:</p><p name="99c5" id="99c5" class="graf graf--p graf-after--p">By definition,</p><figure name="a5e1" id="a5e1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gMFGpJH1sKrb9mb0C8Q1OQ.png" data-width="1108" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*gMFGpJH1sKrb9mb0C8Q1OQ.png"></figure><p name="a6a0" id="a6a0" class="graf graf--p graf-after--figure">then, based on the result of the fitted value for MLR,</p><figure name="abd5" id="abd5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*F4ATkb9S9LdfJThAVVUzCg.png" data-width="1108" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*F4ATkb9S9LdfJThAVVUzCg.png"></figure><p name="3418" id="3418" class="graf graf--p graf-after--figure">Suppose we define that,</p><figure name="ff5d" id="ff5d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zR9GlhJmTo6LZbBx8Z6TeA.png" data-width="1108" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*zR9GlhJmTo6LZbBx8Z6TeA.png"></figure><p name="3474" id="3474" class="graf graf--p graf-after--figure">then,</p><figure name="5431" id="5431" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*25LSSy7e_O7iYeITId_fxA.png" data-width="1018" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*25LSSy7e_O7iYeITId_fxA.png"></figure><p name="0bf2" id="0bf2" class="graf graf--p graf-after--figure">Note that because H-bar matrix is derived from the hat matrix, so it has some of the properties that the hat matrix has. For example,</p><ul class="postList"><li name="9213" id="9213" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Property #1: Symmetric</strong></li><li name="48ed" id="48ed" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Property #2: Idempotent</strong></li></ul><p name="20c2" id="20c2" class="graf graf--p graf-after--li">For matrix H-bar, we also have,</p><figure name="7005" id="7005" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-WpjmbBSovtOCgwY6qAfZw.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*-WpjmbBSovtOCgwY6qAfZw.png"></figure><p name="2423" id="2423" class="graf graf--p graf-after--figure">Proof:</p><p name="276f" id="276f" class="graf graf--p graf-after--p">By definition,</p><figure name="9420" id="9420" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*m6pOr7UB4gpJJOrtte9f6Q.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*m6pOr7UB4gpJJOrtte9f6Q.png"></figure><p name="f740" id="f740" class="graf graf--p graf-after--figure">then,</p><figure name="302c" id="302c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*96DubE7GvSymrDvLWpC52g.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*96DubE7GvSymrDvLWpC52g.png"></figure><p name="8753" id="8753" class="graf graf--p graf-after--figure">By the fact that,</p><figure name="70a4" id="70a4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rNtcx2Z3DPVoGuBWIsM4FQ.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*rNtcx2Z3DPVoGuBWIsM4FQ.png"></figure><p name="059d" id="059d" class="graf graf--p graf-after--figure">then,</p><figure name="5777" id="5777" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ckD_lPDEOIWmWdHgLTLUlg.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*ckD_lPDEOIWmWdHgLTLUlg.png"></figure><p name="c56b" id="c56b" class="graf graf--p graf-after--figure">Note that this H-bar matrix is one of the most important matrix for MLR.</p><p name="ec93" id="ec93" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. OLS Evaluation</strong></p><p name="8277" id="8277" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Evaluation on OLS for Multiple Linear Regression</strong></p><p name="738d" id="738d" class="graf graf--p graf-after--p">Similarly, the OLS estimator for MLR is also the <strong class="markup--strong markup--p-strong">best linear unbiased estimatior</strong> (BLUE).</p><p name="ca58" id="ca58" class="graf graf--p graf-after--p">Proof:</p><p name="4b29" id="4b29" class="graf graf--p graf-after--p">For linearity, we can write,</p><figure name="c070" id="c070" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rrv8CQUl-M9bW8ZYD8pGVw.png" data-width="1036" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*rrv8CQUl-M9bW8ZYD8pGVw.png"></figure><p name="cb25" id="cb25" class="graf graf--p graf-after--figure">this shows a linear relationship between the OLS estimator and the vector <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">y</em></strong>.</p><p name="aa56" id="aa56" class="graf graf--p graf-after--p">For the bias of the OLS estimator, by its definition,</p><figure name="859f" id="859f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xmaxU2l1VQe7ugvGTkYhug.png" data-width="918" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*xmaxU2l1VQe7ugvGTkYhug.png"></figure><p name="3016" id="3016" class="graf graf--p graf-after--figure">then,</p><figure name="1cc5" id="1cc5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*B0dclKQKBIJMujrTKNr3Zg.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*B0dclKQKBIJMujrTKNr3Zg.png"></figure><p name="8c66" id="8c66" class="graf graf--p graf-after--figure">Because,</p><figure name="2b68" id="2b68" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*J4gEhxtM2NyWoKwPZnn03Q.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*J4gEhxtM2NyWoKwPZnn03Q.png"></figure><p name="b43c" id="b43c" class="graf graf--p graf-after--figure">then,</p><figure name="cb2d" id="cb2d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UfliWJGWm3RskQgE56fFpQ.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*UfliWJGWm3RskQgE56fFpQ.png"></figure><p name="fc2d" id="fc2d" class="graf graf--p graf-after--figure">then,</p><figure name="7485" id="7485" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lBIx9718jVHerFHqnxTD3g.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*lBIx9718jVHerFHqnxTD3g.png"></figure><p name="a183" id="a183" class="graf graf--p graf-after--figure">then,</p><figure name="b9ed" id="b9ed" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DxBSHd4b_JsXgyduU8mOjw.png" data-width="918" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*DxBSHd4b_JsXgyduU8mOjw.png"></figure><p name="9763" id="9763" class="graf graf--p graf-after--figure">then,</p><figure name="3b66" id="3b66" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*B5zPGx7PqrEQdUegSZIOmg.png" data-width="932" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*B5zPGx7PqrEQdUegSZIOmg.png"></figure><p name="c231" id="c231" class="graf graf--p graf-after--figure">then,</p><figure name="255f" id="255f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9Wy2qGEHsrnQbTNViJ2pkA.png" data-width="932" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*9Wy2qGEHsrnQbTNViJ2pkA.png"></figure><p name="9aa4" id="9aa4" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Expectation of the OLS Estimators</strong></p><p name="2694" id="2694" class="graf graf--p graf-after--p">Based on the fact that the OLS estimator is unbiased, then we can have,</p><figure name="2d44" id="2d44" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ohknEs87d5dlcyjhke7gYg.png" data-width="932" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*ohknEs87d5dlcyjhke7gYg.png"></figure><p name="d6de" id="d6de" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) The Definition of the Variance-Covariance Matrix</strong></p><p name="4a05" id="4a05" class="graf graf--p graf-after--p">To calculate a variance of a vector of random variables, we are going to have a variance-covariance matrix. Suppose we have a vector of random variables x1, x2, …, xn,</p><figure name="695f" id="695f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*c_XPSVVaqGRkvFya7lm1DQ.png" data-width="932" data-height="166" src="https://cdn-images-1.medium.com/max/800/1*c_XPSVVaqGRkvFya7lm1DQ.png"></figure><p name="390c" id="390c" class="graf graf--p graf-after--figure">then, the variance of this vector is defined by,</p><figure name="45eb" id="45eb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Hn_YMn4J4ZJNWmJ_Q5N-ow.png" data-width="1172" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*Hn_YMn4J4ZJNWmJ_Q5N-ow.png"></figure><p name="e254" id="e254" class="graf graf--p graf-after--figure">then,</p><figure name="e95c" id="e95c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rRVd7V8DuhJDhY-g7ZSWmA.png" data-width="1520" data-height="188" src="https://cdn-images-1.medium.com/max/800/1*rRVd7V8DuhJDhY-g7ZSWmA.png"></figure><p name="c317" id="c317" class="graf graf--p graf-after--figure">this is also,</p><figure name="3fd4" id="3fd4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mfBUVbR1Hx5Uo-15ENt1Wg.png" data-width="1654" data-height="188" src="https://cdn-images-1.medium.com/max/800/1*mfBUVbR1Hx5Uo-15ENt1Wg.png"></figure><p name="9559" id="9559" class="graf graf--p graf-after--figure">and this is also,</p><figure name="280f" id="280f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*cq9BjAXnjRPa6xOo4f0jvw.png" data-width="1134" data-height="188" src="https://cdn-images-1.medium.com/max/800/1*cq9BjAXnjRPa6xOo4f0jvw.png"></figure><p name="4dba" id="4dba" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Variance-Covariance Matrix of the OLS Estimators</strong></p><p name="7a83" id="7a83" class="graf graf--p graf-after--p">In the sections of SLR, when we calculate the variance of an estimator, we are then going to have a single value of the variance. But, however, because the OLS estimator for MLR is a vector, then to calculate its variance, we are going to have a variance-covariance matrix.</p><figure name="0869" id="0869" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*A9xwzSFeHw1VncEA77PU3A.png" data-width="984" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*A9xwzSFeHw1VncEA77PU3A.png"></figure><p name="ddb1" id="ddb1" class="graf graf--p graf-after--figure">Proof:</p><p name="552f" id="552f" class="graf graf--p graf-after--p">By the definition of the variance-covariance matrix, we are then going to have,</p><figure name="0b84" id="0b84" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*03lmpEeyua9V-QzbECV1Tg.png" data-width="1172" data-height="88" src="https://cdn-images-1.medium.com/max/800/1*03lmpEeyua9V-QzbECV1Tg.png"></figure><p name="cabe" id="cabe" class="graf graf--p graf-after--figure">By the expectation of the OLS estimator,</p><figure name="3018" id="3018" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UgLoEBKOagBwVCKbV606Vg.png" data-width="1172" data-height="88" src="https://cdn-images-1.medium.com/max/800/1*UgLoEBKOagBwVCKbV606Vg.png"></figure><p name="f027" id="f027" class="graf graf--p graf-after--figure">then, by the bias (estimated error) of the OLS estimator,</p><figure name="4215" id="4215" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*JUntQ48FKKlY7zEhjynzWQ.png" data-width="1172" data-height="82" src="https://cdn-images-1.medium.com/max/800/1*JUntQ48FKKlY7zEhjynzWQ.png"></figure><p name="7e41" id="7e41" class="graf graf--p graf-after--figure">then,</p><figure name="1a17" id="1a17" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1CfjI1w1Va0a5hRjtE8tbQ.png" data-width="984" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*1CfjI1w1Va0a5hRjtE8tbQ.png"></figure><p name="ff29" id="ff29" class="graf graf--p graf-after--figure">then, because <strong class="markup--strong markup--p-strong">X</strong> is fixed in this case,</p><figure name="c557" id="c557" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jZ9cBgkr_lAHzyRv2kb1rw.png" data-width="984" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*jZ9cBgkr_lAHzyRv2kb1rw.png"></figure><p name="9f77" id="9f77" class="graf graf--p graf-after--figure">by definition of the variance-covariance matrix,</p><figure name="3252" id="3252" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*oNnj3i3CK7FBm6iuyBANIQ.png" data-width="984" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*oNnj3i3CK7FBm6iuyBANIQ.png"></figure><p name="a5be" id="a5be" class="graf graf--p graf-after--figure">By our assumption,</p><figure name="296c" id="296c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kXLnW1Tr7u_8h0z5AtEL6A.png" data-width="1036" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*kXLnW1Tr7u_8h0z5AtEL6A.png"></figure><p name="c49a" id="c49a" class="graf graf--p graf-after--figure">then,</p><figure name="4140" id="4140" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*asckyuIfqDoGXpLYsLpugw.png" data-width="984" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*asckyuIfqDoGXpLYsLpugw.png"></figure><p name="9629" id="9629" class="graf graf--p graf-after--figure">then,</p><figure name="6fec" id="6fec" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vEd5jFLIJ6uy37Y8YSJAqA.png" data-width="984" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*vEd5jFLIJ6uy37Y8YSJAqA.png"></figure><p name="1ba5" id="1ba5" class="graf graf--p graf-after--figure">then,</p><figure name="8276" id="8276" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xGou9nU0fr_-EGnNeWfrkQ.png" data-width="984" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*xGou9nU0fr_-EGnNeWfrkQ.png"></figure><p name="ba4b" id="ba4b" class="graf graf--p graf-after--figure">then,</p><figure name="d87a" id="d87a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*A9xwzSFeHw1VncEA77PU3A.png" data-width="984" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*A9xwzSFeHw1VncEA77PU3A.png"></figure><p name="220d" id="220d" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Expectation of the Fitted Value</strong></p><p name="7e63" id="7e63" class="graf graf--p graf-after--p">Because <strong class="markup--strong markup--p-strong">X</strong> is fixed in this case, then, by the unbiasness of the OLS estimator,</p><figure name="1aa6" id="1aa6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jsJRbdN1i73r7hfxgFhtCw.png" data-width="984" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*jsJRbdN1i73r7hfxgFhtCw.png"></figure><p name="5d28" id="5d28" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(6) Variance of the Fitted Value</strong></p><p name="3ed6" id="3ed6" class="graf graf--p graf-after--p">The variance-covariance matrix of the fitted value is,</p><figure name="5542" id="5542" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*k_q8zh4l023-ufPxiFafEA.png" data-width="984" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*k_q8zh4l023-ufPxiFafEA.png"></figure><p name="f83e" id="f83e" class="graf graf--p graf-after--figure">Proof:</p><figure name="81de" id="81de" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MQ0LCwEcMf1xtoEyc3krIQ.png" data-width="984" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*MQ0LCwEcMf1xtoEyc3krIQ.png"></figure><p name="7fad" id="7fad" class="graf graf--p graf-after--figure">then,</p><figure name="e4a9" id="e4a9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ezhssXZjaGh5-ZyaDVuQEw.png" data-width="984" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*ezhssXZjaGh5-ZyaDVuQEw.png"></figure><p name="d45e" id="d45e" class="graf graf--p graf-after--figure">by the variance of the OLS estimator,</p><figure name="8136" id="8136" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VdRqO-UjAq3nyHy0hlDdxA.png" data-width="984" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*VdRqO-UjAq3nyHy0hlDdxA.png"></figure><p name="6497" id="6497" class="graf graf--p graf-after--figure">then,</p><figure name="987d" id="987d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2BGwIdhjSoDx1ci9VCUchg.png" data-width="984" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*2BGwIdhjSoDx1ci9VCUchg.png"></figure><p name="f9d5" id="f9d5" class="graf graf--p graf-after--figure">by the definition of the hat matrix,</p><figure name="c3b2" id="c3b2" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*qtU2DzBxGUrGhnVKXsydkw.png" data-width="984" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*qtU2DzBxGUrGhnVKXsydkw.png"></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/b91eeebca210"><time class="dt-published" datetime="2020-10-31T04:30:32.310Z">October 31, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/linear-regression-5-mlr-hat-matrix-and-mlr-ols-evaluation-b91eeebca210" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>