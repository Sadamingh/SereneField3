<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Linear Regression 1 | Basic Definitions, Simple Linear Regression, and Ordinary Least Square…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Linear Regression 1 | Basic Definitions, Simple Linear Regression, and Ordinary Least Square…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Linear Regression
</section>
<section data-field="body" class="e-content">
<section name="4ce0" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="34b5" id="34b5" class="graf graf--h3 graf--leading graf--title">Linear Regression 1 | Basic Definitions, <strong class="markup--strong markup--h3-strong">Simple Linear Regression, and Ordinary Least Square Estimation</strong></h3><figure name="2bec" id="2bec" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*6yg0MbxfVr5d-HbXUlsj5A.png" data-width="1514" data-height="716" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*6yg0MbxfVr5d-HbXUlsj5A.png"><figcaption class="imageCaption">Origin from <a href="https://github.com/rougier/scientific-visualization-book" data-href="https://github.com/rougier/scientific-visualization-book" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://github.com/rougier/scientific-visualization-book</a></figcaption></figure><ol class="postList"><li name="ff31" id="ff31" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Recall: Basic Definitions</strong></li></ol><p name="4741" id="4741" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) The Definition of Probability</strong></p><p name="885e" id="885e" class="graf graf--p graf-after--p">The probability is used when we have a well-designed model (truth) and we want to answer the questions like what kinds of data will this truth gives us.</p><p name="1c01" id="1c01" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Definition of Statistics</strong></p><p name="b789" id="b789" class="graf graf--p graf-after--p">The statistics are used when we have a set of data and we want to discover the model or truth underlying the data. The statistics rely on the data and it is treated as the inverse of the probability.</p><p name="3504" id="3504" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Probability vs. Statistics</strong></p><p name="2caa" id="2caa" class="graf graf--p graf-after--p">You can find more information from this article:</p><div name="0031" id="0031" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/adamedelwiess/statistics-for-application-1-statistics-vs-probability-lln-vs-clt-549b8b9618b7" data-href="https://medium.com/adamedelwiess/statistics-for-application-1-statistics-vs-probability-lln-vs-clt-549b8b9618b7" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/statistics-for-application-1-statistics-vs-probability-lln-vs-clt-549b8b9618b7"><strong class="markup--strong markup--mixtapeEmbed-strong">Statistics for Application 1 | Statistics vs. Probability, LLN vs. CLT</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Statistics for Application</em>medium.com</a><a href="https://medium.com/adamedelwiess/statistics-for-application-1-statistics-vs-probability-lln-vs-clt-549b8b9618b7" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="f208eac0b321105b3de0b84e6bf4398f" data-thumbnail-img-id="1*xU6O5JeGtmiUtwuR4tX7Ew.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*xU6O5JeGtmiUtwuR4tX7Ew.png);"></a></div><p name="a98a" id="a98a" class="graf graf--p graf-after--mixtapeEmbed">So which one is more commonly used, statistics or probabilities? Well, it is hard to say, but we can confirm that most of the mathematics model we used today (i.e. there’s a 50% chance that it will be a rainy day tomorrow) is statistics and it is not the truth. The reason why we can make this assertion is that our models are based on the data but not the real mathematics models.</p><p name="e35b" id="e35b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) The Definition of the Outcomes</strong></p><p name="a9d3" id="a9d3" class="graf graf--p graf-after--p">In the probability theory, an outcome is a possible result of an experiment or trial. For example, if we flip a coin twice, the result of one flip is either head (H) or tail (T), so the outcomes can be one of the four consequences (H, H), (H, T), (T, H), or (T, T).</p><p name="17fa" id="17fa" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) The Definition of the Events</strong></p><p name="6213" id="6213" class="graf graf--p graf-after--p">In the probability theory, an event is a set of outcomes. That means each event must contain at least one outcome, but they can contain as many outcomes as they like. For example, suppose we have several events A, B, and C,</p><ul class="postList"><li name="f173" id="f173" class="graf graf--li graf-after--p">A = flip a coin twice, each outcome is either a head or a tail</li></ul><p name="9b66" id="9b66" class="graf graf--p graf-after--li">⇒ A = {(H, H), (H, T), (T, H), (T, T)}</p><ul class="postList"><li name="e5c3" id="e5c3" class="graf graf--li graf-after--p">B = flip a coin twice, with at least one tail</li></ul><p name="953d" id="953d" class="graf graf--p graf-after--li">⇒ B = {(H, T), (T, H), (T, T)}</p><ul class="postList"><li name="d575" id="d575" class="graf graf--li graf-after--p">C = flip a coin twice, with only tails</li></ul><p name="8ef4" id="8ef4" class="graf graf--p graf-after--li">⇒ C= {(T, T)}</p><p name="c179" id="c179" class="graf graf--p graf-after--p">Usually, we use <em class="markup--em markup--p-em">w</em> to denote an event and we use Ω to denote the event that has all the outcomes of an experiment or a trial.</p><p name="0da1" id="0da1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) The Definition of the Random Variables</strong></p><p name="9f10" id="9f10" class="graf graf--p graf-after--p">The random variable is defined as a <strong class="markup--strong markup--p-strong">measurable function</strong> that takes on random values by any given events. The random variables are denoted as X(<em class="markup--em markup--p-em">w</em>) or X. For example, we define a random variable of X by the number of heads after flipping a coin twice, then X should be,</p><figure name="906e" id="906e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-PfS4jFp-EZUUZJRJ2r3Kg.png" data-width="1972" data-height="368" src="https://cdn-images-1.medium.com/max/800/1*-PfS4jFp-EZUUZJRJ2r3Kg.png"></figure><p name="63c5" id="63c5" class="graf graf--p graf-after--figure">A loose definition of the random variables is that a random variable is a <strong class="markup--strong markup--p-strong">quantity</strong> that takes on random values related to our data. We are going to use this definition because this one is easier to understand.</p><p name="93bd" id="93bd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) The Definition of the Bias</strong></p><p name="2b17" id="2b17" class="graf graf--p graf-after--p">Suppose we conducted an experiment and then collected a set of data, what we want to figure out is that “why we have these data?”. The answer to this question is that it could be because of the truth of the underlying model (of the god’s world), or it can be because of the way we collected the data.</p><p name="99da" id="99da" class="graf graf--p graf-after--p">The truth of the underlying model, as we have said, can be stimulated by the statistics. However, because we can not estimate the impacts because of the experiment method, so we have to do our best to reduce the impact of the choice of the experimental methods.</p><p name="516c" id="516c" class="graf graf--p graf-after--p">This impact of the method is called the bias of the experiment.</p><figure name="57d2" id="57d2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*7P_fRPxg0sEp1UjIGDaS2Q.png" data-width="2050" data-height="786" src="https://cdn-images-1.medium.com/max/800/1*7P_fRPxg0sEp1UjIGDaS2Q.png"></figure><p name="3fcc" id="3fcc" class="graf graf--p graf-after--figure">We try to avoid the bias as much as possible so that we can have a better estimate of the truth,</p><figure name="6e75" id="6e75" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0TkucCUMJSfkgg5UfwEi3g.png" data-width="2050" data-height="826" src="https://cdn-images-1.medium.com/max/800/1*0TkucCUMJSfkgg5UfwEi3g.png"></figure><p name="e27b" id="e27b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(8) Two Schools of Statistics</strong></p><figure name="d363" id="d363" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*q0xy4aaMlc93gHSr1JCr6w.png" data-width="1544" data-height="700" src="https://cdn-images-1.medium.com/max/800/1*q0xy4aaMlc93gHSr1JCr6w.png"><figcaption class="imageCaption"><a href="https://xkcd.com/1132/" data-href="https://xkcd.com/1132/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://xkcd.com/1132/</a></figcaption></figure><p name="1c03" id="1c03" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Frequentist View</strong> - the parameters of probabilistic models are fixed, but we just don’t know them.</p><p name="ba0a" id="ba0a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Bayesian View</strong> — the parameters of probabilistic models are not only unknown but also random.</p><p name="082f" id="082f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) The Definition of the Probability Distributions</strong></p><p name="54b0" id="54b0" class="graf graf--p graf-after--p">For a discrete random variable, the probability distribution <em class="markup--em markup--p-em">p</em> describes how likely each of those random values is. So suppose we have an observing value <em class="markup--em markup--p-em">a</em> and <em class="markup--em markup--p-em">p</em>(<em class="markup--em markup--p-em">a</em>) refers to the probability of observing a value <em class="markup--em markup--p-em">a</em>.</p><p name="370a" id="370a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) The Definition of the Empirical Distributions</strong></p><p name="4ca4" id="4ca4" class="graf graf--p graf-after--p">The empirical distribution of a dataset or usually called the <strong class="markup--strong markup--p-strong">distribution of the data</strong> is the relative frequency of each value in some observed dataset.</p><p name="b92b" id="b92b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(11) The Definition of Expectation</strong></p><p name="7298" id="7298" class="graf graf--p graf-after--p">The expectation of a random variable is the average value of this random variable. We often use the notation 𝔼 or μ to represent the expectation of a random variable.</p><figure name="e877" id="e877" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WYUOXlMZACXDcQTvgeNnIw.png" data-width="1104" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*WYUOXlMZACXDcQTvgeNnIw.png"></figure><p name="a26c" id="a26c" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(12) The Definition of Variance</strong></p><p name="6779" id="6779" class="graf graf--p graf-after--p">The variance of a random variable is a measure of how spread out it is. We often use the notation <em class="markup--em markup--p-em">Var</em> or σ² to represent the expectation of a random variable.</p><figure name="6e93" id="6e93" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*goa0w1Cj0MTMIw64uA8cMw.png" data-width="1104" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*goa0w1Cj0MTMIw64uA8cMw.png"></figure><p name="5b80" id="5b80" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(13) The Definition of a Sample Measure</strong></p><p name="d60d" id="d60d" class="graf graf--p graf-after--p">The value we used to describe or summarize a sample is called a sample measure. It is denoted with a hat, for example,</p><figure name="97a6" id="97a6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KenqaBia0uOJo07b92LUJw.png" data-width="966" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*KenqaBia0uOJo07b92LUJw.png"></figure><p name="b496" id="b496" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(14)</strong> <strong class="markup--strong markup--p-strong">The Definition of Sample Mean</strong></p><p name="b31d" id="b31d" class="graf graf--p graf-after--p">The mean value of the sample is the average value of a sample. The sample mean is a sample measure.</p><figure name="e814" id="e814" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mgY_ASDwXXgL3BcczkBZKQ.png" data-width="950" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*mgY_ASDwXXgL3BcczkBZKQ.png"></figure><p name="1189" id="1189" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(15)</strong> <strong class="markup--strong markup--p-strong">The Definition of Sample Variance</strong></p><p name="9191" id="9191" class="graf graf--p graf-after--p">The variance of a sample describes how spread out of data in a sample. The sample variance is a sample measure.</p><figure name="72e2" id="72e2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FmOa8Q36E55VDRmzWMTYVA.png" data-width="1152" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*FmOa8Q36E55VDRmzWMTYVA.png"></figure><p name="9120" id="9120" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(16) The Definition of Unbiased Sample Measure</strong></p><p name="d1d1" id="d1d1" class="graf graf--p graf-after--p">Mathematically, an unbiased sample measure should follow the feature that, on average, we do expect its value to be the real parameter of the truth.</p><figure name="29cc" id="29cc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MbgI2QKQH90xhpT5yDYS6w.png" data-width="966" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*MbgI2QKQH90xhpT5yDYS6w.png"></figure><p name="4d70" id="4d70" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(17) The Definition of the Standard Deviation</strong></p><p name="403b" id="403b" class="graf graf--p graf-after--p">The standard deviation of a sample is the square root of its variance.</p><figure name="67a5" id="67a5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gf423ovgL7Kwv121GryPTg.png" data-width="1152" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*gf423ovgL7Kwv121GryPTg.png"></figure><p name="aa60" id="aa60" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">2. Simple Linear Regression and the Ordinary Least Square Solution</strong></p><p name="ca59" id="ca59" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of the Regression</strong></p><p name="7a33" id="7a33" class="graf graf--p graf-after--p">The term of regression is invented by an English Victorian statistician named <a href="https://en.wikipedia.org/wiki/Francis_Galton" data-href="https://en.wikipedia.org/wiki/Francis_Galton" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Francis Galton</a> when he tried to describe the biological phenomenon that the heights of the descendants of tall ancestors tend to regress down towards a normal average.</p><p name="de9f" id="de9f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2)</strong> <strong class="markup--strong markup--p-strong">The Definition of Linear Regression</strong></p><p name="3a9f" id="3a9f" class="graf graf--p graf-after--p">The linear regression is, literally, a regression that must have a linear form of a mathematical model. It is such an incredible and powerful tool for analyzing our daily data.</p><p name="3733" id="3733" class="graf graf--p graf-after--p">The term linear means that the parameters of this model should be linear instead of the variable itself. This can be hard to understand at the first glance. But we can think about the following examples,</p><figure name="7884" id="7884" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3qpmudr9fQwa5xTpoms7sg.png" data-width="1408" data-height="252" src="https://cdn-images-1.medium.com/max/800/1*3qpmudr9fQwa5xTpoms7sg.png"></figure><p name="fd30" id="fd30" class="graf graf--p graf-after--figure">These datasets can all be treated as a linear relationship. This is because we can produce them with a reproduce of xi, and the parameters will still be linear.</p><p name="3e83" id="3e83" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3)</strong> <strong class="markup--strong markup--p-strong">The Definition of Simple Linear Regression</strong></p><p name="4ccc" id="4ccc" class="graf graf--p graf-after--p">We are going to begin with the easiest model. Suppose we have got two variables and we would like to fit a line to our data. Define that <em class="markup--em markup--p-em">x</em> is our <strong class="markup--strong markup--p-strong">independent variable</strong> (aka. predictor variable) and <em class="markup--em markup--p-em">y</em> is our <strong class="markup--strong markup--p-strong">dependent variable</strong> (aka. response variable). For example, we have a dataset like</p><figure name="01bc" id="01bc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mSXPyTbPjW-wyxffjo6yBw.png" data-width="1818" data-height="578" src="https://cdn-images-1.medium.com/max/800/1*mSXPyTbPjW-wyxffjo6yBw.png"></figure><p name="9990" id="9990" class="graf graf--p graf-after--figure">Then, we can fit a line with the form of,</p><figure name="db31" id="db31" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*SZPJQtIC__cKCXY6I1ZIvg.png" data-width="1126" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*SZPJQtIC__cKCXY6I1ZIvg.png"></figure><p name="26f7" id="26f7" class="graf graf--p graf-after--figure">where it is quite clear that β1-cap is the slope of the line and β0-cap is the intercept of the line. Note that we are using β0-cap and β1-cap and this is because we are not getting the real parameters, instead, what we are getting is the parameters based on our given data. So we can not directly write β0 and β1 in this case. Draw this line in the graph, which is,</p><figure name="4c4b" id="4c4b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BSSxeckVZTRDPgD6LdIyvA.png" data-width="1826" data-height="576" src="https://cdn-images-1.medium.com/max/800/1*BSSxeckVZTRDPgD6LdIyvA.png"></figure><p name="abce" id="abce" class="graf graf--p graf-after--figure">But now, we have this line but we can not have the data. So this line seems meaningless. Suppose we are given every <em class="markup--em markup--p-em">x</em>i, we can still not exactly fit in the line to get all the <em class="markup--em markup--p-em">y</em>i. So what can we do now?</p><p name="1c65" id="1c65" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4)</strong> <strong class="markup--strong markup--p-strong">The Definition of the Residual</strong></p><p name="ba7d" id="ba7d" class="graf graf--p graf-after--p">Based on the line we have talked about, we are actually given is an estimator of yi (yi-cap) by given xi.</p><figure name="93fc" id="93fc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ew4EyDeft4xMMozcv3MGiQ.png" data-width="1126" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*Ew4EyDeft4xMMozcv3MGiQ.png"></figure><p name="4553" id="4553" class="graf graf--p graf-after--figure">But, there is still a difference between yi-cap and the true value yi. And this difference changes when xi changes (so it is a random variable). We define the difference between the yi-cap and the true value yi as the residual of xi, which can be denoted as ei.</p><figure name="738c" id="738c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*D06_dzLxX_Ma_wcjpy-N6w.png" data-width="1126" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*D06_dzLxX_Ma_wcjpy-N6w.png"></figure><p name="29c2" id="29c2" class="graf graf--p graf-after--figure">In the graph, we can randomly show an ei and an ej like,</p><figure name="c245" id="c245" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UYpUs8zkmkza9nNkXIq3uQ.png" data-width="1254" data-height="394" src="https://cdn-images-1.medium.com/max/800/1*UYpUs8zkmkza9nNkXIq3uQ.png"></figure><p name="55b7" id="55b7" class="graf graf--p graf-after--figure">Thus, in conclusion, the form of the simple linear regression for a given sample of two variables (or a dataset of two variables) is,</p><figure name="35a8" id="35a8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3Gly18LD2oFm5mk-Oi27yg.png" data-width="1042" data-height="72" src="https://cdn-images-1.medium.com/max/800/1*3Gly18LD2oFm5mk-Oi27yg.png"></figure><p name="25ed" id="25ed" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5)</strong> <strong class="markup--strong markup--p-strong">The Definition of the Error Term</strong></p><p name="2751" id="2751" class="graf graf--p graf-after--p">Previously what we are talking about is under the condition that we are given a sample or a dataset. So how about if we don’t have any samples or data and what is the truth behind two variables? Probably, suppose these two variables have a linear relationship similar to the above, we can then write a similar formula, which is,</p><figure name="dcf4" id="dcf4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*iL7a6tflD9OifzXwXK_mFQ.png" data-width="1042" data-height="72" src="https://cdn-images-1.medium.com/max/800/1*iL7a6tflD9OifzXwXK_mFQ.png"></figure><p name="764a" id="764a" class="graf graf--p graf-after--figure">In this formula, β0 and β1 are the true parameters and ϵi is called the<strong class="markup--strong markup--p-strong"> error term</strong>. Here, the ϵi is also a random variable but it is very different from the residual ei. The residual can actually be seen as an estimator of the error term, but we can not call it in that way because ϵi is not a fixed value (instead, it is a random variable. We can only have an estimator of a fixed parameter).</p><p name="4aec" id="4aec" class="graf graf--p graf-after--p">What this formula means is that the god actually rolls the dice! We tried to grab a yi by a given xi, but the god will not give us the same value for the same xi, this is because that the god randomly chooses a random value and then add it to our result.</p><p name="e622" id="e622" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) Assumptions for the Error Term</strong></p><p name="87b9" id="87b9" class="graf graf--p graf-after--p">So now we have known the definition of the error term and it should be a random variable, however, we can know nothing else from the previous discussion. This is absolutely not good for any further discussions. And there is a need we make an assumption of this error term in order to get more information.</p><p name="9f50" id="9f50" class="graf graf--p graf-after--p">First of all, we can have a strong assumption that gives all the information we need for this random variable. So,</p><ul class="postList"><li name="b412" id="b412" class="graf graf--li graf-after--p">Assumption #1: the strong condition of Gaussian noise</li></ul><figure name="4522" id="4522" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*A1FjMMISAZCQSRBkjpxfTQ.png" data-width="1152" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*A1FjMMISAZCQSRBkjpxfTQ.png"></figure><p name="a96f" id="a96f" class="graf graf--p graf-after--figure">But we can reduce the conditions or assumptions so as to make the conclusions more general, we have three loose assumptions here,</p><ul class="postList"><li name="073f" id="073f" class="graf graf--li graf-after--p">Assumption #2: zero mean</li></ul><figure name="2c52" id="2c52" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*pKrhOsS-ZID2yIiL-JGeIQ.png" data-width="1152" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*pKrhOsS-ZID2yIiL-JGeIQ.png"></figure><ul class="postList"><li name="cc43" id="cc43" class="graf graf--li graf-after--figure">Assumption #3: population variance</li></ul><figure name="ef72" id="ef72" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*HIz7k5usTHgrrkyq0diEDQ.png" data-width="1152" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*HIz7k5usTHgrrkyq0diEDQ.png"></figure><ul class="postList"><li name="40b3" id="40b3" class="graf graf--li graf-after--figure">Assumption #4: no correlation between errors</li></ul><figure name="92ce" id="92ce" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*5q_0k6_KY3tZ_shSaF0f_Q.png" data-width="1152" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*5q_0k6_KY3tZ_shSaF0f_Q.png"></figure><p name="f898" id="f898" class="graf graf--p graf-after--figure">For different assumptions, we can have different interesting conclusions. And we are now going to start with the assumption #1 and save the others for the later discussions.</p><p name="42b0" id="42b0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) The Assumption for the Independent Variable</strong></p><p name="fa24" id="fa24" class="graf graf--p graf-after--p">The independent variable is called so because we make an assumption of it. The assumption is that the independent variable has a fixed effect on the dependent variable. By this assumption, we don’t have to worry about how the distribution of the independent variable impacts the dependent variable.</p><p name="036e" id="036e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) Solving For the Best Fit Line</strong></p><p name="ad25" id="ad25" class="graf graf--p graf-after--p">Suppose we have a set of data (<em class="markup--em markup--p-em">x</em>1, <em class="markup--em markup--p-em">y</em>1) … (<em class="markup--em markup--p-em">x</em>n, <em class="markup--em markup--p-em">y</em>n), or a sample set or a set of observations, then it turns out that we can find as many lines we want to fit in the data. But there is only one line best fits all the data. This question is equivalent to solving the following optimization problem of <strong class="markup--strong markup--p-strong">the sum of the squares</strong>,</p><figure name="c98e" id="c98e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qytdAsNls4VvLLwRHah9Kw.png" data-width="1152" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*qytdAsNls4VvLLwRHah9Kw.png"></figure><p name="77b3" id="77b3" class="graf graf--p graf-after--figure">This is known as the <strong class="markup--strong markup--p-strong">least-squares linear regression problem </strong>(or the <strong class="markup--strong markup--p-strong">Ordinary Least Squares Regression Problem</strong>, or<strong class="markup--strong markup--p-strong"> OLS</strong>). We can denote this as the function <em class="markup--em markup--p-em">s</em>, which is,</p><figure name="1eee" id="1eee" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DfmOkQMwo9ojx9pL1M2SiA.png" data-width="1152" data-height="92" src="https://cdn-images-1.medium.com/max/800/1*DfmOkQMwo9ojx9pL1M2SiA.png"></figure><p name="05c5" id="05c5" class="graf graf--p graf-after--figure">Thus,</p><figure name="3a74" id="3a74" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6sA_Wijp3hvWSNDHHu8pTQ.png" data-width="1152" data-height="82" src="https://cdn-images-1.medium.com/max/800/1*6sA_Wijp3hvWSNDHHu8pTQ.png"></figure><p name="1c05" id="1c05" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(9) A Common Mistake</strong></p><p name="efb9" id="efb9" class="graf graf--p graf-after--p">People usually mixed up the notations, sometimes I saw people sometimes write things like,</p><figure name="b4fb" id="b4fb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*L4zVnCPVXTP8et5gADVSkg.png" data-width="1152" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*L4zVnCPVXTP8et5gADVSkg.png"></figure><p name="6907" id="6907" class="graf graf--p graf-after--figure">or,</p><figure name="3068" id="3068" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9odg0K1IVF4WFZZjKmIjlQ.png" data-width="1152" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*9odg0K1IVF4WFZZjKmIjlQ.png"></figure><p name="71e7" id="71e7" class="graf graf--p graf-after--figure">These are completely meaningless because we can’t mix the notations up. Keep carefully in mind that ϵi is the error term for the truth and <em class="markup--em markup--p-em">e</em>i is the residual for the sample.</p><p name="5a91" id="5a91" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) Solving the OLS Estimator</strong></p><p name="7ff1" id="7ff1" class="graf graf--p graf-after--p">By,</p><figure name="1c68" id="1c68" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qytdAsNls4VvLLwRHah9Kw.png" data-width="1152" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*qytdAsNls4VvLLwRHah9Kw.png"></figure><p name="902c" id="902c" class="graf graf--p graf-after--figure">Now we would like to solve β0-cap and β1-cap. First of all, let’s use the partial derivation,</p><figure name="798d" id="798d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vta6dPFvYzo2-s3xGt3zPQ.png" data-width="1152" data-height="196" src="https://cdn-images-1.medium.com/max/800/1*vta6dPFvYzo2-s3xGt3zPQ.png"></figure><p name="c65d" id="c65d" class="graf graf--p graf-after--figure">then,</p><figure name="6c18" id="6c18" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VmUwJPTH0rNCpq_EFqGbjQ.png" data-width="1152" data-height="162" src="https://cdn-images-1.medium.com/max/800/1*VmUwJPTH0rNCpq_EFqGbjQ.png"></figure><p name="3676" id="3676" class="graf graf--p graf-after--figure">then,</p><figure name="5879" id="5879" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CR0CYk1LUvKRCt6naDgQrg.png" data-width="1152" data-height="162" src="https://cdn-images-1.medium.com/max/800/1*CR0CYk1LUvKRCt6naDgQrg.png"></figure><p name="5b36" id="5b36" class="graf graf--p graf-after--figure">then,</p><figure name="314c" id="314c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PzuvxsOlGp6Re5ERm1rRoQ.png" data-width="1152" data-height="162" src="https://cdn-images-1.medium.com/max/800/1*PzuvxsOlGp6Re5ERm1rRoQ.png"></figure><p name="ba50" id="ba50" class="graf graf--p graf-after--figure">then,</p><figure name="2cea" id="2cea" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Q-A1n1zx2c_W-AD1nzTLfA.png" data-width="1152" data-height="162" src="https://cdn-images-1.medium.com/max/800/1*Q-A1n1zx2c_W-AD1nzTLfA.png"></figure><p name="1daf" id="1daf" class="graf graf--p graf-after--figure">then,</p><figure name="eb35" id="eb35" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8rZv4lK5yR9QzXCAWl1Dxw.png" data-width="1152" data-height="162" src="https://cdn-images-1.medium.com/max/800/1*8rZv4lK5yR9QzXCAWl1Dxw.png"></figure><p name="0ea8" id="0ea8" class="graf graf--p graf-after--figure">Because,</p><figure name="4b4e" id="4b4e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*j-hpiCOqmfTQSCL9uotmNA.png" data-width="1152" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*j-hpiCOqmfTQSCL9uotmNA.png"></figure><p name="197b" id="197b" class="graf graf--p graf-after--figure">then,</p><figure name="fde5" id="fde5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_EG3yCF7Y49_XTuIkqhn7A.png" data-width="1152" data-height="156" src="https://cdn-images-1.medium.com/max/800/1*_EG3yCF7Y49_XTuIkqhn7A.png"></figure><p name="55ca" id="55ca" class="graf graf--p graf-after--figure">then,</p><figure name="5740" id="5740" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*F5x1rO-4BnwFYnU0FoCzKA.png" data-width="1152" data-height="172" src="https://cdn-images-1.medium.com/max/800/1*F5x1rO-4BnwFYnU0FoCzKA.png"></figure><p name="9174" id="9174" class="graf graf--p graf-after--figure">This is the solution of the OLS estimator.</p><p name="96d4" id="96d4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(10) The Second Solution of the OLS Estimator</strong></p><p name="795e" id="795e" class="graf graf--p graf-after--p">By the result above, we can have,</p><figure name="3d76" id="3d76" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FrLjLZoo_EQGWqm4y02Qhw.png" data-width="1152" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*FrLjLZoo_EQGWqm4y02Qhw.png"></figure><p name="70e2" id="70e2" class="graf graf--p graf-after--figure">then, we can have,</p><figure name="362c" id="362c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nD7LH1i12LOm4fqM0SK7wA.png" data-width="1152" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*nD7LH1i12LOm4fqM0SK7wA.png"></figure><p name="fa88" id="fa88" class="graf graf--p graf-after--figure">then,</p><figure name="ff56" id="ff56" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dZiN8mku3Hl8_wnztYrxGA.png" data-width="1152" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*dZiN8mku3Hl8_wnztYrxGA.png"></figure><p name="6df1" id="6df1" class="graf graf--p graf-after--figure">then,</p><figure name="1d66" id="1d66" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qgjyXjH2RIzd90ElK7_uGQ.png" data-width="1152" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*qgjyXjH2RIzd90ElK7_uGQ.png"></figure><p name="029b" id="029b" class="graf graf--p graf-after--figure">then,</p><figure name="ab52" id="ab52" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1RwMLfCeJyohM0msr3QHAw.png" data-width="1152" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*1RwMLfCeJyohM0msr3QHAw.png"></figure><p name="7ad8" id="7ad8" class="graf graf--p graf-after--figure">then,</p><figure name="b7d0" id="b7d0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*t1o5rYLK4aOhmkLSESRN5Q.png" data-width="1152" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*t1o5rYLK4aOhmkLSESRN5Q.png"></figure><p name="97e3" id="97e3" class="graf graf--p graf-after--figure">then,</p><figure name="8c35" id="8c35" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ycmcuPrmJweCczt5WDWXiA.png" data-width="1152" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*ycmcuPrmJweCczt5WDWXiA.png"></figure><p name="983d" id="983d" class="graf graf--p graf-after--figure">Recall that the denominator is the sample variance of the independent variable and the molecular is the sample covariance of the independent variable and the dependent variable. Thus, suppose we have notation that,</p><p name="36e2" id="36e2" class="graf graf--p graf-after--p">then,</p><figure name="cbf7" id="cbf7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*V8FAInTZw2VPGRPsXtkDGA.png" data-width="1206" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*V8FAInTZw2VPGRPsXtkDGA.png"></figure><p name="0deb" id="0deb" class="graf graf--p graf-after--figure">thus,</p><figure name="e0d2" id="e0d2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*sDVLwcFD3aUiImfiQImr7Q.png" data-width="1152" data-height="92" src="https://cdn-images-1.medium.com/max/800/1*sDVLwcFD3aUiImfiQImr7Q.png"></figure><p name="0060" id="0060" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(11) The Third Solution of the OLS Estimator</strong></p><p name="5b39" id="5b39" class="graf graf--p graf-after--p">Because we can know that the molecular is the sample covariance of the independent variable, then we can turn this sample covariance into a sample correlation.</p><p name="f6bc" id="f6bc" class="graf graf--p graf-after--p">The sample correlation coefficient is defined as,</p><figure name="ce2a" id="ce2a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DWLxc9L3GX33dF0V4ct1Uw.png" data-width="1152" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*DWLxc9L3GX33dF0V4ct1Uw.png"></figure><p name="56f5" id="56f5" class="graf graf--p graf-after--figure">where sx and sy are the sample standard deviation of variable x and y.</p><p name="5f03" id="5f03" class="graf graf--p graf-after--p">Then by,</p><figure name="fb17" id="fb17" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ycmcuPrmJweCczt5WDWXiA.png" data-width="1152" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*ycmcuPrmJweCczt5WDWXiA.png"></figure><p name="bbc8" id="bbc8" class="graf graf--p graf-after--figure">we can have,</p><figure name="12e0" id="12e0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CIBzG0Zpwjx_ud87dT5JcA.png" data-width="1152" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*CIBzG0Zpwjx_ud87dT5JcA.png"></figure><p name="8519" id="8519" class="graf graf--p graf-after--figure">Because,</p><figure name="2496" id="2496" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3R1db-msYc3aP_j0-nCtkQ.png" data-width="1152" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*3R1db-msYc3aP_j0-nCtkQ.png"></figure><p name="21d8" id="21d8" class="graf graf--p graf-after--figure">then,</p><figure name="cdcf" id="cdcf" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0aWwt6_HaAV_D88EMxbDvQ.png" data-width="1152" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*0aWwt6_HaAV_D88EMxbDvQ.png"></figure><p name="88d4" id="88d4" class="graf graf--p graf-after--figure">then,</p><figure name="9b4f" id="9b4f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3n6iKHGB2EtCfQhkYCLQrw.png" data-width="1152" data-height="88" src="https://cdn-images-1.medium.com/max/800/1*3n6iKHGB2EtCfQhkYCLQrw.png"></figure><p name="6b11" id="6b11" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(12) The Fourth Solution of the OLS Estimator</strong></p><p name="337d" id="337d" class="graf graf--p graf-after--p">Also, by</p><figure name="1ec6" id="1ec6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FrLjLZoo_EQGWqm4y02Qhw.png" data-width="1152" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*FrLjLZoo_EQGWqm4y02Qhw.png"></figure><p name="4dd9" id="4dd9" class="graf graf--p graf-after--figure">we can know that,</p><figure name="06e4" id="06e4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*L5TrsFLFHei4YTFgQKbKPQ.png" data-width="1152" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*L5TrsFLFHei4YTFgQKbKPQ.png"></figure><p name="79a0" id="79a0" class="graf graf--p graf-after--figure">then,</p><figure name="8e8d" id="8e8d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*sXrYOGVDhZcYo4uO9Leu0w.png" data-width="1152" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*sXrYOGVDhZcYo4uO9Leu0w.png"></figure><p name="6ed3" id="6ed3" class="graf graf--p graf-after--figure">then,</p><figure name="0886" id="0886" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yK0VFCWAbLR4YZWg0I745w.png" data-width="1152" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*yK0VFCWAbLR4YZWg0I745w.png"></figure><p name="aae0" id="aae0" class="graf graf--p graf-after--figure">also, by previous discussion of the second solution,</p><figure name="82c2" id="82c2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nE7OnQxWBHnX-SnKXFDWQA.png" data-width="1152" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*nE7OnQxWBHnX-SnKXFDWQA.png"></figure><p name="fb4c" id="fb4c" class="graf graf--p graf-after--figure">Suppose we define that,</p><figure name="9d54" id="9d54" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ijky7MGMGY4E_eoKJZA3Qw.png" data-width="1152" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*Ijky7MGMGY4E_eoKJZA3Qw.png"></figure><p name="36d2" id="36d2" class="graf graf--p graf-after--figure">then,</p><figure name="77ff" id="77ff" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BagX-y61X1QMSbAkpgu9-A.png" data-width="1152" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*BagX-y61X1QMSbAkpgu9-A.png"></figure><p name="eb4b" id="eb4b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(13) A Summary of the OLS Solution</strong></p><p name="8243" id="8243" class="graf graf--p graf-after--p">Based on the discussions above, we can have that,</p><figure name="a30a" id="a30a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fAZPR4m7nMfVqc-tEj_ulg.png" data-width="1152" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*fAZPR4m7nMfVqc-tEj_ulg.png"></figure><p name="475a" id="475a" class="graf graf--p graf-after--figure">where,</p><figure name="20c1" id="20c1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_TJ7KcOlEHVS7LwjHFK_nw.png" data-width="1152" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*_TJ7KcOlEHVS7LwjHFK_nw.png"></figure><p name="1211" id="1211" class="graf graf--p graf-after--figure">and,</p><figure name="0a63" id="0a63" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gE_F5Xjdby_4Th4-RrvbTA.png" data-width="1152" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*gE_F5Xjdby_4Th4-RrvbTA.png"></figure><p name="94b0" id="94b0" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">3. Evaluation the Ordinary Least Square Solution</strong></p><p name="0363" id="0363" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Gauss-Markov Theorem</strong></p><p name="6eb9" id="6eb9" class="graf graf--p graf-after--p">The least-square estimators β0-cap and β1-cap are unbiased. This is to say that,</p><figure name="7bee" id="7bee" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VloncN4RDuXxqQOJkl8eTQ.png" data-width="1152" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*VloncN4RDuXxqQOJkl8eTQ.png"></figure><p name="f2ce" id="f2ce" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Proof:</strong></p><p name="fc16" id="fc16" class="graf graf--p graf-after--p">We have defined ki satisfies</p><figure name="55ed" id="55ed" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gE_F5Xjdby_4Th4-RrvbTA.png" data-width="1152" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*gE_F5Xjdby_4Th4-RrvbTA.png"></figure><p name="a7ce" id="a7ce" class="graf graf--p graf-after--figure">then,</p><figure name="f4c8" id="f4c8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ctwos4J1HfYYZSk7WlmXEQ.png" data-width="1152" data-height="112" src="https://cdn-images-1.medium.com/max/800/1*ctwos4J1HfYYZSk7WlmXEQ.png"></figure><p name="90ab" id="90ab" class="graf graf--p graf-after--figure">moreover,</p><figure name="2087" id="2087" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Bl325t518z8z-FbNFDbwxg.png" data-width="1152" data-height="142" src="https://cdn-images-1.medium.com/max/800/1*Bl325t518z8z-FbNFDbwxg.png"></figure><p name="6624" id="6624" class="graf graf--p graf-after--figure">By the definition of the bias, we have,</p><figure name="ed5f" id="ed5f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EISc8uTXHl50hYEAv8RPvw.png" data-width="1152" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*EISc8uTXHl50hYEAv8RPvw.png"></figure><p name="46f1" id="46f1" class="graf graf--p graf-after--figure">because</p><figure name="0d15" id="0d15" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*piND-r32ta6NE5DnNx-w_g.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*piND-r32ta6NE5DnNx-w_g.png"></figure><p name="770a" id="770a" class="graf graf--p graf-after--figure">then,</p><figure name="8b17" id="8b17" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KzVH7TYSBBUFG_US_rD3-Q.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*KzVH7TYSBBUFG_US_rD3-Q.png"></figure><p name="a20d" id="a20d" class="graf graf--p graf-after--figure">then,</p><figure name="457f" id="457f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zw4NGem_eZAnVLLkgroJaw.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*zw4NGem_eZAnVLLkgroJaw.png"></figure><p name="9fa7" id="9fa7" class="graf graf--p graf-after--figure">Because we are under the assumption of a Gaussian distributed error term ϵi ~ N(0, σ²), then,</p><figure name="d332" id="d332" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RK-MI1K1FO3v5ikQ3X6P7g.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*RK-MI1K1FO3v5ikQ3X6P7g.png"></figure><p name="e55a" id="e55a" class="graf graf--p graf-after--figure">Also,</p><figure name="9170" id="9170" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8FHLC7qp8W-008HiWDSECg.png" data-width="1016" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*8FHLC7qp8W-008HiWDSECg.png"></figure><p name="afdc" id="afdc" class="graf graf--p graf-after--figure">then,</p><figure name="c40d" id="c40d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*F1JfzABxo1hupI9FPOgSkg.png" data-width="1016" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*F1JfzABxo1hupI9FPOgSkg.png"></figure><p name="062a" id="062a" class="graf graf--p graf-after--figure">For β0-cap, we have,</p><figure name="66e9" id="66e9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RApljBjqEHN5QAYmvaFTZA.png" data-width="1016" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*RApljBjqEHN5QAYmvaFTZA.png"></figure><p name="a361" id="a361" class="graf graf--p graf-after--figure">then,</p><figure name="89d2" id="89d2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-k6m2oeGw8rOMJ_np2c9_g.png" data-width="1016" data-height="78" src="https://cdn-images-1.medium.com/max/800/1*-k6m2oeGw8rOMJ_np2c9_g.png"></figure><p name="ace7" id="ace7" class="graf graf--p graf-after--figure">then,</p><figure name="9b33" id="9b33" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*46DUWHMJtyMcm9s2XbBy1A.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*46DUWHMJtyMcm9s2XbBy1A.png"></figure><p name="fd6e" id="fd6e" class="graf graf--p graf-after--figure">then,</p><figure name="51ae" id="51ae" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0Zt2fsvMMEZmlBXE_xjGlg.png" data-width="1016" data-height="86" src="https://cdn-images-1.medium.com/max/800/1*0Zt2fsvMMEZmlBXE_xjGlg.png"></figure><p name="b5e7" id="b5e7" class="graf graf--p graf-after--figure">Thus,</p><figure name="337a" id="337a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VloncN4RDuXxqQOJkl8eTQ.png" data-width="1152" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*VloncN4RDuXxqQOJkl8eTQ.png"></figure><p name="001e" id="001e" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Variance of the Estimator</strong></p><p name="f937" id="f937" class="graf graf--p graf-after--p">Now we have known that the expectation of the estimators equals the true parameter under the Gaussian noise (error) assumption, so what are the variances of these two estimators?</p><p name="c7b0" id="c7b0" class="graf graf--p graf-after--p">Similarly, we have,</p><figure name="ae3d" id="ae3d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pk0bI0L_lFPivjJiuqBRpw.png" data-width="1016" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*pk0bI0L_lFPivjJiuqBRpw.png"></figure><p name="53a7" id="53a7" class="graf graf--p graf-after--figure">We have known that,</p><figure name="80ac" id="80ac" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*piND-r32ta6NE5DnNx-w_g.png" data-width="1078" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*piND-r32ta6NE5DnNx-w_g.png"></figure><p name="5e23" id="5e23" class="graf graf--p graf-after--figure">with β0, β1 fixed and given xi, the error term ϵi contributes all the variance of yi. Also, because ϵi ~ N(0, σ²), then,</p><figure name="82a7" id="82a7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*t02ySoS6h4TBPOnuWcZTXA.png" data-width="1016" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*t02ySoS6h4TBPOnuWcZTXA.png"></figure><p name="960c" id="960c" class="graf graf--p graf-after--figure">then,</p><figure name="176f" id="176f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HTsDFol_DGmS0UtyPij_jg.png" data-width="1016" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*HTsDFol_DGmS0UtyPij_jg.png"></figure><p name="9d88" id="9d88" class="graf graf--p graf-after--figure">by the definition of ki,</p><figure name="b83b" id="b83b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EYORXt4F-Jmz7j30-m1LIg.png" data-width="1016" data-height="92" src="https://cdn-images-1.medium.com/max/800/1*EYORXt4F-Jmz7j30-m1LIg.png"></figure><p name="2a0c" id="2a0c" class="graf graf--p graf-after--figure">then,</p><figure name="4929" id="4929" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*wpvzrZH3b9RNLdzcxpLvFw.png" data-width="1016" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*wpvzrZH3b9RNLdzcxpLvFw.png"></figure><p name="013c" id="013c" class="graf graf--p graf-after--figure">then,</p><figure name="5e38" id="5e38" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8NSIV3KLK3834xjwVbMTMg.png" data-width="1016" data-height="116" src="https://cdn-images-1.medium.com/max/800/1*8NSIV3KLK3834xjwVbMTMg.png"></figure><p name="a6c1" id="a6c1" class="graf graf--p graf-after--figure">thus,</p><figure name="aa24" id="aa24" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*XkjLsLq12V93tljrGuQ-mQ.png" data-width="1016" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*XkjLsLq12V93tljrGuQ-mQ.png"></figure><p name="07a2" id="07a2" class="graf graf--p graf-after--figure">For β0-cap, we have,</p><figure name="2049" id="2049" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rdtpE1Tc8goMuSurvHQvcg.png" data-width="1102" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*rdtpE1Tc8goMuSurvHQvcg.png"></figure><p name="3382" id="3382" class="graf graf--p graf-after--figure">then,</p><figure name="d377" id="d377" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*f9yPcL0kX4WU68PsafF9vg.png" data-width="1102" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*f9yPcL0kX4WU68PsafF9vg.png"></figure><p name="963c" id="963c" class="graf graf--p graf-after--figure">then,</p><figure name="5be6" id="5be6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*isBmm4SRDEj5ldzROI80pg.png" data-width="1102" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*isBmm4SRDEj5ldzROI80pg.png"></figure><p name="c1a0" id="c1a0" class="graf graf--p graf-after--figure">then,</p><figure name="0226" id="0226" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-H-qhWZvcu9mI4eaX8BoGA.png" data-width="1102" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*-H-qhWZvcu9mI4eaX8BoGA.png"></figure><p name="baee" id="baee" class="graf graf--p graf-after--figure">then,</p><figure name="0c5f" id="0c5f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*cYDqPTt_-It2iyyHyah_OA.png" data-width="1102" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*cYDqPTt_-It2iyyHyah_OA.png"></figure><p name="6b24" id="6b24" class="graf graf--p graf-after--figure">then,</p><figure name="5eb6" id="5eb6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KD4I-v-9i1sIDGfNQVUsTg.png" data-width="1102" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*KD4I-v-9i1sIDGfNQVUsTg.png"></figure><p name="19a9" id="19a9" class="graf graf--p graf-after--figure">with,</p><figure name="a5ef" id="a5ef" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NcP2DXQ_FJTFgKyHOq_GcQ.png" data-width="1102" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*NcP2DXQ_FJTFgKyHOq_GcQ.png"></figure><p name="1fed" id="1fed" class="graf graf--p graf-after--figure">then,</p><figure name="2555" id="2555" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*km-a_kBCMW-JYl-ZHSLo8A.png" data-width="1102" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*km-a_kBCMW-JYl-ZHSLo8A.png"></figure><p name="cc5c" id="cc5c" class="graf graf--p graf-after--figure">then,</p><figure name="002e" id="002e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4KtW-NBut3vy59p1vQmlhw.png" data-width="1102" data-height="114" src="https://cdn-images-1.medium.com/max/800/1*4KtW-NBut3vy59p1vQmlhw.png"></figure><p name="6d1d" id="6d1d" class="graf graf--p graf-after--figure">thus,</p><figure name="4064" id="4064" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0W0cGZ_dbT9xFpOK0qluWQ.png" data-width="1102" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*0W0cGZ_dbT9xFpOK0qluWQ.png"></figure><p name="89c3" id="89c3" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Best Linear Unbiased Estimator</strong></p><p name="4a93" id="4a93" class="graf graf--p graf-after--p">In fact, we give a very high evaluation of the OLS estimator under the assumption of the Gaussian noise, which is <strong class="markup--strong markup--p-strong">BLUE</strong> (full name: the best linear unbiased estimators). By the Gauss-Markov theorem, we can know that β0-cap and β1-cap are unbiased estimators.</p><p name="222e" id="222e" class="graf graf--p graf-after--p">Also, by the fourth solution of the OLS estimator, which is,</p><figure name="9b95" id="9b95" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BagX-y61X1QMSbAkpgu9-A.png" data-width="1152" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*BagX-y61X1QMSbAkpgu9-A.png"></figure><p name="f6eb" id="f6eb" class="graf graf--p graf-after--figure">we can know that β1-cap is a linear estimator.</p><p name="b795" id="b795" class="graf graf--p graf-after--p">By what we have calculated in above,</p><figure name="0ed8" id="0ed8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-H-qhWZvcu9mI4eaX8BoGA.png" data-width="1102" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*-H-qhWZvcu9mI4eaX8BoGA.png"></figure><p name="502a" id="502a" class="graf graf--p graf-after--figure">this is to say that,</p><figure name="ec99" id="ec99" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ryyfJQD0HyRUFs-FzIx1sw.png" data-width="1102" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*ryyfJQD0HyRUFs-FzIx1sw.png"></figure><p name="92f2" id="92f2" class="graf graf--p graf-after--figure">then, β0-cap is also a linear estimator.</p><p name="1f74" id="1f74" class="graf graf--p graf-after--p graf--trailing">We are going to leave an open question here. Why is OLS the best estimator? We are going to discuss it in the following sections.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/53e953bec520"><time class="dt-published" datetime="2020-10-15T11:24:07.185Z">October 15, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/linear-regression-1-basic-definitions-simple-linear-regression-and-ordinary-least-square-53e953bec520" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>