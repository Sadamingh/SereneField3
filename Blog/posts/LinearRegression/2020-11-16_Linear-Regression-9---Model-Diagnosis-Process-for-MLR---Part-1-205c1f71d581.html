<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Linear Regression 9 | Model Diagnosis Process for MLR - Part 1</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Linear Regression 9 | Model Diagnosis Process for MLR - Part 1</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Linear Regression
</section>
<section data-field="body" class="e-content">
<section name="d523" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1ef9" id="1ef9" class="graf graf--h3 graf--leading graf--title">Linear Regression 9 | <strong class="markup--strong markup--h3-strong">Model Diagnosis Process for MLR - Part 1</strong></h3><figure name="86e6" id="86e6" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*JRXYaS8AmzV2HWueCj2mRA.png" data-width="1514" data-height="716" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*JRXYaS8AmzV2HWueCj2mRA.png"></figure><ol class="postList"><li name="ee40" id="ee40" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Model Diagnosis Process for MLR</strong></li></ol><p name="2704" id="2704" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(0) Goal of Modeling</strong></p><ul class="postList"><li name="8ab7" id="8ab7" class="graf graf--li graf-after--p">make prediction</li><li name="3764" id="3764" class="graf graf--li graf-after--li">capture the significant predictors</li><li name="5f35" id="5f35" class="graf graf--li graf-after--li">explain the changes in the dependent variable</li></ul><p name="29d1" id="29d1" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) Step 1. Check Multicollinearity</strong></p><ul class="postList"><li name="6bed" id="6bed" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Reason:</strong> two predictors are correlated or highly correlated.</li><li name="144e" id="144e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Worst result:</strong> XtX is non-invertible (singular) and we can not calculate its inverse matrix. Thus, the OLSE can’t be achieved.</li><li name="30df" id="30df" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Worse result:</strong> XtX is close to non-invertible, and we are going to have a matrix of inverse XtX matrix with huge values.</li><li name="7164" id="7164" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Symptom</strong></li></ul><p name="7525" id="7525" class="graf graf--p graf-after--li">a. OLSE changes <strong class="markup--strong markup--p-strong">hugely</strong> when adding or dropping independent variables to or from the model, this implies that the newly added or dropped variable has a multicollinearity problem with the other variables in the model (sometimes even changes the sign);</p><p name="3603" id="3603" class="graf graf--p graf-after--p">b. It can be hard to reject the null hypothesis and there are several variables in the model that are not significant in the t-test. This is because the variance of the OLSE σ²inverse(XtX) can be huge, which makes the standard error for each OLSE large, and it contributes to a small t statistics in our model.</p><p name="ec87" id="ec87" class="graf graf--p graf-after--p">c. The sum of squares change a lot in the type 1 ANOVA test.</p><ul class="postList"><li name="cafe" id="cafe" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Detection</strong></li></ul><p name="3510" id="3510" class="graf graf--p graf-after--li">a. <strong class="markup--strong markup--p-strong">Pairwise Correlation Plot</strong>: Heatmap correlation plot between predictors</p><p name="0b21" id="0b21" class="graf graf--p graf-after--p">b. <strong class="markup--strong markup--p-strong">Variance Inflation Factor (VIF)</strong>: measures how much the variance is inflated in the coefficient estimates caused by an independent variable xj</p><figure name="e45d" id="e45d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*C8RzBD3mfZewQ8b0EAYvhw.png" data-width="1090" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*C8RzBD3mfZewQ8b0EAYvhw.png"></figure><p name="e3a5" id="e3a5" class="graf graf--p graf-after--figure">Where Rj² is from the model xj ~ x1 + x2 +… + x(j-1) + x(j+1) +… + xk.</p><p name="63f1" id="63f1" class="graf graf--p graf-after--p">This measures how much variance is inflated by xj. Empirically, if VIF is between 1~10, then we think this inflation is okay are we are not going to think that there’s a serious multicollinearity problem in the model. However, if VIF is larger than 10, we can draw the conclusion that xj is causing a multicollinearity problem.</p><p name="28e6" id="28e6" class="graf graf--p graf-after--p">This also means that it’s not a good idea if you have an R² of more than 0.9 and you choose to add more explanatory variables into the model.</p><ul class="postList"><li name="723a" id="723a" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Solution</strong></li></ul><p name="8082" id="8082" class="graf graf--p graf-after--li">a. Fast Solution: <strong class="markup--strong markup--p-strong">Drop</strong> some highly correlated variables.</p><p name="3987" id="3987" class="graf graf--p graf-after--p">b. Seldom used: Use a <strong class="markup--strong markup--p-strong">linear combination</strong> of highly correlated variables (especially they are on the same scale).</p><p name="310d" id="310d" class="graf graf--p graf-after--p">c.<strong class="markup--strong markup--p-strong"> Regularization</strong>: the simple idea is to standardize the data so that the variance of each OLSE won’t impact too much of the general result</p><p name="4c86" id="4c86" class="graf graf--p graf-after--p">d. <strong class="markup--strong markup--p-strong">PCA</strong>: principal component analysis is actually a good way to reduce the multicollinearity problem, but it can make it hard for us to interpret the data</p><p name="1e0e" id="1e0e" class="graf graf--p graf-after--p">e. <strong class="markup--strong markup--p-strong">Partial Least-Squared Regression</strong>: just also another method, and we don’t have to understand this.</p><ul class="postList"><li name="844c" id="844c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Note</strong></li></ul><p name="8956" id="8956" class="graf graf--p graf-after--li">If dropping 1 or 2 variables, there’s still a multicollinearity problem, then just stop the dropping and leave the model there.</p><p name="b43d" id="b43d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Step 2. Fit the Initial Model</strong></p><p name="da12" id="da12" class="graf graf--p graf-after--p">This step is to fit the data in our model, as we have done before. The first model that we generate is called the initial model.</p><p name="b7c3" id="b7c3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Step 3. Check Influential Points</strong></p><ul class="postList"><li name="2859" id="2859" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Outlier</strong>: An observation has a response value yi that is very far from yi-hat</li><li name="ace2" id="ace2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">High Leverage Point</strong>: An observation that has an unusual combination of predictor values that could influence yi-hat</li><li name="a505" id="a505" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Inferential Points Problem: </strong>An observation that is both an outlier and has high leverage that impacted the fitted model significantly, is called the influential point problem.</li></ul><figure name="874f" id="874f" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*n0YMZsRzkhGXtK7fSQBQUg.png" data-width="1448" data-height="292" src="https://cdn-images-1.medium.com/max/800/1*n0YMZsRzkhGXtK7fSQBQUg.png"></figure><ul class="postList"><li name="28a4" id="28a4" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Effects of Inferential Points Problem</strong></li></ul><p name="6830" id="6830" class="graf graf--p graf-after--li">The influential points significantly affect the model estimation, because with or without these influential observations will lead to significantly different models.</p><ul class="postList"><li name="0d6a" id="0d6a" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Detection</strong></li></ul><p name="76a4" id="76a4" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">a. Detect Outliers</strong></p><p name="6b5d" id="6b5d" class="graf graf--p graf-after--p">(i) with <strong class="markup--strong markup--p-strong">Semi</strong>-<a href="https://en.wikipedia.org/wiki/Studentized_residual" data-href="https://en.wikipedia.org/wiki/Studentized_residual" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Studentized Residual</strong></a> (this is called semi because the estimated variance of ei is not MSE).</p><figure name="a723" id="a723" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_hB4toZ4ssni96pfqc8SUw.png" data-width="904" data-height="104" src="https://cdn-images-1.medium.com/max/800/1*_hB4toZ4ssni96pfqc8SUw.png"></figure><p name="9e2a" id="9e2a" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">The rule of thumb</strong> is when |ei*|&gt;3, then the point <em class="markup--em markup--p-em">i</em> is probably an outlier.</p><p name="5a80" id="5a80" class="graf graf--p graf-after--p">(ii) With<strong class="markup--strong markup--p-strong"> Internal </strong>(read) <strong class="markup--strong markup--p-strong">Studentized Residual</strong></p><p name="ed13" id="ed13" class="graf graf--p graf-after--p">Recall what we have studied for the residuals of MLR, the difference between the observed values and the fitted values is called the residual, and</p><figure name="e509" id="e509" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*EPjrbx2UPamIkWK4.png" data-width="1108" data-height="64" src="https://cdn-images-1.medium.com/max/800/0*EPjrbx2UPamIkWK4.png"></figure><p name="48dc" id="48dc" class="graf graf--p graf-after--figure">Then the variance of the residual is</p><figure name="1cf8" id="1cf8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*wbSBXqfALZCNbHlX0CH27Q.png" data-width="784" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*wbSBXqfALZCNbHlX0CH27Q.png"></figure><p name="7ada" id="7ada" class="graf graf--p graf-after--figure">Based on the assumption of normality,</p><figure name="d9fd" id="d9fd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*KLLIS4zaA9fdT8lh.png" data-width="1036" data-height="62" src="https://cdn-images-1.medium.com/max/800/0*KLLIS4zaA9fdT8lh.png"></figure><p name="f3ac" id="f3ac" class="graf graf--p graf-after--figure">then,</p><figure name="445c" id="445c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4MfgiYibktYgXXOE2bydCg.png" data-width="718" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*4MfgiYibktYgXXOE2bydCg.png"></figure><p name="29dc" id="29dc" class="graf graf--p graf-after--figure">then,</p><figure name="e35e" id="e35e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*uKY7WEgpl96F0_sHrX6maQ.png" data-width="718" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*uKY7WEgpl96F0_sHrX6maQ.png"></figure><p name="6e8d" id="6e8d" class="graf graf--p graf-after--figure">when σ² is unknown, then we can use MSE to estimate it, then,</p><figure name="4df1" id="4df1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RJx3wHqubf-gYryV1243_Q.png" data-width="718" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*RJx3wHqubf-gYryV1243_Q.png"></figure><p name="3bc0" id="3bc0" class="graf graf--p graf-after--figure">Then the internal studentized residual is,</p><figure name="5e42" id="5e42" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Pz_HcVdEFFsQcmxnn-ozGQ.png" data-width="718" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*Pz_HcVdEFFsQcmxnn-ozGQ.png"></figure><p name="a8c8" id="a8c8" class="graf graf--p graf-after--figure">(iii) With<strong class="markup--strong markup--p-strong"> External</strong> <strong class="markup--strong markup--p-strong">Studentized Residual (also detect the influence)</strong></p><p name="4166" id="4166" class="graf graf--p graf-after--p">The external studentized residual can decide both the outliers and the influence of a point, which is defined as,</p><figure name="7e85" id="7e85" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*38VK8qTp805UEWBm35t42g.png" data-width="866" data-height="124" src="https://cdn-images-1.medium.com/max/800/1*38VK8qTp805UEWBm35t42g.png"></figure><p name="a8ca" id="a8ca" class="graf graf--p graf-after--figure">Proof:</p><figure name="1fb8" id="1fb8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9R4uFZkWcI-l7tLA0EordQ.png" data-width="866" data-height="154" src="https://cdn-images-1.medium.com/max/800/1*9R4uFZkWcI-l7tLA0EordQ.png"></figure><p name="2bb9" id="2bb9" class="graf graf--p graf-after--figure">where MSE(i) is defined as the MSE of the data removing point i,</p><figure name="f0de" id="f0de" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ithyIovUCWzeHXzJxv204A.png" data-width="866" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*ithyIovUCWzeHXzJxv204A.png"></figure><p name="62d2" id="62d2" class="graf graf--p graf-after--figure">then,</p><figure name="77f2" id="77f2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*IiRyyZYFK1yoj8XNw1QvKA.png" data-width="866" data-height="124" src="https://cdn-images-1.medium.com/max/800/1*IiRyyZYFK1yoj8XNw1QvKA.png"></figure><p name="cc6d" id="cc6d" class="graf graf--p graf-after--figure">thus,</p><figure name="1ade" id="1ade" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*W1bP0HlGYQKUNeqSRjOiHA.png" data-width="866" data-height="176" src="https://cdn-images-1.medium.com/max/800/1*W1bP0HlGYQKUNeqSRjOiHA.png"></figure><p name="ec20" id="ec20" class="graf graf--p graf-after--figure">Proved.</p><p name="741a" id="741a" class="graf graf--p graf-after--p">Then to use this result to show whether a point is an influential outlier or not, this statistic of the external studentized residual<strong class="markup--strong markup--p-strong"> </strong>follows,</p><figure name="c168" id="c168" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-EDghW9jXaXd2e1lLGKJ1w.png" data-width="866" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*-EDghW9jXaXd2e1lLGKJ1w.png"></figure><p name="9ce0" id="9ce0" class="graf graf--p graf-after--figure">So when,</p><figure name="caec" id="caec" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*XszWiGKhWNzrnnF9YNISyw.png" data-width="866" data-height="78" src="https://cdn-images-1.medium.com/max/800/1*XszWiGKhWNzrnnF9YNISyw.png"></figure><p name="96c7" id="96c7" class="graf graf--p graf-after--figure">then we can detect that the i-th observation is an influential outlier with a high influence.</p><p name="0e3b" id="0e3b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">b. Detect High Leverage </strong>with the Hat Matrix</p><p name="205b" id="205b" class="graf graf--p graf-after--p">Suppose we are given the hat matrix of an MLR, then the i-th row and i-th column entity of the hat matrix is defined as the leverage of the i-th observation.</p><figure name="463c" id="463c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xk5vc3h5tDyYzWclIi_2YQ.png" data-width="904" data-height="78" src="https://cdn-images-1.medium.com/max/800/1*xk5vc3h5tDyYzWclIi_2YQ.png"></figure><p name="1745" id="1745" class="graf graf--p graf-after--figure">The properties of the leverages are,</p><p name="efa5" id="efa5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">i. Ranging between 0 and 1</strong></p><figure name="2457" id="2457" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*643fB0GlEdKI_ihw9-YH-Q.png" data-width="784" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*643fB0GlEdKI_ihw9-YH-Q.png"></figure><p name="e0cd" id="e0cd" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">ii. Trace of the hat matrix</strong></p><figure name="cb05" id="cb05" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*r3V6tqkEUcx7nG5-ljRV1Q.png" data-width="784" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*r3V6tqkEUcx7nG5-ljRV1Q.png"></figure><p name="27f1" id="27f1" class="graf graf--p graf-after--figure">Proof:</p><div name="0526" id="0526" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/adamedelwiess/linear-regression-6-sum-of-squared-errors-sum-of-squares-and-type-1-2-3-anova-for-mlr-f489146c14f8" data-href="https://medium.com/adamedelwiess/linear-regression-6-sum-of-squared-errors-sum-of-squares-and-type-1-2-3-anova-for-mlr-f489146c14f8" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/linear-regression-6-sum-of-squared-errors-sum-of-squares-and-type-1-2-3-anova-for-mlr-f489146c14f8"><strong class="markup--strong markup--mixtapeEmbed-strong">Linear Regression 6 | Sum of Squared Errors, Sum of Squares, and Type 1,2,3 ANOVA for MLR</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Linear Regression</em>medium.com</a><a href="https://medium.com/adamedelwiess/linear-regression-6-sum-of-squared-errors-sum-of-squares-and-type-1-2-3-anova-for-mlr-f489146c14f8" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="4a1456e72e77153fc7cb624ecfb34f02" data-thumbnail-img-id="1*JRXYaS8AmzV2HWueCj2mRA.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*JRXYaS8AmzV2HWueCj2mRA.png);"></a></div><p name="519a" id="519a" class="graf graf--p graf-after--mixtapeEmbed"><strong class="markup--strong markup--p-strong">iii. Average Leverage</strong></p><figure name="2dc1" id="2dc1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0oRV5v7z7NCXmvqfRWQpuw.png" data-width="784" data-height="78" src="https://cdn-images-1.medium.com/max/800/1*0oRV5v7z7NCXmvqfRWQpuw.png"></figure><p name="7642" id="7642" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">iv. Rule of Thumb</strong></p><p name="f735" id="f735" class="graf graf--p graf-after--p">An observation with,</p><figure name="2fc3" id="2fc3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DqlqXvOPxvza9O0g46zHuQ.png" data-width="976" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*DqlqXvOPxvza9O0g46zHuQ.png"></figure><p name="eff9" id="eff9" class="graf graf--p graf-after--figure">can be considered as the high leverage points.</p><p name="d066" id="d066" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">c. Detect High Leveraged Outliers (Influential) </strong>with <strong class="markup--strong markup--p-strong">Cook’s Distance</strong></p><p name="0d7d" id="0d7d" class="graf graf--p graf-after--p">We don’t have to fully understand this, instead, we have to remember the conclusion of the Cook’s Distance and how it can be used to detect the influential outliers.</p><p name="7228" id="7228" class="graf graf--p graf-after--p">The Cook’s distance for the i-th observation is defined as,</p><figure name="79aa" id="79aa" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ptX17hypFuAIvSMZZL_uVA.png" data-width="866" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*ptX17hypFuAIvSMZZL_uVA.png"></figure><p name="314d" id="314d" class="graf graf--p graf-after--figure">where,</p><p name="1947" id="1947" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">yj </em>is the <em class="markup--em markup--p-em">j</em>th fitted response value and <em class="markup--em markup--p-em">yj</em>(<em class="markup--em markup--p-em">i</em>) is the <em class="markup--em markup--p-em">j</em>th fitted response value, where the fit does not include observation <em class="markup--em markup--p-em">i.</em></p><p name="89d9" id="89d9" class="graf graf--p graf-after--p">Another expression (with the same value) of Cook’s distance is,</p><figure name="02b9" id="02b9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*e1kRzkBJ2SI7P5Xnxh2Syg.png" data-width="866" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*e1kRzkBJ2SI7P5Xnxh2Syg.png"></figure><p name="1c9d" id="1c9d" class="graf graf--p graf-after--figure">A more detailed explanation of the Cook’s distance can be found here,</p><div name="d6a4" id="d6a4" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@lymielynn/a-little-closer-to-cooks-distance-e8cc923a3250" data-href="https://medium.com/@lymielynn/a-little-closer-to-cooks-distance-e8cc923a3250" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@lymielynn/a-little-closer-to-cooks-distance-e8cc923a3250"><strong class="markup--strong markup--mixtapeEmbed-strong">A little closer to Cook’s distance</strong><br><em class="markup--em markup--mixtapeEmbed-em">How outliers can affect your model performance?</em>medium.com</a><a href="https://medium.com/@lymielynn/a-little-closer-to-cooks-distance-e8cc923a3250" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="474b070e463f9f73fed3c38e1f7b603c" data-thumbnail-img-id="0*Zi5ISEnPAPYw420z.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*Zi5ISEnPAPYw420z.png);"></a></div><p name="74c6" id="74c6" class="graf graf--p graf-after--mixtapeEmbed">The rule of thumbs with the Cook’s distance is that the observation is an influential outlier when <strong class="markup--strong markup--p-strong">Di &gt; 4/n</strong>, where n is the number of total observations.</p><p name="f932" id="f932" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) Step 4. Check Heteroscedasticity</strong></p><ul class="postList"><li name="57f3" id="57f3" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Reason</strong>: We have to maintain the model assumption of the constant variance σ² for all error terms.</li><li name="1416" id="1416" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Heteroscedasticity</strong>: occurs when this assumption is violated, which means the variance of errors are non-constant. More commonly, it refers to the spread of the residual changes</li><li name="1d3f" id="1d3f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Symptom</strong>:</li></ul><p name="5418" id="5418" class="graf graf--p graf-after--li">a. The OLS estimate <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">β</em></strong> is still linear and unbiased, but it is not the best estimate any more. We can not say <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">β</em></strong> has the smallest variance among all unbiased linear estimators. There’s another estimator with a smaller variance.</p><p name="35ab" id="35ab" class="graf graf--p graf-after--p">b. The standard error of βk-hat for all OLS output are incorrect estimate of the standard deviation of βk-hat. It will result in a misleading t-test and a misleading confidence interval.</p><p name="5049" id="5049" class="graf graf--p graf-after--p">c. Predictions of y are still unbiased, but the prediction intervales are incorrect.</p><p name="0afb" id="0afb" class="graf graf--p graf-after--p">d. <strong class="markup--strong markup--p-strong">Variance-Covariance Matrix of the OLS Estimators</strong></p><p name="b486" id="b486" class="graf graf--p graf-after--p">Recall the variance-covariance matrix of OLSE for MLR,</p><figure name="ddc7" id="ddc7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*WtUIgbSo_cKKucCk.png" data-width="984" data-height="62" src="https://cdn-images-1.medium.com/max/800/0*WtUIgbSo_cKKucCk.png"></figure><p name="7cd2" id="7cd2" class="graf graf--p graf-after--figure">When, heteroscedasticity exists, the real variance of <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">β-hat</em></strong> is,</p><figure name="3f09" id="3f09" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*em5DsHxkpMOLxDmJmvJVoA.png" data-width="866" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*em5DsHxkpMOLxDmJmvJVoA.png"></figure><p name="72bc" id="72bc" class="graf graf--p graf-after--figure">then, because the constant variance assumption no longer holds, then,</p><figure name="82f4" id="82f4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lc4QzNH7kIJkMx6Z1u6WvQ.png" data-width="866" data-height="178" src="https://cdn-images-1.medium.com/max/800/1*lc4QzNH7kIJkMx6Z1u6WvQ.png"></figure><p name="5413" id="5413" class="graf graf--p graf-after--figure">thus,</p><figure name="eed8" id="eed8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*GHnMM3jSZYU-xRH-P2iZcg.png" data-width="866" data-height="186" src="https://cdn-images-1.medium.com/max/800/1*GHnMM3jSZYU-xRH-P2iZcg.png"></figure><p name="5451" id="5451" class="graf graf--p graf-after--figure">so the estimate of,</p><figure name="1dae" id="1dae" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*svPy_yKO2rbCbjF9Uusbwg.png" data-width="866" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*svPy_yKO2rbCbjF9Uusbwg.png"></figure><p name="99ef" id="99ef" class="graf graf--p graf-after--figure">Then, if we keep applying the OLS method,</p><p name="3eef" id="3eef" class="graf graf--p graf-after--p">i. <strong class="markup--strong markup--p-strong">Not the Best</strong> Estimator</p><figure name="f284" id="f284" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*inAizIhMP23jree5xxrx1A.png" data-width="866" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*inAizIhMP23jree5xxrx1A.png"></figure><p name="12ce" id="12ce" class="graf graf--p graf-after--figure">ii. <strong class="markup--strong markup--p-strong">Biased</strong> Estimator</p><figure name="76ae" id="76ae" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*D0GD2Fz4Ae83vbBLGlSFLA.png" data-width="866" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*D0GD2Fz4Ae83vbBLGlSFLA.png"></figure><p name="aaa8" id="aaa8" class="graf graf--p graf-after--figure">is now a biased estimator and the standard error of the β-hat is incorrect.</p><p name="46c7" id="46c7" class="graf graf--p graf-after--p">iii. The Predictions and the Confidence Intervals will be Incorrect</p><p name="1965" id="1965" class="graf graf--p graf-after--p">Recall what we have learned for the variance of the fitted value,</p><div name="f124" id="f124" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/adamedelwiess/linear-regression-5-mlr-hat-matrix-and-mlr-ols-evaluation-b91eeebca210" data-href="https://medium.com/adamedelwiess/linear-regression-5-mlr-hat-matrix-and-mlr-ols-evaluation-b91eeebca210" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/adamedelwiess/linear-regression-5-mlr-hat-matrix-and-mlr-ols-evaluation-b91eeebca210"><strong class="markup--strong markup--mixtapeEmbed-strong">Linear Regression 5 | MLR, Hat Matrix, and MLR-OLS Evaluation</strong><br><em class="markup--em markup--mixtapeEmbed-em">Series: Linear Regression</em>medium.com</a><a href="https://medium.com/adamedelwiess/linear-regression-5-mlr-hat-matrix-and-mlr-ols-evaluation-b91eeebca210" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="d3de28cc0797ec0f25d2e11740e4ab0a" data-thumbnail-img-id="1*JRXYaS8AmzV2HWueCj2mRA.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*JRXYaS8AmzV2HWueCj2mRA.png);"></a></div><p name="8f48" id="8f48" class="graf graf--p graf-after--mixtapeEmbed">When there’s no heteroscedasticity,</p><figure name="c1fd" id="c1fd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*CXHjdIGedEi1L3DQ.png" data-width="984" data-height="64" src="https://cdn-images-1.medium.com/max/800/0*CXHjdIGedEi1L3DQ.png"></figure><p name="2477" id="2477" class="graf graf--p graf-after--figure">Then the confidence interval for the observation i is,</p><figure name="58dd" id="58dd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jTl86r3X5Gu0fguJQ4wuAw.png" data-width="866" data-height="88" src="https://cdn-images-1.medium.com/max/800/1*jTl86r3X5Gu0fguJQ4wuAw.png"></figure><p name="50ab" id="50ab" class="graf graf--p graf-after--figure">where, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">xi</em></strong> is the transpose of the i-th row in the matrix X.</p><p name="2680" id="2680" class="graf graf--p graf-after--p">also, the prediction interval for the observation i is,</p><figure name="3c9f" id="3c9f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WWK5ysEci96QPQjcJjht7A.png" data-width="866" data-height="78" src="https://cdn-images-1.medium.com/max/800/1*WWK5ysEci96QPQjcJjht7A.png"></figure><p name="838f" id="838f" class="graf graf--p graf-after--figure">However, when heteroscedasticity exists,</p><figure name="e55f" id="e55f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*aUfqosYBKyTMYK6ffAk-5g.png" data-width="1206" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*aUfqosYBKyTMYK6ffAk-5g.png"></figure><p name="76b0" id="76b0" class="graf graf--p graf-after--figure">But, because</p><figure name="fd19" id="fd19" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*khuXU7BpTjPsCCibp5c6AQ.png" data-width="866" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*khuXU7BpTjPsCCibp5c6AQ.png"></figure><p name="91dd" id="91dd" class="graf graf--p graf-after--figure">So,</p><figure name="2e08" id="2e08" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*L3g6EajbdOaeMPbctCIJ5A.png" data-width="1046" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*L3g6EajbdOaeMPbctCIJ5A.png"></figure><p name="5575" id="5575" class="graf graf--p graf-after--figure">Therefore, the prediction / confidence interval calculate by OLS estimator would be incorrect.</p><ul class="postList"><li name="2c06" id="2c06" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Detection</strong></li></ul><p name="ad7c" id="ad7c" class="graf graf--p graf-after--li">a. Residual vs. Fitted Value Plot</p><p name="2717" id="2717" class="graf graf--p graf-after--p">The model will be observed obvious change of the bandwidth in the plot with the given data if there’s a heteroscedasticity problem.</p><figure name="1b07" id="1b07" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Eox5Dt6hw7EHorwoKZLp-Q.png" data-width="1230" data-height="412" src="https://cdn-images-1.medium.com/max/800/1*Eox5Dt6hw7EHorwoKZLp-Q.png"></figure><p name="dc74" id="dc74" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">b. Breusch-Pagan Test (BP Test)</strong></p><p name="fd7d" id="fd7d" class="graf graf--p graf-after--p">The basic idea of this test is that the variance of the errors should not change given difference predictor values. If the assumption of constant variance is violated, then the variance would be changed with predictor values.</p><p name="2e0b" id="2e0b" class="graf graf--p graf-after--p">i. Process of the Breusch-Pagan Test</p><p name="1b97" id="1b97" class="graf graf--p graf-after--p">Step 1. Fit the MLR model.</p><figure name="2c6b" id="2c6b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WgyQqqtHkrNYD6wQE-MBEQ.png" data-width="760" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*WgyQqqtHkrNYD6wQE-MBEQ.png"></figure><p name="db4f" id="db4f" class="graf graf--p graf-after--figure">Step 2. Obtain the residual.</p><figure name="fed7" id="fed7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*LnMVfbJYCKNwUkY2E5L4Uw.png" data-width="760" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*LnMVfbJYCKNwUkY2E5L4Uw.png"></figure><p name="4e72" id="4e72" class="graf graf--p graf-after--figure">Step 3. Build an auxiliary regression model (this is called auxiliary because it does not estimate the model of primary interest)</p><figure name="356e" id="356e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qwKsJxIcCoVQb9qfRvqhoA.png" data-width="1060" data-height="86" src="https://cdn-images-1.medium.com/max/800/1*qwKsJxIcCoVQb9qfRvqhoA.png"></figure><p name="4aef" id="4aef" class="graf graf--p graf-after--figure">And obtain R² value of this model.</p><p name="0038" id="0038" class="graf graf--p graf-after--p">Step 4. Hypothesis Testing</p><figure name="2d0d" id="2d0d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*La-qvDGXxOUiIHcdwQijZQ.png" data-width="860" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*La-qvDGXxOUiIHcdwQijZQ.png"></figure><p name="9ff6" id="9ff6" class="graf graf--p graf-after--figure">The testing statistic is,</p><figure name="1b29" id="1b29" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Pjmymay52FvMv-JCIpQTgA.png" data-width="860" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*Pjmymay52FvMv-JCIpQTgA.png"></figure><p name="220e" id="220e" class="graf graf--p graf-after--figure">The testing result is,</p><p name="ae96" id="ae96" class="graf graf--p graf-after--p">when fail to reject H0,</p><figure name="19e0" id="19e0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UaO4ndq2-4YuMk6qRLLuDg.png" data-width="860" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*UaO4ndq2-4YuMk6qRLLuDg.png"></figure><p name="677d" id="677d" class="graf graf--p graf-after--figure">then we can conclude that there’s no significant heteroscedasticity problem at the 100(1-α)% confidence level.</p><p name="e4f9" id="e4f9" class="graf graf--p graf-after--p">when reject H0,</p><figure name="ed94" id="ed94" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nckG7P8p7zIwgmo4SE0Ang.png" data-width="860" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*nckG7P8p7zIwgmo4SE0Ang.png"></figure><p name="d170" id="d170" class="graf graf--p graf-after--figure">then we can conclude that there’s a significant heteroscedasticity problem at the 100(1-α)% confidence level.</p><p name="6e4f" id="6e4f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">c. White Test </strong>(A more advanced BP test)</p><p name="47c0" id="47c0" class="graf graf--p graf-after--p">We will not cover this now, maybe later.</p><ul class="postList"><li name="21b8" id="21b8" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Solution</strong></li></ul><p name="f784" id="f784" class="graf graf--p graf-after--li">a. <strong class="markup--strong markup--p-strong">Log-Transformation</strong> on y: The natural log with <em class="markup--em markup--p-em">e</em> as its base is the most common transformation in MLR. This will be working because the small values that are close together are spread further out and the large values that are spread out are brought close to each other. See an example of this,</p><figure name="f14a" id="f14a" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/c60d4826143eb5c061248d82c990876b.js"></script></figure><figure name="aa4d" id="aa4d" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*a_v_SMmjusm5jR7whwFpSw.png" data-width="892" data-height="378" src="https://cdn-images-1.medium.com/max/800/1*a_v_SMmjusm5jR7whwFpSw.png"></figure><p name="05c6" id="05c6" class="graf graf--p graf-after--figure">Now, let’s download this <a href="https://statisticsbyjim.com/wp-content/uploads/2017/08/Heteroscedasticity.csv" data-href="https://statisticsbyjim.com/wp-content/uploads/2017/08/Heteroscedasticity.csv" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">dataset</a>. We’ll use Accident as the dependent variable and Population for the independent variable. Then, the following program is to draw a residual vs. fitted value plot,</p><figure name="24c2" id="24c2" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/07b0e0c44d46b84332c037cca07289b4.js"></script></figure><figure name="647e" id="647e" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*mK9-sOSIlbwVLOTGNyAHrg.png" data-width="838" data-height="364" src="https://cdn-images-1.medium.com/max/800/1*mK9-sOSIlbwVLOTGNyAHrg.png"></figure><p name="7ca2" id="7ca2" class="graf graf--p graf-after--figure">We can detect a heteroscedasticity problem in this model. Then we conduct the log transformation on the dependent variable and draw another residual vs. fitted value plot,</p><figure name="9356" id="9356" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/34f8367eb265834a71f0e0f4408a033a.js"></script></figure><figure name="f9ee" id="f9ee" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*io3pMSmSRx9xTeBKZxNUYQ.png" data-width="860" data-height="370" src="https://cdn-images-1.medium.com/max/800/1*io3pMSmSRx9xTeBKZxNUYQ.png"></figure><p name="135d" id="135d" class="graf graf--p graf-after--figure">Now we can conclude that there’s no heteroscedasticity problem in this model based on this plot. However, there seems to be a linearity problem (non-linear) in this model. In order to eliminate this non-linear effect, we have to conduct an exponential calculation on the independent variable (we are going to explain in details later).</p><figure name="35d5" id="35d5" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/5dbd21340dd45b226dd0422852a7df0c.js"></script></figure><figure name="eed4" id="eed4" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*6wyNLigIaS7yfcZKHpbJKQ.png" data-width="932" data-height="360" src="https://cdn-images-1.medium.com/max/800/1*6wyNLigIaS7yfcZKHpbJKQ.png"></figure><p name="8a0e" id="8a0e" class="graf graf--p graf-after--figure">Let’s review what we have talked about by a brief summary,</p><figure name="f933" id="f933" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*K41xH1LiUUuTNkDsqbkRtA.png" data-width="1508" data-height="356" src="https://cdn-images-1.medium.com/max/800/1*K41xH1LiUUuTNkDsqbkRtA.png"></figure><p name="8be9" id="8be9" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">b. Weighted Regression</strong></p><p name="66a1" id="66a1" class="graf graf--p graf-after--p">Although the log transformation is easy to use, however, it has to be used only when the residual vs. fitted value plot has a “Funnel” shape. This is a relatively strong condition and you can image that most of the heteroscedasticity problems we meet can not satisfy this feature. Therefore, we have generate a more general solution for the heteroscedasticity problem. This general solution is called the weight regression (or the weighted least square regression).</p><figure name="2d69" id="2d69" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*wir3tMR6N-mD45COlFSebw.png" data-width="1170" data-height="680" src="https://cdn-images-1.medium.com/max/800/1*wir3tMR6N-mD45COlFSebw.png"></figure><p name="e3c5" id="e3c5" class="graf graf--p graf-after--figure">For a heteroscedasticity problem, we can conclude that,</p><figure name="df65" id="df65" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-62A8BZaL4Mc5SmiS35bqQ.png" data-width="940" data-height="192" src="https://cdn-images-1.medium.com/max/800/1*-62A8BZaL4Mc5SmiS35bqQ.png"></figure><p name="3955" id="3955" class="graf graf--p graf-after--figure">Suppose we define Σ as,</p><figure name="8308" id="8308" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AHky6Qtrb324vRBjDha0Dw.png" data-width="940" data-height="192" src="https://cdn-images-1.medium.com/max/800/1*AHky6Qtrb324vRBjDha0Dw.png"></figure><p name="f0e7" id="f0e7" class="graf graf--p graf-after--figure">then,</p><figure name="afe0" id="afe0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5qUGoPd_kAPmn_dfVdDLXQ.png" data-width="940" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*5qUGoPd_kAPmn_dfVdDLXQ.png"></figure><p name="3907" id="3907" class="graf graf--p graf-after--figure">Suppose we define a weight matrix,</p><figure name="ab47" id="ab47" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9nRsVOh_-P2pVSyxptUhqg.png" data-width="794" data-height="212" src="https://cdn-images-1.medium.com/max/800/1*9nRsVOh_-P2pVSyxptUhqg.png"></figure><p name="6ae7" id="6ae7" class="graf graf--p graf-after--figure">then the OLSE should be,</p><figure name="f217" id="f217" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mhSeO74ZbVvSAkhR_z-wHg.png" data-width="794" data-height="76" src="https://cdn-images-1.medium.com/max/800/1*mhSeO74ZbVvSAkhR_z-wHg.png"></figure><p name="6203" id="6203" class="graf graf--p graf-after--figure">But how we can get the estimator W-hat of the weight matrix W? This is a tricky problem. A specific case of determining the value as follows,</p><p name="4341" id="4341" class="graf graf--p graf-after--p">When a plot of the residual against a predictor has a funnel shape, then we can fit the model of “ei² ~ xt”. Then use the fitted values of the model as the estimated variance. The example implementation code is,</p><figure name="5e6d" id="5e6d" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/Sadamingh/7406669249c358e114da879e7c8cae2b.js"></script></figure><p name="e1ac" id="e1ac" class="graf graf--p graf-after--figure">c. Other Solutions (Not in details): generalized least squares, robust regression, or conducting the non-normality and non-linear solutions in the first place (sometimes, this can improve the heteroscedasticity problem as well).</p><p name="6f55" id="6f55" class="graf graf--p graf-after--p">Note that here’re the other topics that we are going to cover in the part 2:</p><ul class="postList"><li name="b708" id="b708" class="graf graf--li graf-after--p">(5) Step 5. Check Normality</li><li name="9f5c" id="9f5c" class="graf graf--li graf-after--li">(6) Step 6. Check Linearity</li><li name="016f" id="016f" class="graf graf--li graf-after--li">(7) Step 7. Modify the initial model and fit the data again</li><li name="44b6" id="44b6" class="graf graf--li graf-after--li">(8) Step 8. Best Subset Model Selection: based on adjusted R²</li><li name="ea5c" id="ea5c" class="graf graf--li graf-after--li">(9) Step 9. Step-wise Model Selection: based on t-test</li><li name="173e" id="173e" class="graf graf--li graf-after--li graf--trailing">(10) Step 10. AIC/BIC Method</li></ul></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/205c1f71d581"><time class="dt-published" datetime="2020-11-16T09:27:18.785Z">November 16, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/linear-regression-9-model-diagnosis-process-for-mlr-part-1-205c1f71d581" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>