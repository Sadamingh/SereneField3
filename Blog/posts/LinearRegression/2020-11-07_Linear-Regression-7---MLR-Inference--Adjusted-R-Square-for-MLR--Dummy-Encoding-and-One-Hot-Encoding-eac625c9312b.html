<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Linear Regression 7 | MLR Inference, Adjusted R-Square for MLR, Dummy Encoding and One-Hot Encoding</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Linear Regression 7 | MLR Inference, Adjusted R-Square for MLR, Dummy Encoding and One-Hot Encoding</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Linear Regression
</section>
<section data-field="body" class="e-content">
<section name="e5da" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="5629" id="5629" class="graf graf--h3 graf--leading graf--title">Linear Regression 7 | <strong class="markup--strong markup--h3-strong">MLR Inference, Adjusted R-Square for MLR, Dummy Encoding and One-Hot Encoding</strong></h3><figure name="6dbd" id="6dbd" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*JRXYaS8AmzV2HWueCj2mRA.png" data-width="1514" data-height="716" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*JRXYaS8AmzV2HWueCj2mRA.png"></figure><ol class="postList"><li name="c15b" id="c15b" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">MLR Inference</strong></li></ol><p name="ec5c" id="ec5c" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) F Statistics</strong></p><p name="bcbb" id="bcbb" class="graf graf--p graf-after--p">As what we have discussed, similarly, the F statistics for MLR is defined by,</p><figure name="36e2" id="36e2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VX2xSxTNd5q1_-fg89jjAA.png" data-width="926" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*VX2xSxTNd5q1_-fg89jjAA.png"></figure><p name="3ede" id="3ede" class="graf graf--p graf-after--figure">Suppose we have a more general case that is for some of the terms in a linear model, we want to test whether there’s at least one parameter not equal to 0. Denote the full model as ‘<strong class="markup--strong markup--p-strong">F</strong>’ and the partial model as ‘<strong class="markup--strong markup--p-strong">R</strong>’, then we have the statistical inference as,</p><figure name="bb19" id="bb19" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gmReH-FpZkyM3gYyAw-CRg.png" data-width="926" data-height="64" src="https://cdn-images-1.medium.com/max/800/1*gmReH-FpZkyM3gYyAw-CRg.png"></figure><p name="b027" id="b027" class="graf graf--p graf-after--figure">Note that this is not strictly correct because the H1 hypothesis is no need to be F, it can actually be something between R and F (not including R).</p><p name="b511" id="b511" class="graf graf--p graf-after--p">And the F statistics can be written in a more general way as,</p><figure name="624c" id="624c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zcnU2zsF82Mqhfb3gc5xvg.png" data-width="926" data-height="108" src="https://cdn-images-1.medium.com/max/800/1*zcnU2zsF82Mqhfb3gc5xvg.png"></figure><p name="d6a6" id="d6a6" class="graf graf--p graf-after--figure">It is also clear to see that when we want to test all the terms except for the error term in an MLR model, then,</p><figure name="1d36" id="1d36" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WFTK20eitbUNbXXOXAfO-g.png" data-width="926" data-height="108" src="https://cdn-images-1.medium.com/max/800/1*WFTK20eitbUNbXXOXAfO-g.png"></figure><p name="f30f" id="f30f" class="graf graf--p graf-after--figure">this is also,</p><figure name="358a" id="358a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*TBa9NlrAy5kBETE0eFZGNA.png" data-width="926" data-height="108" src="https://cdn-images-1.medium.com/max/800/1*TBa9NlrAy5kBETE0eFZGNA.png"></figure><p name="6635" id="6635" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Student’s T Statistics</strong></p><p name="6403" id="6403" class="graf graf--p graf-after--p">Based on our previous discussions,</p><figure name="5f3f" id="5f3f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*96Y9diCVvK3xdYWBwYwOug.png" data-width="934" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*96Y9diCVvK3xdYWBwYwOug.png"></figure><p name="71d2" id="71d2" class="graf graf--p graf-after--figure">Similarly to SLR, the t-test for the j-th individual coefficient is,</p><figure name="4698" id="4698" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*e17sPgeQkAgAsIJw533r9g.png" data-width="868" data-height="66" src="https://cdn-images-1.medium.com/max/800/1*e17sPgeQkAgAsIJw533r9g.png"></figure><p name="0a24" id="0a24" class="graf graf--p graf-after--figure">then, when σ² is known,</p><figure name="ee9f" id="ee9f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*r6_Ch42T6vohTd34MUPoOw.png" data-width="934" data-height="122" src="https://cdn-images-1.medium.com/max/800/1*r6_Ch42T6vohTd34MUPoOw.png"></figure><p name="f89a" id="f89a" class="graf graf--p graf-after--figure">when σ² is unknown,</p><figure name="d79e" id="d79e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jWTeEJUWtUGtO2PhdR0czw.png" data-width="868" data-height="116" src="https://cdn-images-1.medium.com/max/800/1*jWTeEJUWtUGtO2PhdR0czw.png"></figure><p name="c2d4" id="c2d4" class="graf graf--p graf-after--figure">where,</p><figure name="9437" id="9437" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HKtNQxAdSy5t5CAdvfGQRQ.png" data-width="868" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*HKtNQxAdSy5t5CAdvfGQRQ.png"></figure><p name="dd1c" id="dd1c" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) R²</strong></p><p name="f259" id="f259" class="graf graf--p graf-after--p">Similarly to SLR, the definition of R² is,</p><figure name="1fbc" id="1fbc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vi7NSca0Kn_T3vWWGLi3uA.png" data-width="926" data-height="92" src="https://cdn-images-1.medium.com/max/800/1*vi7NSca0Kn_T3vWWGLi3uA.png"></figure><p name="e8f7" id="e8f7" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) A Misunderstanding for R²: The Underfitting Problem</strong></p><p name="d35c" id="d35c" class="graf graf--p graf-after--p">Sometimes we generate a model and find out all the parameters are significant at the 95% level. However, it is the low R² makes us nervous about our final results. For example, we may have an R² = 2% which indicates that the model is really bad and there are also many other explanatory variables not being included. This is commonly called an <strong class="markup--strong markup--p-strong">underfitted model</strong> or an underfitting problem.</p><p name="a5cf" id="a5cf" class="graf graf--p graf-after--p">But to have an underfitted model doesn’t mean that our model is not significant. Although it can be really bad for prediction, it can actually include some critically significant variables in the model. Many people misunderstand the meaning of the R² and they change the explanatory variables after they have a low R². Well, to some extent, this can work, but a wiser idea is to add more significant explanatory variables into the model until you a higher R². But if you don’t want to make predictions or if the full model is not so important for you, it’s okay if you report a low R².</p><p name="dc12" id="dc12" class="graf graf--p graf-after--p">But it can also be a really bad idea if we have too many explanatory variables in the model and get a really big R². This is the overfitting problem that we are going to introduce.</p><p name="857a" id="857a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) The Overfitting Problems for R²</strong></p><p name="f12a" id="f12a" class="graf graf--p graf-after--p">Let’s think about a problem, if I keep adding significant explanatory variables into the linear model, what can we finally have? Actually, we are going to have a model with a higher and higher R² and this is nothing strange by the definition of the SSR (we are not going to prove this here but you can have a try).</p><p name="1983" id="1983" class="graf graf--p graf-after--p">With more explanatory variables (whether or not they are significant or not) being added, we can find that is harder to tell the relationship between these newly added variables and the dependent variable. And it can also appear that even though our model more and more fits our model, it will become really bad for prediction because the model is overly explained by the redundant variables and it randomly tells the story about the noise in the data set. That’s why this model is bad if we have an overfitting problem.</p><figure name="e36a" id="e36a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jaAE2MoCc1LYKva0sOHEIQ.png" data-width="1144" data-height="258" src="https://cdn-images-1.medium.com/max/800/1*jaAE2MoCc1LYKva0sOHEIQ.png"></figure><p name="5fcb" id="5fcb" class="graf graf--p graf-after--figure">However, can R² tells us that there’s an overfitting problem in the model? No, because the R² will be larger and it implies that the dataset is well explained by the model. So our question is that, is R² a good measure for an MLR model? Probably NOT when there’s an overfitting problem! Because we can actually know nothing about whether it is overfitting or not if we use only R² as our measure. That’s why we have to find a new measure for our model.</p><p name="22a2" id="22a2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) The Definition of the Adjusted R²</strong></p><p name="e511" id="e511" class="graf graf--p graf-after--p">The adjusted R² is an alternative to R² and it tells us whether or not this is an existing overfitting problem in the model. How come? The problem when we are adding new variables to the model is that we actually not only add the variance of the regression but also we reduce the degree of freedom. There are to ways that we can have a higher R² based on its following definition,</p><figure name="3d90" id="3d90" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*bW500oDLaksep2HaeWesgQ.png" data-width="934" data-height="104" src="https://cdn-images-1.medium.com/max/800/1*bW500oDLaksep2HaeWesgQ.png"></figure><ul class="postList"><li name="57d4" id="57d4" class="graf graf--li graf-after--figure">get a higher MSR =&gt; what we want</li><li name="4944" id="4944" class="graf graf--li graf-after--li">get a higher <em class="markup--em markup--li-em">k</em> =&gt; to add more explanatory variables and the model can become overfitting</li></ul><p name="90e3" id="90e3" class="graf graf--p graf-after--li">Suppose we want to make a new measure that will not count the degree of freedom so that a higher <em class="markup--em markup--p-em">k</em> won’t affect our measure to some extent. There can be two ways to construct this measure. Then the definition of the adjusted R² is defined by,</p><figure name="402d" id="402d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-6e0IfBq9uw67_dKvcGgzQ.png" data-width="934" data-height="104" src="https://cdn-images-1.medium.com/max/800/1*-6e0IfBq9uw67_dKvcGgzQ.png"></figure><p name="3336" id="3336" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(6) Increasing Adjusted R² Or Decreasing Adjusted R²</strong></p><p name="3695" id="3695" class="graf graf--p graf-after--p">There are two cases when the adjusted R² will be increasing (or decreasing),</p><p name="23e0" id="23e0" class="graf graf--p graf-after--p">First, adding a regressor will increase (decrease) adjusted R² depending on whether the absolute value of the t-statistic associated with that regressor is greater (less) than one in value. Adjusted R² is unchanged if that absolute t-statistic is exactly equal to one. If you drop a regressor from the model, the converse of the above result applies.</p><p name="131c" id="131c" class="graf graf--p graf-after--p">Second, adding a group of regressors to the model will increase (decrease) adjusted R² depending on whether the F-statistic for testing that their coefficients are all zero is greater (less) than one in value. Adjusted R² is unchanged if that F-statistic is exactly equal to one. If you drop a group of regressors from the model, the converse of the above result applies.</p><p name="c3b5" id="c3b5" class="graf graf--p graf-after--p">There are religious proofs for these two cases and you can refer to <a href="https://davegiles.blogspot.com/2013/07/the-adjusted-r-squared-again.html" data-href="https://davegiles.blogspot.com/2013/07/the-adjusted-r-squared-again.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">click me</a> and <a href="https://davegiles.blogspot.com/2013/05/when-will-adjusted-r-squared-increase.html" data-href="https://davegiles.blogspot.com/2013/05/when-will-adjusted-r-squared-increase.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">click me</a> for more information.</p><p name="8b7a" id="8b7a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Negative Adjusted R²</strong></p><p name="c0d8" id="c0d8" class="graf graf--p graf-after--p">Surprisingly, the adjusted R² can be negative. This is because it is actually not a must for our measure to be positive if the negative values are not too large and we can also have 1 as our upper bound of the method.</p><p name="df74" id="df74" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Dummy Variables in MLR</strong></p><p name="e585" id="e585" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of the Category Variable</strong></p><p name="5fea" id="5fea" class="graf graf--p graf-after--p">The category variables are variables that take a limited and fixed number of possible non-ordered values. For example, gender is a kind of category variable that takes the non-ordered value of female or male.</p><p name="cedf" id="cedf" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Definition of the Dummy Variable</strong></p><p name="9b09" id="9b09" class="graf graf--p graf-after--p">The dummy variable is a kind of variable that only takes only the value 1 and 0. And the meaning of 0 is that we haven’t got this effect and 1 is that we have this effect. There’s no order between 0 and 1.</p><p name="9195" id="9195" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Dummy Encoding</strong></p><p name="045e" id="045e" class="graf graf--p graf-after--p">Dummy encoding means to code a category variable with <em class="markup--em markup--p-em">k</em> different values to <em class="markup--em markup--p-em">k</em>-1 dummy variables. We can not set the values of 0, 1, 2, … because this will imply the order of the category value.</p><figure name="a86c" id="a86c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fTiac8R-v8hk0ca--OCO9A.png" data-width="1210" data-height="334" src="https://cdn-images-1.medium.com/max/800/1*fTiac8R-v8hk0ca--OCO9A.png"></figure><p name="fb08" id="fb08" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) One-Hot Encoding</strong></p><p name="9b6c" id="9b6c" class="graf graf--p graf-after--p">Dummy encoding means to code a category variable with <em class="markup--em markup--p-em">k</em> different values to <em class="markup--em markup--p-em">k</em> dummy variables. We can not set the values of 0, 1, 2, … because this will imply the order of the category value.</p><figure name="1903" id="1903" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ck3r0CzpckCidfrPuQ5FgA.png" data-width="1210" data-height="334" src="https://cdn-images-1.medium.com/max/800/1*Ck3r0CzpckCidfrPuQ5FgA.png"></figure><p name="bb43" id="bb43" class="graf graf--p graf-after--figure">Note we are not going to focus on the one-hot encoding in this section.</p><p name="b7a4" id="b7a4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) The Definition of the Baseline</strong></p><p name="da0d" id="da0d" class="graf graf--p graf-after--p">The baseline value is the value that after a dummy encoding process, the value of the total when all dummy variables equal to zero. Actually, the impact of the baseline value has been added to the intercept parameter. We define the baseline because, by this means, we can reduce a dummy variable.</p><figure name="6dc6" id="6dc6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rNhumDHHPUL5CB74HZ4Xaw.png" data-width="1210" data-height="678" src="https://cdn-images-1.medium.com/max/800/1*rNhumDHHPUL5CB74HZ4Xaw.png"></figure><p name="9900" id="9900" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Interpretation of the Parameter of the Dummy Variable</strong></p><p name="d13b" id="d13b" class="graf graf--p graf-after--p">Because the dummy variable only has two values 1 and 0,</p><ul class="postList"><li name="fb40" id="fb40" class="graf graf--li graf-after--p">For <strong class="markup--strong markup--li-strong">dummy encoding</strong>, the coefficient of a dummy variable in a linear model measures the <strong class="markup--strong markup--li-strong">change</strong> (or difference) caused in <em class="markup--em markup--li-em">y</em> when the observation is in the dummy level compared to the baseline level.</li><li name="8cfc" id="8cfc" class="graf graf--li graf-after--li">For <strong class="markup--strong markup--li-strong">one-hot encoding</strong>, the coefficient of a dummy variable in a linear model measures the <strong class="markup--strong markup--li-strong">impact</strong> of a dummy variable on the dependent variable.</li></ul><p name="7377" id="7377" class="graf graf--p graf-after--li">For python, by default, we are going to model with the dummy encoding method.</p><p name="1044" id="1044" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Student’s T-Test for Dummy Variable</strong></p><p name="5a2a" id="5a2a" class="graf graf--p graf-after--p">The student’s t-test for dummy encoding examines the significant difference between the other values and the baseline value (reference level). While, for one-hot encoding, it examines the impact of each value on the dependent value.</p><p name="9536" id="9536" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) ANOVA with Dummy Variable</strong></p><p name="f145" id="f145" class="graf graf--p graf-after--p">Suppose we three values (i.e. blue, red, green) of a category variable (i.e. color), and by python, this variable will automatically be dummy encoded to 2 dummy variables and the ANOVA will test on the model after the dummy encoding.</p><p name="3461" id="3461" class="graf graf--p graf-after--p">For type I ANOVA, the F statistics will tell the sequence result based on the order of the independent variables being added to the model. If F statistics for type I ANOVA is significant, that means there’s at least one of the parameters for these 2 dummy variables is not 0 given the reduced model.</p><p name="8a81" id="8a81" class="graf graf--p graf-after--p">For type II ANOVA, the F statistics will tell the partical result. If F statistics for type I ANOVA is significant, that means there’s at least one of the parameters for these 2 dummy variables is not 0 given the reduced model.</p><p name="d50f" id="d50f" class="graf graf--p graf-after--p">Note that the degree of freedom of the category variable will be the number of the values of this category variable k minus 1 (or k-1).</p><p name="1578" id="1578" class="graf graf--p graf-after--p">To force a numeric variable as a category variable in python, we can use,</p><pre name="d4a9" id="d4a9" class="graf graf--pre graf-after--p">model = smf.ols(&#39;y~C(x)&#39;, data=df).fit()</pre><p name="d966" id="d966" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">(7) Remaining Problems for Dummy and One-Hot Encoding</strong></p><p name="1bdb" id="1bdb" class="graf graf--p graf-after--p">For dummy encoding of a categorical variable, we have to add k-1 dummy variables based on the number of different categories k. For one-hot encoding, we have to add k dummy variables. Then there will be two issues because of this,</p><ul class="postList"><li name="8d33" id="8d33" class="graf graf--li graf-after--p">Multicollinearity (which we are going to introduce later)</li><li name="8364" id="8364" class="graf graf--li graf-after--li graf--trailing">Not enough data (In linear regression, we want the #of parameters &lt; the # of the observations, and it will be nice if we have # observations surpass 10~20). (which is the field of power analysis)</li></ul></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/eac625c9312b"><time class="dt-published" datetime="2020-11-07T06:05:27.525Z">November 7, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/linear-regression-7-mlr-inference-adjusted-r-square-for-mlr-dummy-encoding-and-one-hot-encoding-eac625c9312b" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>