<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Probability and Statistics 9 | Hypothesis Testing, Type I Error, Type II Error</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Probability and Statistics 9 | Hypothesis Testing, Type I Error, Type II Error</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Probability and Statistics
</section>
<section data-field="body" class="e-content">
<section name="bb4f" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="388f" id="388f" class="graf graf--h3 graf--leading graf--title">Probability and Statistics 9 | <strong class="markup--strong markup--h3-strong">Hypothesis Testing, Type I Error, Type II Error</strong></h3><figure name="3cff" id="3cff" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*YTeSIjiYirbk4CZavQXCqg.jpeg" data-width="1602" data-height="1141" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*YTeSIjiYirbk4CZavQXCqg.jpeg"></figure><ol class="postList"><li name="d77a" id="d77a" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Hypothesis Testing</strong></li></ol><p name="08ec" id="08ec" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) The Two-Sample Context De Moivre-La Place Theorem</strong></p><p name="ca18" id="ca18" class="graf graf--p graf-after--p">If we can say that,</p><figure name="a127" id="a127" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mRotOSNyjyoXb3covfUfdg.png" data-width="1398" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*mRotOSNyjyoXb3covfUfdg.png"></figure><p name="3a24" id="3a24" class="graf graf--p graf-after--figure">and,</p><figure name="dd29" id="dd29" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ynOMYV63o3u_4c12LnHf-g.png" data-width="1398" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*ynOMYV63o3u_4c12LnHf-g.png"></figure><p name="a14f" id="a14f" class="graf graf--p graf-after--figure">then given,</p><figure name="363b" id="363b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*abL9kQ_KrjBM5EGo-64KqQ.png" data-width="1398" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*abL9kQ_KrjBM5EGo-64KqQ.png"></figure><p name="90a9" id="90a9" class="graf graf--p graf-after--figure">then, if we define the pooled sample proportion as,</p><figure name="f793" id="f793" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*uEyYgw_2d09IfdZQO7zRGg.png" data-width="1398" data-height="98" src="https://cdn-images-1.medium.com/max/800/1*uEyYgw_2d09IfdZQO7zRGg.png"></figure><p name="7bd1" id="7bd1" class="graf graf--p graf-after--figure">we have De Moivre -La Place theorem in the two-sample context as,</p><figure name="d3bf" id="d3bf" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*23mXnhRfpQWkPHCwB9CCAg.png" data-width="1398" data-height="148" src="https://cdn-images-1.medium.com/max/800/1*23mXnhRfpQWkPHCwB9CCAg.png"></figure><p name="0694" id="0694" class="graf graf--p graf-after--figure">It will be wrong if we use,</p><figure name="caff" id="caff" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tsKSwcrAvQf9tGcA6PmXDg.png" data-width="1398" data-height="152" src="https://cdn-images-1.medium.com/max/800/1*tsKSwcrAvQf9tGcA6PmXDg.png"></figure><p name="77b3" id="77b3" class="graf graf--p graf-after--figure">because this can be used for calculating the confidence interval but not for the hypothesis test. This is because the two samples, in this case, are exactly drawn from the same random variable. So we have to pull them together to create a single π-cap.</p><p name="36ac" id="36ac" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Two-Sample Context De Moivre -La Place Theorem: </strong>An Example</p><p name="79cf" id="79cf" class="graf graf--p graf-after--p">For example, some students all run a marathon (literally). They are not really designed to run marathons but we want to see if there is any difference between men and women with respect to completion rates.</p><p name="bfe1" id="bfe1" class="graf graf--p graf-after--p">Suppose we have 52 men and 40 women, and the number of successes in men is 10 with the number of successes in women is 15.</p><p name="7dad" id="7dad" class="graf graf--p graf-after--p">So our hypothesis should be,</p><figure name="48eb" id="48eb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KXxMzaaREhF6UpsGJ9Xkdg.png" data-width="1398" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*KXxMzaaREhF6UpsGJ9Xkdg.png"></figure><p name="d964" id="d964" class="graf graf--p graf-after--figure">To check in both groups, the sample successes and the sample failures are ≥ 10, so it can be treated to obey the De Moivre -La Place theorem. Note that,</p><figure name="12ca" id="12ca" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gDnPVkY2KWIfTBUBBcrE-w.png" data-width="1512" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*gDnPVkY2KWIfTBUBBcrE-w.png"></figure><p name="4071" id="4071" class="graf graf--p graf-after--figure">Assumes that if we have H₀ holds, then,</p><figure name="83bb" id="83bb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dBrcX8xjWXiTK1M-uag3oQ.png" data-width="1512" data-height="180" src="https://cdn-images-1.medium.com/max/800/1*dBrcX8xjWXiTK1M-uag3oQ.png"></figure><p name="6dc9" id="6dc9" class="graf graf--p graf-after--figure">This is also to say that,</p><figure name="d89b" id="d89b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-uPiYjNkmJ-ChaNAePWNpg.png" data-width="1512" data-height="72" src="https://cdn-images-1.medium.com/max/800/1*-uPiYjNkmJ-ChaNAePWNpg.png"></figure><p name="f29f" id="f29f" class="graf graf--p graf-after--figure">if the null hypothesis H₀ is true.</p><p name="ea15" id="ea15" class="graf graf--p graf-after--p">So then basically we have two ways to test this hypothesis H₀: to use a P-value approach or to use a critical value approach.</p><p name="6af1" id="6af1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) The P-Value Approach</strong></p><p name="b864" id="b864" class="graf graf--p graf-after--p">Here in order to check the null hypothesis H₀, e can plot the following graph of the standardized normal distribution as,</p><figure name="d3fd" id="d3fd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*au88MBSsNGUwSK2DbL6JdQ.png" data-width="1312" data-height="290" src="https://cdn-images-1.medium.com/max/800/1*au88MBSsNGUwSK2DbL6JdQ.png"></figure><p name="3621" id="3621" class="graf graf--p graf-after--figure">Because 2 times .102 = .204 &gt; .05 = α, then we can conclude that there is not enough evidence at this time to reject H₀. For example, there’s not enough evidence right now to suggest different marathon completion rates.</p><p name="fa82" id="fa82" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) The Critical Value Approach</strong></p><p name="bc2c" id="bc2c" class="graf graf--p graf-after--p">Here in order to check the null hypothesis H₀, e can plot the following graph of the standardized normal distribution as,</p><figure name="1bc1" id="1bc1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3gVoXru5xmlofLPJ-uc6Tg.png" data-width="1570" data-height="320" src="https://cdn-images-1.medium.com/max/800/1*3gVoXru5xmlofLPJ-uc6Tg.png"></figure><p name="28cf" id="28cf" class="graf graf--p graf-after--figure">Because -1.96 &lt; 1.266 = Z &lt; 1.96, we do not have enough evidence at this time to reject H₀.</p><p name="ea12" id="ea12" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) Confidence Interval for Two Samples</strong></p><p name="eb3e" id="eb3e" class="graf graf--p graf-after--p">Now as we have just discussed, the endpoints of the 100(1-α)% confidence interval for the difference of two proportions is therefore given by,</p><figure name="e31e" id="e31e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RKE05ss2HW5FbbD3ThyCTA.png" data-width="1570" data-height="144" src="https://cdn-images-1.medium.com/max/800/1*RKE05ss2HW5FbbD3ThyCTA.png"></figure><p name="3fd4" id="3fd4" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">2. Fisher Transformation for Correlation Inference</strong></p><p name="0962" id="0962" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Hypothesis Testing for |ρ₀| &lt; 0.50</strong></p><p name="f715" id="f715" class="graf graf--p graf-after--p">Suppose that you have a bivariate sample {(X1, Y1), …, (Xn, Yn)} and you estimate the true population correlation coefficient ρ by way of the usual sample correlation coefficient ρ. Under the null hypothesis of</p><figure name="a7f6" id="a7f6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Uw5rmvUJts68UicYOwN_qQ.png" data-width="1428" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*Uw5rmvUJts68UicYOwN_qQ.png"></figure><p name="f277" id="f277" class="graf graf--p graf-after--figure">we postulate some value ρ₀ for the true population correlation between the random variables X and Y. If the random variables X and Y be jointly normally distributed and ρ₀ be equal to zero, then based on the result of Fisher,</p><ul class="postList"><li name="43de" id="43de" class="graf graf--li graf-after--p">if |ρ₀| &lt; 0.50, then</li></ul><figure name="0712" id="0712" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*fgZICR1X9EA8MWs17e4DlQ.png" data-width="1428" data-height="146" src="https://cdn-images-1.medium.com/max/800/1*fgZICR1X9EA8MWs17e4DlQ.png"></figure><p name="d623" id="d623" class="graf graf--p graf-after--figure">However, we can hardly find any case that the random variables X and Y be jointly normally distributed and ρ₀ be equal to zero, this is a very strong statement and it rarely happens in practice. But experience has shown that if ρ₀ does not wander too far away from zero, and if the random variables X and Y do not deviate horribly from the underlying assumption of joint normality, the test statistic has a distribution that is approximately standard normal.</p><p name="792a" id="792a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) Hypothesis Testing for |ρ₀| &gt; 0.50</strong></p><p name="8c3b" id="8c3b" class="graf graf--p graf-after--p">When |ρ₀| moves closer to 1, we can no longer expect Z. This is because the Gaussian distribution with a correlation of 0.99, the sampling distribution of ρ will be severely skewed to the left and the results are no longer held. So Fisher identified the problem and then design a transform that is named after him. The Fisher transformation is,</p><figure name="cf87" id="cf87" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*J1rnC9mHkI4plOj_4JURMg.png" data-width="1428" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*J1rnC9mHkI4plOj_4JURMg.png"></figure><p name="0b88" id="0b88" class="graf graf--p graf-after--figure">Based on that</p><ul class="postList"><li name="5ba7" id="5ba7" class="graf graf--li graf-after--p">if |ρ₀| &gt; 0.50, then</li></ul><figure name="cbba" id="cbba" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*Z8Z4gczVzX3OiN0pCAWvSA.png" data-width="1428" data-height="146" src="https://cdn-images-1.medium.com/max/800/1*Z8Z4gczVzX3OiN0pCAWvSA.png"></figure><p name="0282" id="0282" class="graf graf--p graf-after--figure">And actually, Fisher determined that F(ρ-cap) was closer to normal than just ρ-cap, but if we have |ρ-cap|&lt;0.5, then the F(ρ-cap) ≈ ρ-cap and so the transformation is less necessary when |ρ-cap|&lt;0.5.</p><p name="9acf" id="9acf" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) Confidence Interval for Correlation Inference</strong></p><p name="7612" id="7612" class="graf graf--p graf-after--p">Suppose that,</p><ul class="postList"><li name="20cf" id="20cf" class="graf graf--li graf-after--p">if |ρ-cap| &lt; 0.50, then</li></ul><figure name="015a" id="015a" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*hc6kxoHHqcGPCgzHHlo78Q.png" data-width="1428" data-height="136" src="https://cdn-images-1.medium.com/max/800/1*hc6kxoHHqcGPCgzHHlo78Q.png"></figure><ul class="postList"><li name="3ced" id="3ced" class="graf graf--li graf-after--figure">if |ρ-cap| &gt; 0.50, then the non-transformed confidence interval is,</li></ul><figure name="acab" id="acab" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*TFPkqtPlt6ZFD-lMRsiHLA.png" data-width="1428" data-height="132" src="https://cdn-images-1.medium.com/max/800/1*TFPkqtPlt6ZFD-lMRsiHLA.png"></figure><p name="9989" id="9989" class="graf graf--p graf-after--figure">Because this interval resides in a space that doesn’t correspond to the normal space in which we expect to see or interpret confidence intervals. Given the inverse function of the Fisher transformation as,</p><figure name="6ede" id="6ede" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6jggjXkSz9pUdv0IImpPVg.png" data-width="1428" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*6jggjXkSz9pUdv0IImpPVg.png"></figure><p name="514e" id="514e" class="graf graf--p graf-after--figure">Suppose if we call the left-hand endpoint of this interval <em class="markup--em markup--p-em">a</em> and call the right-hand endpoint of this interval <em class="markup--em markup--p-em">b</em>. Then the Fisher-transformed confidence interval for ρ is given by,</p><figure name="5281" id="5281" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*D1NdVssTeJnMI7fR-1rgdQ.png" data-width="1428" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*D1NdVssTeJnMI7fR-1rgdQ.png"></figure><p name="5cb1" id="5cb1" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(4) Two-Sample Inference for Correlation Coefficients</strong></p><p name="acec" id="acec" class="graf graf--p graf-after--p">Suppose we have,</p><figure name="b3ca" id="b3ca" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Hot_pZ3kef86NwPtQdV_fQ.png" data-width="1428" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*Hot_pZ3kef86NwPtQdV_fQ.png"></figure><p name="ef98" id="ef98" class="graf graf--p graf-after--figure">Then the appropriate test statistic is,</p><figure name="10ae" id="10ae" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*k_C5t3ejEjoFpJnxjssXtA.png" data-width="1428" data-height="166" src="https://cdn-images-1.medium.com/max/800/1*k_C5t3ejEjoFpJnxjssXtA.png"></figure><p name="af74" id="af74" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Two-Sample Confidence Interval for Correlation Coefficients</strong></p><p name="2142" id="2142" class="graf graf--p graf-after--p">The 100(1-α)% confidence interval for the difference between the two samples is,</p><figure name="35f1" id="35f1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5NOnbc-q6YqMaZKIW1NmFg.png" data-width="1452" data-height="296" src="https://cdn-images-1.medium.com/max/800/1*5NOnbc-q6YqMaZKIW1NmFg.png"></figure><p name="c059" id="c059" class="graf graf--p graf-after--figure">actually, we have the 100(1-α)% confidence interval as,</p><figure name="8382" id="8382" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ReeUR0l16_dW8PnDlDLm6g.png" data-width="1452" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*ReeUR0l16_dW8PnDlDLm6g.png"></figure><p name="bd6b" id="bd6b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">3. Two-Sample Testing For Means</strong></p><p name="6b90" id="6b90" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Case 1: If Two Samples Have the Same True Variance</strong></p><p name="ec46" id="ec46" class="graf graf--p graf-after--p">When,</p><figure name="9bd0" id="9bd0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9HVEdDfivO9oP9CuZjsVMg.png" data-width="1452" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*9HVEdDfivO9oP9CuZjsVMg.png"></figure><p name="71c6" id="71c6" class="graf graf--p graf-after--figure">then we have hypotheses as,</p><figure name="dcc8" id="dcc8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*XrdeQoOrQzKwVtU7_n-d-g.png" data-width="1452" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*XrdeQoOrQzKwVtU7_n-d-g.png"></figure><p name="20b0" id="20b0" class="graf graf--p graf-after--figure">Then the pooled variance is,</p><figure name="d21b" id="d21b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3VyrC9IyFa0SNS4bmCvDjw.png" data-width="1452" data-height="116" src="https://cdn-images-1.medium.com/max/800/1*3VyrC9IyFa0SNS4bmCvDjw.png"></figure><p name="f7d1" id="f7d1" class="graf graf--p graf-after--figure">Then, the T statistics is,</p><figure name="7494" id="7494" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*a9cqHrBGTEVL_7S-F0sNQw.png" data-width="1324" data-height="206" src="https://cdn-images-1.medium.com/max/800/1*a9cqHrBGTEVL_7S-F0sNQw.png"></figure><p name="c442" id="c442" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) Case 2: If Two Samples Have different True Variance</strong></p><p name="8ced" id="8ced" class="graf graf--p graf-after--p">When,</p><figure name="4142" id="4142" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VQlXbKCXMrC5Qr1RmptAng.png" data-width="1324" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*VQlXbKCXMrC5Qr1RmptAng.png"></figure><p name="3a2c" id="3a2c" class="graf graf--p graf-after--figure">then we have hypotheses as,</p><figure name="0a1b" id="0a1b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*XrdeQoOrQzKwVtU7_n-d-g.png" data-width="1452" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*XrdeQoOrQzKwVtU7_n-d-g.png"></figure><p name="6fb5" id="6fb5" class="graf graf--p graf-after--figure">Then the Satterthwaite’s degree of freedom is,</p><figure name="980a" id="980a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*a55YkjdVPvdBflNVhvcLDQ.png" data-width="1324" data-height="202" src="https://cdn-images-1.medium.com/max/800/1*a55YkjdVPvdBflNVhvcLDQ.png"></figure><p name="abd4" id="abd4" class="graf graf--p graf-after--figure">Then, the T statistics is,</p><figure name="aa4f" id="aa4f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*wM0kWgfX0CEZ4bCMwQOU3g.png" data-width="1324" data-height="202" src="https://cdn-images-1.medium.com/max/800/1*wM0kWgfX0CEZ4bCMwQOU3g.png"></figure><p name="b7b8" id="b7b8" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(3) Quick Checks on the Same True Variance</strong></p><ul class="postList"><li name="e62f" id="e62f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Lazy Approach:</strong> Always assume that nothing is equal, we can use case 2</li><li name="ae6e" id="ae6e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Normal Approach: </strong>Use a formal statistical test to check the null hypothesis,</li></ul><figure name="3b71" id="3b71" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*bjZp1LS8WBVIM4Prszzzyg.png" data-width="1324" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*bjZp1LS8WBVIM4Prszzzyg.png"></figure><ul class="postList"><li name="96fb" id="96fb" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Dirty Approach</strong>: Use F test for two sample variance …</li></ul><p name="9530" id="9530" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(4) F Testing for Variances of Two Samples</strong></p><p name="c5bc" id="c5bc" class="graf graf--p graf-after--p">If we have,</p><figure name="9ea3" id="9ea3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ngzaK3_pGOfZGtxVBQDdVg.png" data-width="1324" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*ngzaK3_pGOfZGtxVBQDdVg.png"></figure><p name="811f" id="811f" class="graf graf--p graf-after--figure">Then, suppose</p><figure name="1521" id="1521" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Gqe3PrFXaTgMAssa7bCS5Q.png" data-width="1324" data-height="54" src="https://cdn-images-1.medium.com/max/800/1*Gqe3PrFXaTgMAssa7bCS5Q.png"></figure><p name="8f28" id="8f28" class="graf graf--p graf-after--figure">this χ² is a chi-squared random variable with <em class="markup--em markup--p-em">n</em> degrees of freedom.</p><p name="b22f" id="b22f" class="graf graf--p graf-after--p">Suppose we have X ~ χ²(<em class="markup--em markup--p-em">n</em>) and Y ~ χ²(<em class="markup--em markup--p-em">m</em>), and X and Y are independent random variables, then,</p><figure name="b138" id="b138" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nnyz7VlfmJVslbEF_Admeg.png" data-width="1324" data-height="162" src="https://cdn-images-1.medium.com/max/800/1*nnyz7VlfmJVslbEF_Admeg.png"></figure><p name="5872" id="5872" class="graf graf--p graf-after--figure">So if we have,</p><figure name="b923" id="b923" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ngzaK3_pGOfZGtxVBQDdVg.png" data-width="1324" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*ngzaK3_pGOfZGtxVBQDdVg.png"></figure><p name="2371" id="2371" class="graf graf--p graf-after--figure">and,</p><figure name="3e9f" id="3e9f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*k5oW2667gqm_nVqAaY6W5A.png" data-width="1260" data-height="60" src="https://cdn-images-1.medium.com/max/800/1*k5oW2667gqm_nVqAaY6W5A.png"></figure><p name="2f3d" id="2f3d" class="graf graf--p graf-after--figure">we also know that,</p><figure name="e657" id="e657" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nmjeTF8psJQSmjyZzDUrow.png" data-width="1260" data-height="116" src="https://cdn-images-1.medium.com/max/800/1*nmjeTF8psJQSmjyZzDUrow.png"></figure><p name="e191" id="e191" class="graf graf--p graf-after--figure">then,</p><figure name="f29a" id="f29a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*evCrDXj9Y7p8mdPLRmUSCQ.png" data-width="1260" data-height="202" src="https://cdn-images-1.medium.com/max/800/1*evCrDXj9Y7p8mdPLRmUSCQ.png"></figure><p name="fe50" id="fe50" class="graf graf--p graf-after--figure">because our null hypothesis is,</p><figure name="e7a5" id="e7a5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*bjZp1LS8WBVIM4Prszzzyg.png" data-width="1324" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*bjZp1LS8WBVIM4Prszzzyg.png"></figure><p name="38e6" id="38e6" class="graf graf--p graf-after--figure">then,</p><figure name="c484" id="c484" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6L0S-UgdlrWwBm_64Z6etg.png" data-width="1260" data-height="116" src="https://cdn-images-1.medium.com/max/800/1*6L0S-UgdlrWwBm_64Z6etg.png"></figure><p name="5bd1" id="5bd1" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(5) Confidence Interval of the Variances Ratio between Two Samples</strong></p><p name="cf86" id="cf86" class="graf graf--p graf-after--p">Based on our previous discussion, we have the 100(1-α)% confidence interval of the variance ratio as,</p><figure name="a3f7" id="a3f7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_0STfTH3iyypyu8rI5KXHw.png" data-width="1260" data-height="132" src="https://cdn-images-1.medium.com/max/800/1*_0STfTH3iyypyu8rI5KXHw.png"></figure><p name="631e" id="631e" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">4. A Quick Review of Errors</strong></p><p name="310b" id="310b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) The Definition of the Type I Error</strong></p><p name="fd65" id="fd65" class="graf graf--p graf-after--p">A type I error occurs when we reject the null hypothesis when it is actually true. α is the value of the highest risk of a type I error that we can tolerate.</p><p name="6117" id="6117" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(2) The Definition of the Type II Error</strong></p><p name="35bb" id="35bb" class="graf graf--p graf-after--p">A type II error occurs when we do not reject the null hypothesis when it is actually false. β is the value of the highest risk of a type II error that we can tolerate.</p><p name="f140" id="f140" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) The Definition of the P-Value</strong></p><p name="a684" id="a684" class="graf graf--p graf-after--p">The P-value is actually the risk of a type I error calculated through the statistics. When P-value is smaller then α, that means we can tolerate the risk of rejecting the null hypothesis when it is actually true, so we can reject the null hypothesis. When P-value is greater then α, that means we can not tolerate the risk of rejecting the null hypothesis when it is actually true, so we can not have enough evidence to reject the null hypothesis.</p><p name="f845" id="f845" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) The Definition of Power</strong></p><p name="0657" id="0657" class="graf graf--p graf-after--p graf--trailing">The power of a statistical test is the probability that you reject the null hypothesis when the null hypothesis is actually false. There are three ways to increase the power of a hypothesis test: a) increase α; b) increase |μ0-μ1|, i.e. the effect size; c) Increase <em class="markup--em markup--p-em">n</em>, the sample size.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/9352212b215"><time class="dt-published" datetime="2020-09-21T10:05:08.394Z">September 21, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/probability-and-statistics-9-hypothesis-testing-type-i-error-type-ii-error-9352212b215" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>