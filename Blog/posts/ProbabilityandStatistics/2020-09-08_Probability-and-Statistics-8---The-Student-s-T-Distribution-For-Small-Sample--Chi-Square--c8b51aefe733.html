<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Probability and Statistics 8 | The Student’s T Distribution For Small Sample, Chi-Square…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Probability and Statistics 8 | The Student’s T Distribution For Small Sample, Chi-Square…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Series: Probability and Statistics
</section>
<section data-field="body" class="e-content">
<section name="e4b8" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="75bc" id="75bc" class="graf graf--h3 graf--leading graf--title">Probability and Statistics 8 | <strong class="markup--strong markup--h3-strong">The Student’s T Distribution For Small Sample, Chi-Square Distribution For Variance, and Confidence Interval</strong></h3><figure name="39ab" id="39ab" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*YTeSIjiYirbk4CZavQXCqg.jpeg" data-width="1602" data-height="1141" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*YTeSIjiYirbk4CZavQXCqg.jpeg"></figure><ol class="postList"><li name="0312" id="0312" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">The Student’s T Distribution</strong></li></ol><p name="a0a3" id="a0a3" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">(1) The Standardized Center Limit Theorem</strong></p><p name="278d" id="278d" class="graf graf--p graf-after--p">Note that if CLT says that for large <em class="markup--em markup--p-em">n</em>, we have that,</p><figure name="e177" id="e177" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rIFy8WTzQIV4AzaXpMEm8Q.png" data-width="1284" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*rIFy8WTzQIV4AzaXpMEm8Q.png"></figure><p name="04b8" id="04b8" class="graf graf--p graf-after--figure">then, this also means that,</p><figure name="df4e" id="df4e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BBO0GOSVfmUuYo0bCGdETg.png" data-width="1284" data-height="128" src="https://cdn-images-1.medium.com/max/800/1*BBO0GOSVfmUuYo0bCGdETg.png"></figure><p name="8860" id="8860" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) The Weaken Condition of Standardized Center Limit Theorem</strong></p><p name="bb13" id="bb13" class="graf graf--p graf-after--p">A weakening of the standard result is ( for example, when we don’t know σ ) is to replace σ by <em class="markup--em markup--p-em">s</em>, where <em class="markup--em markup--p-em">s</em> is given by,</p><figure name="89f0" id="89f0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*IgJku0Y9Zvk4QsG9q2OC8Q.png" data-width="1284" data-height="166" src="https://cdn-images-1.medium.com/max/800/1*IgJku0Y9Zvk4QsG9q2OC8Q.png"></figure><p name="e49b" id="e49b" class="graf graf--p graf-after--figure">This gives us a random variable, which is frequently noted as T.</p><p name="f7da" id="f7da" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) The Student’s T Distribution</strong></p><p name="3fc0" id="3fc0" class="graf graf--p graf-after--p">Suppose we have an estimator of σ which is, as we have said, <em class="markup--em markup--p-em">s</em>, and by the standardized center limit theorem, we can then have, the random variable T satisfies,</p><figure name="84d0" id="84d0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*x03tbBpanEGwg6PUFdf8gg.png" data-width="1284" data-height="128" src="https://cdn-images-1.medium.com/max/800/1*x03tbBpanEGwg6PUFdf8gg.png"></figure><p name="a813" id="a813" class="graf graf--p graf-after--figure">we then call the distribution the student’s T distribution and the parameter (<em class="markup--em markup--p-em">n-</em>1) is called the degree of freedom in this distribution.</p><figure name="0ec4" id="0ec4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*o5lUykCgypRmxok9fKoxfA.png" data-width="1886" data-height="432" src="https://cdn-images-1.medium.com/max/800/1*o5lUykCgypRmxok9fKoxfA.png"></figure><p name="58ba" id="58ba" class="graf graf--p graf-after--figure">Note that the tails of a student’s t distribution are a bit <strong class="markup--strong markup--p-strong">flatter</strong> than a standardized Gaussian (Normal) Distribution. It is also a common result that as n → ∞, we have <em class="markup--em markup--p-em">t</em>( <em class="markup--em markup--p-em">n</em>-1 ) → N(0, 1)</p><p name="e2a0" id="e2a0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Confidence Intervals</strong></p><p name="7683" id="7683" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(1) Motivation For Confidence Intervals</strong></p><p name="308a" id="308a" class="graf graf--p graf-after--p">All confidence-interval-like results come from some CLT-like results or some knowledge about the distribution of a statistic. Here is an example.</p><p name="7499" id="7499" class="graf graf--p graf-after--p">Let X1, …, Xn be a random sample from a normal random variable with unknown mean μ and known variance σ² (This is a quite strong statement and we are cheating here. So what we want is to give a brief example and we will talk about some general cases later). Under the CLT, we have the following result that if we choose a well-chosen threshold in the left tail of the standard normal distribution, and we find the probability is between that well-chosen threshold, then the probability will be exactly equal to 1-α,</p><figure name="7a65" id="7a65" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mhZFvGaNxymww9XY9tOEqA.png" data-width="1434" data-height="230" src="https://cdn-images-1.medium.com/max/800/1*mhZFvGaNxymww9XY9tOEqA.png"></figure><p name="24d8" id="24d8" class="graf graf--p graf-after--figure">Now, let’s take this expression and seek to isolate μ,</p><figure name="82dc" id="82dc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1UrkkqkELFauE3gm2W6zSQ.png" data-width="1434" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*1UrkkqkELFauE3gm2W6zSQ.png"></figure><p name="6d44" id="6d44" class="graf graf--p graf-after--figure">then,</p><figure name="cc36" id="cc36" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*epa821TK2-kz8eSLj_wFOw.png" data-width="1434" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*epa821TK2-kz8eSLj_wFOw.png"></figure><p name="2dae" id="2dae" class="graf graf--p graf-after--figure">then,</p><figure name="b665" id="b665" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CnqsLh65RTjvouDBX3LaWw.png" data-width="1434" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*CnqsLh65RTjvouDBX3LaWw.png"></figure><p name="d86c" id="d86c" class="graf graf--p graf-after--figure">then,</p><figure name="db35" id="db35" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*T7C-ny0ff2ZdIbwNW0HT0w.png" data-width="1694" data-height="294" src="https://cdn-images-1.medium.com/max/800/1*T7C-ny0ff2ZdIbwNW0HT0w.png"></figure><p name="542f" id="542f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">(2) The Interpretations of Confidence Interval</strong></p><p name="508c" id="508c" class="graf graf--p graf-after--p">Let,</p><figure name="742e" id="742e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*GoGsAMfwGFeP1iJRNFYa2Q.png" data-width="1668" data-height="92" src="https://cdn-images-1.medium.com/max/800/1*GoGsAMfwGFeP1iJRNFYa2Q.png"></figure><p name="ada4" id="ada4" class="graf graf--p graf-after--figure">and,</p><figure name="e416" id="e416" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*LINweIdn2SvkSBLd_2YfjA.png" data-width="1668" data-height="92" src="https://cdn-images-1.medium.com/max/800/1*LINweIdn2SvkSBLd_2YfjA.png"></figure><p name="74c1" id="74c1" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">FALSE Interpretation #1:</strong></p><p name="0077" id="0077" class="graf graf--p graf-after--p">(1-α)100% chance of the data lies between (i.e. Xi5) lie between <em class="markup--em markup--p-em">a</em> and<em class="markup--em markup--p-em"> b</em>. This is simply false and the issue here is the confidence interval comes from a CLT result, which is a commentary on the distribution of statistics. By CLT, the quantile of the normal distribution is for the sum of these data, but not for the data themselves.</p><p name="1a98" id="1a98" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">FALSE Interpretation #2:</strong></p><p name="cd19" id="cd19" class="graf graf--p graf-after--p">(1-α)100% chance of the</p><figure name="ca68" id="ca68" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*86hdNo7mjloeELRQVu5v8A.png" data-width="1552" data-height="58" src="https://cdn-images-1.medium.com/max/800/1*86hdNo7mjloeELRQVu5v8A.png"></figure><p name="f025" id="f025" class="graf graf--p graf-after--figure">(standardized) lies between <em class="markup--em markup--p-em">a</em> and<em class="markup--em markup--p-em"> b</em>. This is also false. So what is the exact probability that the X-bars lie in the interval? The answer should be 100% because it constructed to do so. So this interpretation is equally bad with X-bar (standardized) lies between it 100% because <em class="markup--em markup--p-em">a</em> and <em class="markup--em markup--p-em">b</em> are determined by X-bar.</p><p name="11b2" id="11b2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">TRUE Interpretation #3:</strong></p><p name="80fe" id="80fe" class="graf graf--p graf-after--p">There is a (1-α)100% chance that the true population mean μ lies between the random interval <em class="markup--em markup--p-em">a</em> and <em class="markup--em markup--p-em">b</em>.</p><ul class="postList"><li name="4767" id="4767" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Objection</strong></li></ul><p name="b632" id="b632" class="graf graf--p graf-after--li">This interpretation suggests that μ is stochastic and, without th comment “random interval” downplays the notion that a and b are random endpoints.</p><ul class="postList"><li name="67ff" id="67ff" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Assume Connection</strong></li></ul><p name="6a7c" id="6a7c" class="graf graf--p graf-after--li">The random interval <em class="markup--em markup--p-em">a</em> and <em class="markup--em markup--p-em">b</em> is constructed in such a way that contains μ 100(1-α)% of the time.</p><p name="8e1a" id="8e1a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">TRUE Interpretation #4:</strong></p><p name="7053" id="7053" class="graf graf--p graf-after--p">The confidence interval over my particular random sample is [<em class="markup--em markup--p-em">a</em>, <em class="markup--em markup--p-em">b</em>]. Similarly constructed intervals, computed over many different random samples, contain the true population mean μ with probability 1-α.</p><p name="2643" id="2643" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(3) The Prior Confidence Interval</strong></p><p name="1733" id="1733" class="graf graf--p graf-after--p">If,</p><figure name="c631" id="c631" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kxW-z7rd6mlzcnTfXkyV7A.png" data-width="1552" data-height="86" src="https://cdn-images-1.medium.com/max/800/1*kxW-z7rd6mlzcnTfXkyV7A.png"></figure><p name="2c6c" id="2c6c" class="graf graf--p graf-after--figure">is exact that if X1, …, Xn are normal and σ is known. But in practice, both of these assumptions are unrealistic, so we have to find a way to replace or remove these assumptions.</p><ul class="postList"><li name="3a0d" id="3a0d" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Weakening Condition #1</strong></li></ul><p name="4597" id="4597" class="graf graf--p graf-after--li">If the Xi s’ are not normal — but are not severely non-normal — then the interval,</p><figure name="7c03" id="7c03" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kxW-z7rd6mlzcnTfXkyV7A.png" data-width="1552" data-height="86" src="https://cdn-images-1.medium.com/max/800/1*kxW-z7rd6mlzcnTfXkyV7A.png"></figure><p name="ae3e" id="ae3e" class="graf graf--p graf-after--figure">is approximate rather than exact (Justification CLT). So how approximate is depend on how much the Xi deviates from the normal distribution. If Xi ’s are severely skewed with a flat tail, then this is not going to be a good approximation. If Xi ’s are kind of like roughly symmetric with tails not too thin or too fat, then this is going to be a good approximation.</p><ul class="postList"><li name="f048" id="f048" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Weakening Condition #2</strong></li></ul><p name="2ea4" id="2ea4" class="graf graf--p graf-after--li">If σ² is unknown and must be estimated with s², we end up with,</p><figure name="d729" id="d729" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*oDGbLw6mzuCydiTCdlHBIw.png" data-width="1520" data-height="288" src="https://cdn-images-1.medium.com/max/800/1*oDGbLw6mzuCydiTCdlHBIw.png"></figure><p name="c6ea" id="c6ea" class="graf graf--p graf-after--figure">In general, we can make this approximation a little more exact by using,</p><figure name="8bad" id="8bad" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fGKrTWlVk8eM1mjpux68jA.png" data-width="1194" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*fGKrTWlVk8eM1mjpux68jA.png"></figure><p name="c2ae" id="c2ae" class="graf graf--p graf-after--figure">instead of,</p><figure name="3db1" id="3db1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6CsUkXgertgiqe0UtT5XZg.png" data-width="1194" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*6CsUkXgertgiqe0UtT5XZg.png"></figure><p name="7b3b" id="7b3b" class="graf graf--p graf-after--figure">If we do that, and use,</p><figure name="f32c" id="f32c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-1-jJf7Fa7WRYNMF3koxxQ.png" data-width="1194" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*-1-jJf7Fa7WRYNMF3koxxQ.png"></figure><p name="136c" id="136c" class="graf graf--p graf-after--figure">this 100(1-α)% confidence interval is exact when the Xi are normal (but σ unknown) and an approximation otherwise.</p><p name="034a" id="034a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(4) The De Moivre -La Place Theorem</strong>: Precursor for CLT when Xi ‘s are Bernoulli random variables.</p><p name="34a3" id="34a3" class="graf graf--p graf-after--p">Let Xi ~ Ber(<em class="markup--em markup--p-em">p</em>) be independent for <em class="markup--em markup--p-em">i </em>= 1, …, <em class="markup--em markup--p-em">n</em>. Call X = X1 + … + X<em class="markup--em markup--p-em">n</em> (Aside: X ~ Bin(<em class="markup--em markup--p-em">n</em>, <em class="markup--em markup--p-em">p</em>) ), then,</p><figure name="38cf" id="38cf" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-rOJa05w6aBjh0opCR5qfQ.png" data-width="1364" data-height="140" src="https://cdn-images-1.medium.com/max/800/1*-rOJa05w6aBjh0opCR5qfQ.png"></figure><p name="38d6" id="38d6" class="graf graf--p graf-after--figure">in other words,</p><figure name="3983" id="3983" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ATizHLsAjkTFldHB2QR_tw.png" data-width="1364" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*ATizHLsAjkTFldHB2QR_tw.png"></figure><p name="674a" id="674a" class="graf graf--p graf-after--figure">this means,</p><figure name="36a1" id="36a1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qfRU5Vu1lOyLdBWIZi6spw.png" data-width="1364" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*qfRU5Vu1lOyLdBWIZi6spw.png"></figure><p name="eb31" id="eb31" class="graf graf--p graf-after--figure">Aside, I will try to reserve π for the true population proportion and,</p><figure name="d703" id="d703" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WKfQzxOSB_6VDfQH0HwX9g.png" data-width="1364" data-height="50" src="https://cdn-images-1.medium.com/max/800/1*WKfQzxOSB_6VDfQH0HwX9g.png"></figure><p name="d68a" id="d68a" class="graf graf--p graf-after--figure">for the sample proportion.</p><p name="27ed" id="27ed" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(5) The De Moivre -La Place Theorem:</strong> An Example</p><p name="1bc0" id="1bc0" class="graf graf--p graf-after--p">By the standardized CLT, we can have that if we standard the statistic θ, then,</p><figure name="9d6e" id="9d6e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_7eY8ZZe4xEytGexN_BivA.png" data-width="1364" data-height="116" src="https://cdn-images-1.medium.com/max/800/1*_7eY8ZZe4xEytGexN_BivA.png"></figure><p name="bc58" id="bc58" class="graf graf--p graf-after--figure">Suppose that we have π for the true population proportion and,</p><figure name="474f" id="474f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WKfQzxOSB_6VDfQH0HwX9g.png" data-width="1364" data-height="50" src="https://cdn-images-1.medium.com/max/800/1*WKfQzxOSB_6VDfQH0HwX9g.png"></figure><p name="ed6b" id="ed6b" class="graf graf--p graf-after--figure">for the sample proportion. By DL Theorem,</p><figure name="02f4" id="02f4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*q8HORuzYUJkUxEwl7GtGIQ.png" data-width="1364" data-height="146" src="https://cdn-images-1.medium.com/max/800/1*q8HORuzYUJkUxEwl7GtGIQ.png"></figure><p name="0c8c" id="0c8c" class="graf graf--p graf-after--figure">then,</p><figure name="ae82" id="ae82" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*TpS_uaPxASMZr8mI6qaGAw.png" data-width="1364" data-height="146" src="https://cdn-images-1.medium.com/max/800/1*TpS_uaPxASMZr8mI6qaGAw.png"></figure><p name="2794" id="2794" class="graf graf--p graf-after--figure">then,</p><figure name="51ec" id="51ec" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rQpNfjNz0-XQMkRlDvznhg.png" data-width="1364" data-height="146" src="https://cdn-images-1.medium.com/max/800/1*rQpNfjNz0-XQMkRlDvznhg.png"></figure><p name="6572" id="6572" class="graf graf--p graf-after--figure">so,</p><figure name="e504" id="e504" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Nk-Ei69gviZcsImnIexO8A.png" data-width="1364" data-height="138" src="https://cdn-images-1.medium.com/max/800/1*Nk-Ei69gviZcsImnIexO8A.png"></figure><p name="88ac" id="88ac" class="graf graf--p graf-after--figure">is the confidence interval for the 100(1-α)% probability of true value π.</p><p name="9546" id="9546" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(6) The Condition for De Moivre -La Place Theorem As An Approximation</strong></p><p name="73bf" id="73bf" class="graf graf--p graf-after--p">So when is the approximation under DL good? There is a heuristic when</p><figure name="a4cc" id="a4cc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9F5tbXdiGLe7W0d20b7wQQ.png" data-width="1364" data-height="52" src="https://cdn-images-1.medium.com/max/800/1*9F5tbXdiGLe7W0d20b7wQQ.png"></figure><p name="c32b" id="c32b" class="graf graf--p graf-after--figure">then we can say the approximate is just fine. This will be unhelpful if we don’t know π, so just use</p><figure name="0b1f" id="0b1f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WKfQzxOSB_6VDfQH0HwX9g.png" data-width="1364" data-height="50" src="https://cdn-images-1.medium.com/max/800/1*WKfQzxOSB_6VDfQH0HwX9g.png"></figure><p name="13db" id="13db" class="graf graf--p graf-after--figure">to substitute π if we don’t know its exact value in the first place.</p><p name="1190" id="1190" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(7) Chi-Square Distribution</strong></p><p name="d851" id="d851" class="graf graf--p graf-after--p">χ²(<em class="markup--em markup--p-em">n</em>) is, by definition, a random variable generated by <strong class="markup--strong markup--p-strong">summing</strong> <strong class="markup--strong markup--p-strong">squared</strong> independent standard <strong class="markup--strong markup--p-strong">normal</strong> random variables.</p><p name="3a37" id="3a37" class="graf graf--p graf-after--p">For example, if X⊥Y, and Z = X² + Y² and X ~ N(0, 1) with Y ~ N(0, 1), then Z ~χ²(2).</p><p name="43f3" id="43f3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(8) The Confidence Interval for The Variance σ²</strong></p><p name="39ff" id="39ff" class="graf graf--p graf-after--p">If X1, …, Xn are independently identical distributed (aka, i.i.d) normal random variables with mean μ and variance σ², then,</p><figure name="421f" id="421f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Dt6xQNxukmT8KsfXPBrKMQ.png" data-width="1266" data-height="222" src="https://cdn-images-1.medium.com/max/800/1*Dt6xQNxukmT8KsfXPBrKMQ.png"></figure><p name="07f2" id="07f2" class="graf graf--p graf-after--figure">Using this result,</p><figure name="a5ee" id="a5ee" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9LjCyYgGRZogBB-NDjN_yQ.png" data-width="1266" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*9LjCyYgGRZogBB-NDjN_yQ.png"></figure><p name="2ce6" id="2ce6" class="graf graf--p graf-after--figure">by graphing (χ² is a right-skewed distribution), which is,</p><figure name="5686" id="5686" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-Z-PFsRhxLtIv6MY0CMA-A.png" data-width="1266" data-height="356" src="https://cdn-images-1.medium.com/max/800/1*-Z-PFsRhxLtIv6MY0CMA-A.png"></figure><p name="0459" id="0459" class="graf graf--p graf-after--figure">Then,</p><figure name="2f28" id="2f28" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xFRoDy8XTlKKUE9jZlaxJw.png" data-width="1266" data-height="126" src="https://cdn-images-1.medium.com/max/800/1*xFRoDy8XTlKKUE9jZlaxJw.png"></figure><p name="1867" id="1867" class="graf graf--p graf-after--figure">thus,</p><figure name="156f" id="156f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pZMr0Fjqwv95I5gDZDwM9w.png" data-width="1266" data-height="126" src="https://cdn-images-1.medium.com/max/800/1*pZMr0Fjqwv95I5gDZDwM9w.png"></figure><p name="93c4" id="93c4" class="graf graf--p graf-after--figure">this is supposed to be our confidence interval for the variance.</p><p name="9739" id="9739" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">(9) Conditions for Normality Assumption</strong></p><p name="2bbd" id="2bbd" class="graf graf--p graf-after--p">For a theorem—like the one we just used or the student’s T theorem—how much abuse can normality assumption take?</p><p name="3105" id="3105" class="graf graf--p graf-after--p">As long as the random variable/data satisfy the following conditions, most results that we see in this class that depend upon the normality assumption are <strong class="markup--strong markup--p-strong">robust</strong> to its violation.</p><p name="812d" id="812d" class="graf graf--p graf-after--p">(a) No heavy tails for a random variable to no outliers for data drawn from a random variable</p><p name="929b" id="929b" class="graf graf--p graf-after--p">(b) No skewness, particularly severe skewness</p><p name="6848" id="6848" class="graf graf--p graf-after--p">(c) No multi-modality, i.e., you distributions should be unimodal</p><p name="7ff8" id="7ff8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. A Brief Review of Hypothesis Testing</strong></p><p name="cad3" id="cad3" class="graf graf--p graf-after--p">Suppose that X1, …, Xn is drawn from a normal variable in an independent way. Suppose that you want to test H₀: σ² = 25 and H₁: σ² ≠ 25.</p><p name="2a00" id="2a00" class="graf graf--p graf-after--p">So our idea is that assume H₀ holds, which is also to say that if Xi ~ N(μ, σ² = 25), then,</p><figure name="7926" id="7926" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*s_SxBxL3l7BTrhxnX6gV4g.png" data-width="1650" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*s_SxBxL3l7BTrhxnX6gV4g.png"></figure><p name="8c19" id="8c19" class="graf graf--p graf-after--figure">We collect a random sample and take that <em class="markup--em markup--p-em">n</em> = 20 and we find that,</p><figure name="2917" id="2917" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Gfvisrw0NQ-caGRt4trn_w.png" data-width="1286" data-height="56" src="https://cdn-images-1.medium.com/max/800/1*Gfvisrw0NQ-caGRt4trn_w.png"></figure><p name="7dd0" id="7dd0" class="graf graf--p graf-after--figure">We can then calculate,</p><figure name="60a5" id="60a5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*JQ-PZvaYpUIe9l5ISw44fA.png" data-width="1432" data-height="96" src="https://cdn-images-1.medium.com/max/800/1*JQ-PZvaYpUIe9l5ISw44fA.png"></figure><p name="9dc6" id="9dc6" class="graf graf--p graf-after--figure">By calculating,</p><figure name="a694" id="a694" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*--hSs2rdvKhkRGYcfNqWCA.png" data-width="1432" data-height="356" src="https://cdn-images-1.medium.com/max/800/1*--hSs2rdvKhkRGYcfNqWCA.png"></figure><p name="a821" id="a821" class="graf graf--p graf-after--figure">This tail probability .0074 = .74% encodes how rare it is to see a χ² ≥ 37.25 if H₀ is true.</p><p name="0d47" id="0d47" class="graf graf--p graf-after--p graf--trailing">We will talk more about that in the next part.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@adamedelweiss" class="p-author h-card">Adam Edelweiss</a> on <a href="https://medium.com/p/c8b51aefe733"><time class="dt-published" datetime="2020-09-08T07:42:27.752Z">September 8, 2020</time></a>.</p><p><a href="https://medium.com/@adamedelweiss/probability-and-statistics-8-the-students-t-distribution-for-small-sample-chi-square-c8b51aefe733" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 15, 2021.</p></footer></article></body></html>